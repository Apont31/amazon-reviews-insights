{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5523fd7c",
   "metadata": {},
   "source": [
    "# Objectif d'optimisation — C5.2.3\n",
    "\n",
    "**But** : optimiser deux pipelines ML du projet — **Sentiment** (text → TF-IDF/chi2/SVD → LR) et **Emotions** (SBERT embeddings → OneVsRest LR / SGD) — pour améliorer les métriques clés (AP_macro pour émotions, F1/accuracy pour sentiment), tout en restant réaliste vis-à-vis des ressources (RAM 16GB, GPU disponible).\n",
    "\n",
    "**Critères d'évaluation utilisés :**\n",
    "- Emotions : **Average Precision (AP) par label** et **AP_macro** ; seuils optimisés par label (max F1) ; métriques globales micro/macro F1.\n",
    "- Sentiment : **F1_macro**, **accuracy** et matrice de confusion.\n",
    "- Comparaison : tableau récapitulatif (baseline vs optimisé) et calcul du **gain absolu** et **relatif (%)** sur les métriques principales.\n",
    "- Traçabilité : sauvegarder toutes les expérimentations, modèles et paramètres dans `artifacts/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aace28",
   "metadata": {},
   "source": [
    "# Leviers d'optimisation\n",
    "\n",
    "**DATA**\n",
    "- Augmenter la taille d'entraînement (ex. pseudo-labels emotions → tout le dataset).\n",
    "- Filtrer/normaliser les labels rares (seuils min support) ou regrouper étiquettes proches.\n",
    "- Pseudo-labeling & self-training (teacher → student), puis réentrainement sur tout le dataset.\n",
    "- Data augmentation textuelle (back-translation, synonym replace) si viable.\n",
    "\n",
    "**HYPERPARAMÈTRES**\n",
    "- Sentiment : `k` (SelectKBest chi2), `C` (LR), optionnel `n_components` (SVD).\n",
    "- Emotions : `C` (LR), seuils par label (`thr`), régularisation `alpha` si SGD.\n",
    "- Stratégies : GridSearch / RandomizedSearch, K-fold pour thresholds, calibration (sigmoid).\n",
    "\n",
    "**INFRASTRUCTURE**\n",
    "- GPU pour SBERT encoding (batch large).\n",
    "- Memmap + partial_fit (SGD) pour entraîner sur tout le jeu si RAM limitée.\n",
    "- Calibration/OneVsRest en CPU si GPU pas nécessaire.\n",
    "- Logging et checkpoints pour reprendre si plantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f63bc00",
   "metadata": {},
   "source": [
    "# Optimisation — méthode et objectifs\n",
    "\n",
    "**Objectif** : améliorer les performances des modèles *Sentiment* et *Emotions* (AP, F1 micro/macro) en agissant sur :\n",
    "- **DATA** : cleaner / équilibrer / augmenter (ici : pseudo-labels, filtrage des labels rares).\n",
    "- **HYPERPARAMÈTRES** : C pour LogisticRegression, alpha pour SGD, learning_rate, etc.\n",
    "- **INFRA / LIBS** : SBERT embeddings (GPU), memmap + partial_fit (RAM limite), calibration (CalibratedClassifierCV).\n",
    "- **MÉTRIQUES & COMPARAISON** : AP par label (average_precision), F1 micro/macro, tableau comparatif.\n",
    "\n",
    "Approche :\n",
    "1. Diagnostiquer états & jeux de données existants.\n",
    "2. Construire / réutiliser pipelines pour sentiment et emotions.\n",
    "3. Faire une **recherche rapide d’hyperparamètres** sur un *sous-échantillon* (pour ne pas surcharger la machine).\n",
    "4. Si positif, entraîner la version finale (sur train+val) — en mode mémoire-safe (memmap & SGD partial_fit pour large dataset).\n",
    "5. Calibrer probabilités, trouver thresholds (K-fold thresholds) et évaluer sur test.\n",
    "6. Sauvegarder artefacts et produire un tableau de comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6efcb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire artifacts existe : True\n",
      "Fichiers artifacts (quelques-uns) : ['X_all_memmap.dat', 'X_all_memmap_meta.joblib', 'absa_auto_ensemble_conservative.csv', 'absa_auto_ensemble_conservative_disagreements.csv', 'absa_auto_ensemble_disagreements.csv', 'absa_auto_ensemble_labels.csv', 'absa_auto_ensemble_ml_disagreements.csv', 'absa_auto_ensemble_ml_labels.csv', 'absa_auto_mapping.joblib', 'absa_candidates_embeddings.joblib', 'absa_candidates_embs_reduced_labels.joblib', 'absa_candidates_umap_embs_labels.joblib', 'absa_manual_qc_sample.csv', 'absa_reviews_with_auto_aspects.csv', 'emo_calibrated_ovr_sigmoid.joblib', 'emo_clf_final_trainval.joblib', 'emo_clf_final_trainval_calibrated.joblib', 'emo_partial_test_artifacts.joblib', 'emo_sbert_scaler.joblib', 'emo_sgd_partial_models.joblib', 'emo_sgd_partial_models_epoch1.joblib', 'emo_sgd_partial_models_epoch10.joblib', 'emo_sgd_partial_models_epoch11.joblib', 'emo_sgd_partial_models_epoch12.joblib', 'emo_sgd_partial_models_epoch13.joblib', 'emo_sgd_partial_models_epoch14.joblib', 'emo_sgd_partial_models_epoch15.joblib', 'emo_sgd_partial_models_epoch2.joblib', 'emo_sgd_partial_models_epoch3.joblib', 'emo_sgd_partial_models_epoch4.joblib']\n",
      "Objets en mémoire (quelques clés) : ['emo_df', 'clf_final_cal']\n",
      "RAM totale (GB): 15.628524780273438  | disponible (GB): 2.4295654296875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diagnostics rapides — détecte objets existants et capacités machine\n",
    "import os, psutil, joblib, numpy as np, pandas as pd, gc\n",
    "print(\"Répertoire artifacts existe :\", os.path.exists(\"artifacts\"))\n",
    "print(\"Fichiers artifacts (quelques-uns) :\", sorted(os.listdir(\"artifacts\"))[:30] if os.path.exists(\"artifacts\") else [])\n",
    "print(\"Objets en mémoire (quelques clés) :\", [k for k in globals().keys() if k.lower().startswith((\"emo\",\"sent\",\"xtr\",\"xte\",\"ytr\",\"yte\",\"clf\"))][:60])\n",
    "print(\"RAM totale (GB):\", psutil.virtual_memory().total/1024**3, \" | disponible (GB):\", psutil.virtual_memory().available/1024**3)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653db0d",
   "metadata": {},
   "source": [
    "## Optimisation - Sentiment\n",
    "\n",
    "Stratégie:\n",
    "- Diagnostic des jeux (texts / y) et du modèle existant (recycler si présent).\n",
    "- Baseline rapide (réentrainement sur petit échantillon ou ré-évaluation du modèle existant).\n",
    "- Recherche d'hyperparamètres légère (C pour LR) sur sous-échantillon pour ne pas saturer la RAM.\n",
    "- Si dataset grand -> option memmap + SGDClassifier (partial_fit).\n",
    "- Mesures : accuracy / AP (si multi), F1 micro/macro, matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "042217de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dispo: ['X_tfidf_sample.npz']\n",
      "y dispo: ['y_binary_sample.joblib']\n"
     ]
    }
   ],
   "source": [
    "# Imports & chemins\n",
    "from pathlib import Path\n",
    "import time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             average_precision_score, precision_recall_curve, roc_curve,\n",
    "                             f1_score, PrecisionRecallDisplay)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import load, dump\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RNG = 42\n",
    "ART_DIR = Path(\"models\")             # là où tu as sauvegardé sel / sfm / svd_pipe\n",
    "DATA_DIR = Path(\"data/processed\")    # là où sont X_tfidf_*.npz et y_*.joblib si existants\n",
    "OUT_DIR = Path(\"models\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Trouver les derniers X/y produits en 5.2.1/5.2.2\n",
    "X_CAND = sorted(DATA_DIR.glob(\"X_tfidf*.npz\"))\n",
    "Y_CAND = sorted(DATA_DIR.glob(\"y_*binary*.joblib\"))\n",
    "print(\"X dispo:\", [p.name for p in X_CAND][-3:])\n",
    "print(\"y dispo:\", [p.name for p in Y_CAND][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa67e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement: X_tfidf_sample.npz | y_binary_sample.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((120000, 180007), (120000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert X_CAND and Y_CAND, \"Pas de features X / labels y trouvés. Exécute 5.2.1 pour les générer.\"\n",
    "X_path = X_CAND[-1]; y_path = Y_CAND[-1]\n",
    "print(\"Chargement:\", X_path.name, \"|\", y_path.name)\n",
    "\n",
    "X_all = sparse.load_npz(X_path)\n",
    "y_all = load(y_path).astype(int)\n",
    "X_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b9879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (96000, 180007) | val: (12000, 180007) | test: (12000, 180007) | pos ratio train: 0.7614583333333333\n"
     ]
    }
   ],
   "source": [
    "# 80/10/10 (ou adapte selon tes besoins)\n",
    "X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "    X_all, y_all, test_size=0.20, stratify=y_all, random_state=RNG\n",
    ")\n",
    "X_va, X_te, y_va, y_te = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.50, stratify=y_tmp, random_state=RNG\n",
    ")\n",
    "print(\"train:\", X_tr.shape, \"| val:\", X_va.shape, \"| test:\", X_te.shape, \"| pos ratio train:\", y_tr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f2f8258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAPES] train (96000, 180007) | val (12000, 180007) | test (12000, 180007)\n",
      "[DENSITY] train 0.316% | val 0.314% | test 0.322%\n",
      "[DONE] X_tr_, X_va_, X_te_ prêts.\n"
     ]
    }
   ],
   "source": [
    "# === RESCUE CELL ===\n",
    "# Reconstruit X_tr_, X_va_, X_te_ à partir de X_tr/X_va/X_te en appliquant ton pipeline (chi2 -> L1 -> SVD si présents)\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def _apply_optional(sel_obj, X):\n",
    "    return sel_obj.transform(X) if sel_obj is not None else X\n",
    "\n",
    "# 1) Vérifs d'existence des bases\n",
    "assert 'X_tr' in globals() and 'X_va' in globals() and 'X_te' in globals(), \\\n",
    "    \"Il faut d'abord exécuter la cellule qui split X_all/y_all en X_tr, X_va, X_te.\"\n",
    "\n",
    "# 2) Appliquer EXACTEMENT ce que tu as prévu :\n",
    "#    d'abord chi² si 'sel' est présent, puis L1 SFM si 'sfm', puis SVD si 'svd_pipe'\n",
    "X_tr_ = X_tr\n",
    "X_va_ = X_va\n",
    "X_te_ = X_te\n",
    "\n",
    "# chi²\n",
    "if 'sel' in globals() and sel is not None:\n",
    "    X_tr_ = sel.transform(X_tr_)\n",
    "    X_va_ = sel.transform(X_va_)\n",
    "    X_te_ = sel.transform(X_te_)\n",
    "    print(\"[OK] chi² appliqué ->\", X_tr_.shape)\n",
    "\n",
    "# L1 SFM (optionnel)\n",
    "if 'sfm' in globals() and sfm is not None:\n",
    "    X_tr_ = sfm.transform(X_tr_)\n",
    "    X_va_ = sfm.transform(X_va_)\n",
    "    X_te_ = sfm.transform(X_te_)\n",
    "    print(\"[OK] L1 SFM appliqué ->\", X_tr_.shape)\n",
    "\n",
    "# SVD (optionnel) — produit souvent des denses\n",
    "if 'svd_pipe' in globals() and svd_pipe is not None:\n",
    "    # svd_pipe peut être un Pipeline (ex: Normalizer + TruncatedSVD)\n",
    "    X_tr_ = svd_pipe.transform(X_tr_)\n",
    "    X_va_ = svd_pipe.transform(X_va_)\n",
    "    X_te_ = svd_pipe.transform(X_te_)\n",
    "    # Assure un dtype float32\n",
    "    X_tr_ = np.asarray(X_tr_, dtype=np.float32)\n",
    "    X_va_ = np.asarray(X_va_, dtype=np.float32)\n",
    "    X_te_ = np.asarray(X_te_, dtype=np.float32)\n",
    "    print(\"[OK] SVD appliqué ->\", X_tr_.shape, \"(dense)\")\n",
    "\n",
    "# 3) Mise au bon format mémoire\n",
    "# - si c’est encore sparse, garde CSR float32 (cohérent avec tes cellules)\n",
    "# - si c’est dense (après SVD), force np.float32\n",
    "def _to_ideal_format(X):\n",
    "    if sparse.issparse(X):\n",
    "        return X.tocsr().astype(np.float32)\n",
    "    return np.asarray(X, dtype=np.float32)\n",
    "\n",
    "X_tr_ = _to_ideal_format(X_tr_)\n",
    "X_va_ = _to_ideal_format(X_va_)\n",
    "X_te_ = _to_ideal_format(X_te_)\n",
    "\n",
    "# 4) y en int8\n",
    "assert 'y_tr' in globals() and 'y_va' in globals() and 'y_te' in globals(), \\\n",
    "    \"Il faut que y_tr/y_va/y_te existent (créés lors du split).\"\n",
    "y_tr = np.asarray(y_tr, dtype=np.int8)\n",
    "y_va = np.asarray(y_va, dtype=np.int8)\n",
    "y_te = np.asarray(y_te, dtype=np.int8)\n",
    "\n",
    "# 5) Logs utiles\n",
    "def _dens(X):\n",
    "    if sparse.issparse(X):\n",
    "        return X.nnz / (X.shape[0]*X.shape[1])\n",
    "    return np.count_nonzero(X) / (X.size + 1e-9)\n",
    "\n",
    "print(f\"[SHAPES] train {X_tr_.shape} | val {X_va_.shape} | test {X_te_.shape}\")\n",
    "print(f\"[DENSITY] train {(_dens(X_tr_))*100:.3f}% | val {(_dens(X_va_))*100:.3f}% | test {(_dens(X_te_))*100:.3f}%\")\n",
    "print(\"[DONE] X_tr_, X_va_, X_te_ prêts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fecb61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid] best={'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'} | best AP(cv)=0.9838 | time=21812.6s\n",
      "[Grid] AP(val) non calibré = 0.9832\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODELS_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Grid] AP(val) non calibré = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00map_va_gs\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 4) Enregistrer la grille (preuves C5.2.4)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m joblib.dump(gs, \u001b[43mMODELS_DIR\u001b[49m/\u001b[33m\"\u001b[39m\u001b[33mgrid_sentiment_logreg.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved:\u001b[39m\u001b[33m\"\u001b[39m, (MODELS_DIR/\u001b[33m\"\u001b[39m\u001b[33mgrid_sentiment_logreg.joblib\u001b[39m\u001b[33m\"\u001b[39m).as_posix())\n",
      "\u001b[31mNameError\u001b[39m: name 'MODELS_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# === C5.2.4 — GRIDSEARCH ÉLARGI (sur tes features existantes X_tr_, X_va_) ===\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "import numpy as np, pandas as pd, time, joblib\n",
    "from scipy import sparse\n",
    "\n",
    "# 0) sécurité formats mémoire (tu l'avais déjà fait mais on blind)\n",
    "X_tr_ = X_tr_.tocsr().astype(np.float32)\n",
    "X_va_ = X_va_.tocsr().astype(np.float32)\n",
    "X_te_ = X_te_.tocsr().astype(np.float32)\n",
    "y_tr  = np.asarray(y_tr, dtype=np.int8)\n",
    "y_va  = np.asarray(y_va, dtype=np.int8)\n",
    "y_te  = np.asarray(y_te, dtype=np.int8)\n",
    "\n",
    "# 1) grille plus large (lbfgs vs saga, C, class_weight)\n",
    "base = LogisticRegression(max_iter=5000, tol=1e-3)\n",
    "param_grid = {\n",
    "    \"solver\": [\"lbfgs\", \"saga\"],         # saga gère des cas + L1, mais on garde l2 pour stabilité\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"C\": [0.1, 0.25, 0.5, 1.0, 2.0, 5.0],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RNG)\n",
    "\n",
    "# 2) GridSearch sur TRAIN uniquement (on garde VAL pour sélection seuil & calibration)\n",
    "gs = GridSearchCV(\n",
    "    estimator=base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"average_precision\",   # cohérent avec ton AP\n",
    "    cv=cv,\n",
    "    n_jobs=1,                      # éviter copies mémoire massives\n",
    "    verbose=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_tr_, y_tr)\n",
    "t_gs = time.time() - t0\n",
    "print(f\"[Grid] best={gs.best_params_} | best AP(cv)={gs.best_score_:.4f} | time={t_gs:.1f}s\")\n",
    "\n",
    "# 3) Éval rapide sur VAL (avant calibration)\n",
    "proba_va_gs = gs.best_estimator_.predict_proba(X_va_)[:,1]\n",
    "ap_va_gs = average_precision_score(y_va, proba_va_gs)\n",
    "print(f\"[Grid] AP(val) non calibré = {ap_va_gs:.4f}\")\n",
    "\n",
    "# 4) Enregistrer la grille (preuves C5.2.4)\n",
    "joblib.dump(gs, MODELS_DIR/\"grid_sentiment_logreg.joblib\")\n",
    "print(\"Saved:\", (MODELS_DIR/\"grid_sentiment_logreg.joblib\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b777b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models/grid_sentiment_logreg.joblib\n"
     ]
    }
   ],
   "source": [
    "# 4) Enregistrer la grille (preuves C5.2.4)\n",
    "joblib.dump(gs, OUT_DIR/\"grid_sentiment_logreg.joblib\")\n",
    "print(\"Saved:\", (OUT_DIR/\"grid_sentiment_logreg.joblib\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16121b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
