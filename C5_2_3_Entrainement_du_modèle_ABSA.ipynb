{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31512487",
   "metadata": {},
   "source": [
    "## C) ABSA (Aspect-Based Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9f0bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell: imports\n",
    "import os, json, math, re\n",
    "import pandas as pd, numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NLP & embeddings\n",
    "import spacy                     \n",
    "from sentence_transformers import SentenceTransformer  \n",
    "import hdbscan                   \n",
    "from transformers import pipeline \n",
    "\n",
    "# utilities\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cc0229",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m candidates\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Assure-toi que la colonne existe, sinon alerte et propose fallback\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TEXT_COL \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m.columns:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColonne \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEXT_COL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m non trouvée. Vérifie les colonnes du DataFrame (voir diagnostic).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Appliquer sur un échantillon si dataset volumineux\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: extraire candidats d'aspects (noun chunks + regex simple)\n",
    "# Extraction candidates — utilise review_body \n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Choix de la colonne texte : adapte ici\n",
    "# Option A : utiliser uniquement le corps\n",
    "TEXT_COL = 'review_body'\n",
    "\n",
    "# Option B : concaténer title + body (souvent utile)\n",
    "# df['text_combined'] = df[['review_title','review_body']].fillna('').agg(' '.join, axis=1)\n",
    "# TEXT_COL = 'text_combined'\n",
    "\n",
    "# Charger spaCy (choisir modèle FR/EN selon ta data)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")   # si anglais\n",
    "except Exception:\n",
    "    try:\n",
    "        nlp = spacy.load(\"fr_core_news_md\")  # si français\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"spaCy model not found. Installe 'en_core_web_sm' ou 'fr_core_news_md' puis relance.\")\n",
    "\n",
    "def extract_candidates_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    candidates = []\n",
    "    for nc in doc.noun_chunks:\n",
    "        tok = nc.text.strip().lower()\n",
    "        # nettoyage basique\n",
    "        tok = re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", tok)\n",
    "        if len(tok) < 2 or len(tok) > 60:\n",
    "            continue\n",
    "        # filtrer tokens très génériques (optionnel)\n",
    "        if tok in {'the product','this product','it','this','product'}:\n",
    "            continue\n",
    "        candidates.append(tok)\n",
    "    return candidates\n",
    "\n",
    "# Assure-toi que la colonne existe, sinon alerte et propose fallback\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise KeyError(f\"Colonne '{TEXT_COL}' non trouvée. Vérifie les colonnes du DataFrame (voir diagnostic).\")\n",
    "\n",
    "# Appliquer sur un échantillon si dataset volumineux\n",
    "sample_df = df.sample(min(2000, len(df)), random_state=42)  # ajuste sample si tu veux tout\n",
    "sample_df['candidates'] = sample_df[TEXT_COL].fillna(\"\").astype(str).apply(lambda s: extract_candidates_spacy(s))\n",
    "\n",
    "# Comptage et affichage des candidats fréquents\n",
    "from collections import Counter\n",
    "cand_counter = Counter([c for lst in sample_df['candidates'] for c in lst])\n",
    "most_common = cand_counter.most_common(200)\n",
    "pd.DataFrame(most_common, columns=['candidate','count']).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892befc7",
   "metadata": {},
   "source": [
    "Principes rapides :\n",
    "\n",
    "Beaucoup de candidats sont du bruit (you, that, they, the, a lot, etc.). Il faut filtrer les pronoms / stopwords et garder les noms / groupes nominaux utiles (battery, screen, sound quality, price, case, camera...).\n",
    "\n",
    "Normaliser : lower(), lemme (ou stemming), retirer articles (the, a), ponctuation.\n",
    "\n",
    "Regrouper variantes proches (sound, sound quality, the sound quality) en une forme canonique (sound quality).\n",
    "\n",
    "Ensuite : SBERT embeddings + clustering (HDBSCAN) pour former clusters sémantiques et créer une table cluster_id -> canonical_aspect.\n",
    "\n",
    "Enfin : mapper chaque review aux clusters / aspects et bootstrap le sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928f9b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'most_common' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# apply cleaning on top candidates\u001b[39;00m\n\u001b[32m     45\u001b[39m top_k = \u001b[32m300\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m top_candidates = [c \u001b[38;5;28;01mfor\u001b[39;00m c,_ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmost_common\u001b[49m[:top_k]]\n\u001b[32m     47\u001b[39m cleaned = []\n\u001b[32m     48\u001b[39m counts = {}\n",
      "\u001b[31mNameError\u001b[39m: name 'most_common' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: nettoyage / canonicalisation des candidats extraits (produit top_candidates puis top_candidates_clean)\n",
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# charge un petit modèle spacy (anglais ici, adapte si FR)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# supposons que most_common est une liste de tuples (candidate, count) issue précédemment\n",
    "# if not present, rebuild from sample_df['candidates'] like before\n",
    "# most_common = cand_counter.most_common(200)\n",
    "\n",
    "# blacklist de tokens évidents à retirer\n",
    "BLACKLIST = {\n",
    "    'you','that','they','me','them','which','we','these','what','all','something','everything',\n",
    "    'amazon','who','anyone','any','some','nothing','both','this one','the one','the product','product',\n",
    "    'it','this','one','there','br'\n",
    "}\n",
    "ARTICLES = re.compile(r'^(the|a|an)\\s+', flags=re.I)\n",
    "\n",
    "def clean_candidate(cand):\n",
    "    s = cand.strip().lower()\n",
    "    s = ARTICLES.sub('', s)                # remove leading article\n",
    "    s = re.sub(r'[^a-z0-9\\s\\-]', '', s)    # keep letters, numbers, spaces, hyphens\n",
    "    s = re.sub(r'\\s{2,}', ' ', s).strip()\n",
    "    # lemma + POS filter: keep noun chunks / nouns\n",
    "    doc = nlp(s)\n",
    "    # if candidate is pronoun or only stopwords, drop\n",
    "    if len(doc)==0: \n",
    "        return None\n",
    "    # if it's a pronoun or stop word entirely -> drop\n",
    "    if all(tok.pos_ in ('PRON','DET','PART','SCONJ','ADP') or tok.is_stop for tok in doc):\n",
    "        return None\n",
    "    # build normalized lemma form for multi-token\n",
    "    lemmas = [tok.lemma_ for tok in doc if not tok.is_stop]\n",
    "    if not lemmas:\n",
    "        lemmas = [tok.lemma_ for tok in doc]\n",
    "    s_norm = \" \".join(lemmas).strip()\n",
    "    # final blacklist check\n",
    "    if s_norm in BLACKLIST or len(s_norm) <= 1:\n",
    "        return None\n",
    "    return s_norm\n",
    "\n",
    "# apply cleaning on top candidates\n",
    "top_k = 300\n",
    "top_candidates = [c for c,_ in most_common[:top_k]]\n",
    "cleaned = []\n",
    "counts = {}\n",
    "for c,count in most_common[:top_k]:\n",
    "    c_clean = clean_candidate(c)\n",
    "    if c_clean is None:\n",
    "        continue\n",
    "    cleaned.append(c_clean)\n",
    "    counts[c_clean] = counts.get(c_clean, 0) + count\n",
    "\n",
    "# deduplicate and sort by aggregated counts\n",
    "cand_counter_clean = Counter(counts)\n",
    "top_candidates_clean = [c for c,_ in cand_counter_clean.most_common(200)]\n",
    "print(\"Top cleaned candidates (sample):\", top_candidates_clean[:40])\n",
    "# prepare list and counts for next step\n",
    "top_candidates_clean_counts = cand_counter_clean.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e5f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top_candidates_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m sbert = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mparaphrase-multilingual-MiniLM-L12-v2\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# ou 'all-mpnet-base-v2' si ressources ok\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# encode candidates\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m cand_list = \u001b[43mtop_candidates_clean\u001b[49m  \u001b[38;5;66;03m# issue du cell précédent\u001b[39;00m\n\u001b[32m     12\u001b[39m embs = sbert.encode(cand_list, convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# HDBSCAN clustering (robuste, pas besoin de fixer nb clusters)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'top_candidates_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: embeddings + HDBSCAN clustering des candidates nettoyés\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# modèle SBERT multi-lingue (rapide & efficace)\n",
    "sbert = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # ou 'all-mpnet-base-v2' si ressources ok\n",
    "\n",
    "# encode candidates\n",
    "cand_list = top_candidates_clean  # issue du cell précédent\n",
    "embs = sbert.encode(cand_list, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# HDBSCAN clustering (robuste, pas besoin de fixer nb clusters)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=6, metric='euclidean', cluster_selection_epsilon=0.0)\n",
    "labels = clusterer.fit_predict(embs)  # -1 = noise\n",
    "\n",
    "# build clusters dict\n",
    "clusters = defaultdict(list)\n",
    "for cand, lab in zip(cand_list, labels):\n",
    "    clusters[lab].append(cand)\n",
    "\n",
    "# summarize clusters (size + top members)\n",
    "cluster_summary = []\n",
    "for lab, items in sorted(clusters.items(), key=lambda x: (-len(x[1]) if x[0]!=-1 else 999, x[0])):\n",
    "    cluster_summary.append({\"cluster\": lab, \"size\": len(items), \"examples\": items[:10]})\n",
    "import pandas as pd\n",
    "display(pd.DataFrame(cluster_summary).head(40))\n",
    "\n",
    "# Save artefacts for manual inspection\n",
    "import joblib, os\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump({\"candidates\": cand_list, \"embeddings\": embs, \"labels\": labels}, \"artifacts/absa_candidates_embeddings.joblib\")\n",
    "print(\"Saved embeddings+labels -> artifacts/absa_candidates_embeddings.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563bcb57",
   "metadata": {},
   "source": [
    "Lecture rapide (pour l’oral)\n",
    "\n",
    "-1 = points non assignés à un cluster.\n",
    "\n",
    "Causes probables : candidats trop bruités (pronoms, mots génériques), embeddings peu discriminants pour ces candidats, ou paramètres HDBSCAN inadaptés (min_cluster_size trop élevé, pas de réduction UMAP préalable).\n",
    "\n",
    "Objectif : rendre HDBSCAN capable d’identifier des noyaux (ou utiliser un fallback comme KMeans si besoin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554d672b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m emb = \u001b[43membs\u001b[49m  \u001b[38;5;66;03m# variable issue de l'étape SBERT\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mshape embeddings:\u001b[39m\u001b[33m\"\u001b[39m, emb.shape)\n\u001b[32m      6\u001b[39m d = pairwise_distances(emb, metric=\u001b[33m\"\u001b[39m\u001b[33mcosine\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embs' is not defined"
     ]
    }
   ],
   "source": [
    "# Diagnostic embeddings\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "emb = embs  # variable issue de l'étape SBERT\n",
    "print(\"shape embeddings:\", emb.shape)\n",
    "d = pairwise_distances(emb, metric=\"cosine\")\n",
    "# distribution des distances moyennes par point\n",
    "mean_d = d.mean(axis=1)\n",
    "import numpy as np\n",
    "print(\"mean distance (mean of means):\", float(mean_d.mean()))\n",
    "print(\"min/median/max mean-distance:\", float(mean_d.min()), float(np.median(mean_d)), float(mean_d.max()))\n",
    "# montrer 5 nearest neighbors exemples pour le 1er candidat\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=6, metric=\"cosine\").fit(emb)\n",
    "dist, idx = nn.kneighbors(emb[:10])\n",
    "for i in range(10):\n",
    "    print(\"cand:\", cand_list[i], \"neighbors:\", [cand_list[j] for j in idx[i]], \"dists:\", dist[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd17a72",
   "metadata": {},
   "source": [
    "Interprétation rapide de tes résultats (Option A)\n",
    "\n",
    "embs.shape = (109, 384) → tu as 109 candidats encodés (après nettoyage).\n",
    "\n",
    "mean distance ≈ 0.676 (median ≈ 0.675) → les embeddings sont plutôt dispersés, pas ultra-denses — normal pour des phrases courtes et candidates hétérogènes.\n",
    "\n",
    "Les voisins proches pour plusieurs candidats sont très sensés (price → cost, good price, camera → photo, picture, sound → sound quality, speaker) — signe que SBERT capture bien la sémantique pour des candidats utiles.\n",
    "\n",
    "Certains candidats ont voisins distants (ex. lot, time) — signes de candidats vagues/génériques qui font du bruit.\n",
    "\n",
    "Conclusion : les embeddings sont ok pour clustering, mais il reste du bruit et HDBSCAN a considéré tout comme noise avec les paramètres précédents (min_cluster_size trop élevé / pas de réduction UMAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb29c60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cand_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Préfiltre : ne garder que candidats contenant NOUN ou PROPN (améliore qualité clustering)\u001b[39;00m\n\u001b[32m      2\u001b[39m top_candidates_filtered = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cand \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcand_list\u001b[49m:\n\u001b[32m      4\u001b[39m     doc = nlp(cand)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tok.pos_ \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mNOUN\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mPROPN\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m doc):\n",
      "\u001b[31mNameError\u001b[39m: name 'cand_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Préfiltre : ne garder que candidats contenant NOUN ou PROPN (améliore qualité clustering)\n",
    "top_candidates_filtered = []\n",
    "for cand in cand_list:\n",
    "    doc = nlp(cand)\n",
    "    if any(tok.pos_ in (\"NOUN\",\"PROPN\") for tok in doc):\n",
    "        top_candidates_filtered.append(cand)\n",
    "print(\"Avant:\", len(cand_list), \"Après filtrage POS:\", len(top_candidates_filtered))\n",
    "# remplace cand_list par top_candidates_filtered pour la suite si ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25dd9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_candidates_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae457d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cand_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m     cand_list_use = top_candidates_filtered\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     cand_list_use = \u001b[43mcand_list\u001b[49m  \u001b[38;5;66;03m# fallback to original list\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cand_list_use), \u001b[33m\"\u001b[39m\u001b[33mcandidates for embedding/clustering.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 2) (Re-)compute embeddings if necessary\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# If 'embs' exists and was computed for exactly cand_list_use, reuse it, else re-encode.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'cand_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Robust UMAP pipeline with fallback to PCA, and HDBSCAN clustering\n",
    "# - Re-encodes candidates if you used top_candidates_filtered\n",
    "# - Tries different imports for UMAP, falls back to PCA if not present\n",
    "# - Runs HDBSCAN on the reduced embeddings\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 1) ensure cand_list and top_candidates_filtered exist\n",
    "if 'top_candidates_filtered' in globals() and len(top_candidates_filtered)>0:\n",
    "    cand_list_use = top_candidates_filtered\n",
    "else:\n",
    "    cand_list_use = cand_list  # fallback to original list\n",
    "\n",
    "print(\"Using\", len(cand_list_use), \"candidates for embedding/clustering.\")\n",
    "\n",
    "# 2) (Re-)compute embeddings if necessary\n",
    "# If 'embs' exists and was computed for exactly cand_list_use, reuse it, else re-encode.\n",
    "recompute = True\n",
    "if 'embs' in globals():\n",
    "    try:\n",
    "        # simple heuristic: if lengths match, assume emb corresponds to cand_list_use\n",
    "        if len(embs) == len(cand_list_use):\n",
    "            recompute = False\n",
    "    except Exception:\n",
    "        recompute = True\n",
    "\n",
    "if recompute:\n",
    "    print(\"Encoding candidates with SBERT (this may take a few seconds)...\")\n",
    "    embs = sbert.encode(cand_list_use, convert_to_numpy=True, show_progress_bar=True)\n",
    "else:\n",
    "    print(\"Reusing existing embeddings (length matches).\")\n",
    "\n",
    "# 3) Try to import UMAP safely, else fallback to PCA\n",
    "UMAP_cls = None\n",
    "try:\n",
    "    # preferred import for umap-learn\n",
    "    from umap import UMAP as UMAP_cls  # type: ignore\n",
    "    print(\"Imported UMAP via `from umap import UMAP`\")\n",
    "    UMAP_cls = UMAP_cls\n",
    "except Exception:\n",
    "    try:\n",
    "        # some installs expose it under umap.umap_\n",
    "        import umap.umap_ as umap_mod\n",
    "        UMAP_cls = umap_mod.UMAP\n",
    "        print(\"Imported UMAP via `umap.umap_.UMAP`\")\n",
    "    except Exception:\n",
    "        UMAP_cls = None\n",
    "        print(\"UMAP (umap-learn) not available in this environment. Will fallback to PCA.\")\n",
    "\n",
    "# 4) create reducer\n",
    "if UMAP_cls is not None:\n",
    "    reducer = UMAP_cls(n_neighbors=15, min_dist=0.0, n_components=5, random_state=42)\n",
    "else:\n",
    "    from sklearn.decomposition import PCA\n",
    "    reducer = PCA(n_components=5)\n",
    "    print(\"Using PCA as reducer (fallback).\")\n",
    "\n",
    "# 5) reduce dims\n",
    "emb_reduced = reducer.fit_transform(embs)\n",
    "print(\"Reduced embeddings shape:\", getattr(emb_reduced, \"shape\", None))\n",
    "\n",
    "# 6) run HDBSCAN (adjust parameters if many -1)\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=4, metric='euclidean', cluster_selection_epsilon=0.0)\n",
    "labels = clusterer.fit_predict(emb_reduced)\n",
    "print(\"HDBSCAN label counts:\", Counter(labels))\n",
    "\n",
    "# 7) show clusters\n",
    "clusters = defaultdict(list)\n",
    "for cand, lab in zip(cand_list_use, labels):\n",
    "    clusters[lab].append(cand)\n",
    "\n",
    "for lab, items in sorted(clusters.items(), key=lambda x: (-len(x[1]) if x[0] != -1 else 999, x[0]))[:30]:\n",
    "    print(\"CLUSTER\", lab, \"size\", len(items), \"->\", items[:10])\n",
    "\n",
    "# 8) save artifacts for inspection\n",
    "import joblib, os\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump({'candidates': cand_list_use, 'embeddings': embs, 'reduced': emb_reduced, 'labels': labels},\n",
    "            \"artifacts/absa_candidates_embs_reduced_labels.joblib\")\n",
    "print(\"Saved artifacts -> artifacts/absa_candidates_embs_reduced_labels.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e3d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates used: 0\n",
      "Encoding with SBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (0,) took 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "umap-learn not found despite installation. Error: Could not find/load shared object file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\llvmlite\\binding\\ffi.py:141\u001b[39m, in \u001b[36m_lib_wrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fntab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# Lazily wraps new functions as they are requested\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'LLVMPY_AddSymbol'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\llvmlite\\binding\\ffi.py:122\u001b[39m, in \u001b[36m_lib_wrapper._load_lib\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _suppress_cleanup_errors(_importlib_resources_path(\n\u001b[32m    120\u001b[39m         \u001b[34m__name__\u001b[39m.rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m],\n\u001b[32m    121\u001b[39m         get_library_name())) \u001b[38;5;28;01mas\u001b[39;00m lib_path:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28mself\u001b[39m._lib_handle = \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# Check that we can look up expected symbols.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\ctypes\\__init__.py:376\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mumap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing UMAP from umap-learn.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\umap\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mumap_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\umap\\umap_.py:29\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcsgraph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mumap\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistances\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdist\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\numba\\__init__.py:73\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m generate_version_info\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types, errors\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\numba\\core\\config.py:17\u001b[39m\n\u001b[32m     14\u001b[39m     _HAVE_YAML = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllvmlite\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbinding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mll\u001b[39;00m\n\u001b[32m     20\u001b[39m IS_WIN32 = sys.platform.startswith(\u001b[33m'\u001b[39m\u001b[33mwin32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\llvmlite\\binding\\__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThings that rely on the LLVM library\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdylib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexecutionengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\llvmlite\\binding\\dylib.py:36\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# FFI\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mffi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLVMPY_AddSymbol\u001b[49m.argtypes = [\n\u001b[32m     37\u001b[39m     c_char_p,\n\u001b[32m     38\u001b[39m     c_void_p,\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     41\u001b[39m ffi.lib.LLVMPY_SearchAddressOfSymbol.argtypes = [c_char_p]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\llvmlite\\binding\\ffi.py:144\u001b[39m, in \u001b[36m_lib_wrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# Lazily wraps new functions as they are requested\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     cfn = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lib\u001b[49m, name)\n\u001b[32m    145\u001b[39m     wrapped = _lib_fn_wrapper(\u001b[38;5;28mself\u001b[39m._lock, cfn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\llvmlite\\binding\\ffi.py:136\u001b[39m, in \u001b[36m_lib_wrapper._lib\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lib_handle:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lib_handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\llvmlite\\binding\\ffi.py:130\u001b[39m, in \u001b[36m_lib_wrapper._load_lib\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# OSError may be raised if the file cannot be opened, or is not\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# a shared library.\u001b[39;00m\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# AttributeError is raised if LLVMPY_GetVersionInfo does not\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# exist.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCould not find/load shared object file\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Could not find/load shared object file",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing UMAP from umap-learn.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mumap-learn not found despite installation. Error: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m     34\u001b[39m reducer = UMAP(n_neighbors=\u001b[32m15\u001b[39m, min_dist=\u001b[32m0.0\u001b[39m, n_components=\u001b[32m5\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     35\u001b[39m emb_reduced = reducer.fit_transform(embs)\n",
      "\u001b[31mRuntimeError\u001b[39m: umap-learn not found despite installation. Error: Could not find/load shared object file"
     ]
    }
   ],
   "source": [
    "# === Re-run: SBERT embeddings (filtered) -> UMAP -> HDBSCAN ===\n",
    "# Assumptions: top_candidates_filtered exists (from POS filtering).\n",
    "# If not, fallback to cand_list. sbert model must already be loaded as `sbert` (else it will be loaded).\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import joblib, os, time\n",
    "import numpy as np\n",
    "\n",
    "# 1) choose candidate list\n",
    "if 'top_candidates_filtered' in globals() and len(top_candidates_filtered) > 0:\n",
    "    cand_list_use = top_candidates_filtered\n",
    "else:\n",
    "    cand_list_use = globals().get('cand_list', [])\n",
    "print(\"Candidates used:\", len(cand_list_use))\n",
    "\n",
    "# 2) re-encode candidates with SBERT (ensure alignment)\n",
    "t0 = time.time()\n",
    "if 'sbert' in globals():\n",
    "    sbert_model = sbert\n",
    "else:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    sbert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # fallback\n",
    "print(\"Encoding with SBERT...\")\n",
    "embs = sbert_model.encode(cand_list_use, convert_to_numpy=True, show_progress_bar=True, batch_size=64)\n",
    "print(\"Embeddings shape:\", embs.shape, \"took {:.1f}s\".format(time.time()-t0))\n",
    "\n",
    "# 3) UMAP reduction (now that umap-learn is installed)\n",
    "try:\n",
    "    from umap import UMAP\n",
    "    print(\"Using UMAP from umap-learn.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"umap-learn not found despite installation. Error: \" + str(e))\n",
    "\n",
    "reducer = UMAP(n_neighbors=15, min_dist=0.0, n_components=5, random_state=42)\n",
    "emb_reduced = reducer.fit_transform(embs)\n",
    "print(\"Reduced embeddings shape:\", emb_reduced.shape)\n",
    "\n",
    "# 4) HDBSCAN clustering (default params; adjust min_cluster_size if many -1)\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=4, metric='euclidean', cluster_selection_epsilon=0.0)\n",
    "labels = clusterer.fit_predict(emb_reduced)\n",
    "\n",
    "print(\"HDBSCAN label counts:\", Counter(labels))\n",
    "\n",
    "# 5) show clusters summary (top clusters first)\n",
    "clusters = defaultdict(list)\n",
    "for cand, lab in zip(cand_list_use, labels):\n",
    "    clusters[lab].append(cand)\n",
    "\n",
    "for lab, items in sorted(clusters.items(), key=lambda x: (-len(x[1]) if x[0] != -1 else 999, x[0]))[:40]:\n",
    "    print(\"CLUSTER\", lab, \"size\", len(items), \"->\", items[:12])\n",
    "\n",
    "# 6) save artifacts for manual inspection\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump({'candidates': cand_list_use, 'embeddings': embs, 'reduced': emb_reduced, 'labels': labels},\n",
    "            \"artifacts/absa_candidates_umap_embs_labels.joblib\")\n",
    "print(\"Saved -> artifacts/absa_candidates_umap_embs_labels.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64d93c",
   "metadata": {},
   "source": [
    "## Interprétation rapide des clusters (UMAP + HDBSCAN)\n",
    "\n",
    "- **Cluster 0 (size 17)** : regroupe des tokens liés au matériel / affichage / batterie (ex. `camera`, `tv`, `screen`, `battery`, `phone`, `laptop`, `tablet`). -> *aspect matériel / device / display / battery*.\n",
    "- **Cluster 2 (size 17)** : regroupe des tokens autour du prix/qualité/audio (ex. `price`, `quality`, `money`, `sound quality`, `speaker`, `performance`). -> *aspect prix / qualité / son*.\n",
    "- **Cluster 1 (size 15)** : contient câbles, problèmes et termes de contexte (ex. `cable`, `problem`, `home`, `place`) — plus hétérogène, nécessite nettoyage manuel.\n",
    "- **Cluster -1 (noise, size 43)** : beaucoup de tokens génériques (`case`, `time`, `lot`, `box`, `thing`, `photo`) — soit on blacklist ces tokens, soit on ré-affine le clustering.\n",
    "\n",
    "Conclusion : on a plusieurs clusters exploitables (0 & 2). Les tokens en `-1` contiennent plusieurs aspects importants (ex. `battery`, `price`, `screen`) qui pourraient encore être récupérés via ajustement d'hyperparamètres ou par mapping manuel. Prochaine étape : créer un mapping provisoire `cluster -> canonical_aspect`, appliquer aux reviews, exporter un CSV pour validation humaine, puis bootstraper le sentiment sur les spans extraits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec0787",
   "metadata": {},
   "source": [
    "# UMAP + HDBSCAN — guide synthétique (à coller)\n",
    "\n",
    "## Qu’est-ce que c’est ?\n",
    "- **UMAP** (Uniform Manifold Approximation and Projection) réduit la dimensionnalité des embeddings en préservant la structure locale : il rapproche dans l’espace réduit les points sémantiquement proches.  \n",
    "- **HDBSCAN** (Hierarchical Density-Based Spatial Clustering) identifie des **noyaux de densité** (clusters) dans cet espace et marque les points non-asssignables comme `-1` (noise). Il ne nécessite pas de nombre de clusters fixé à l’avance.\n",
    "\n",
    "## Pourquoi les utiliser ensemble ?\n",
    "UMAP **densifie** l’espace sémantique (regroupe voisins similaires), puis HDBSCAN **découvre automatiquement** les groupes denses correspondant souvent à des aspects (ex. `battery`, `screen`, `price`) tout en filtrant le bruit.\n",
    "\n",
    "## Paramètres clés (valeurs de départ recommandées)\n",
    "- **UMAP**\n",
    "  - `n_neighbors=15` (taille du voisinage local)  \n",
    "  - `min_dist=0.0` (permet de rapprocher fortement les voisins)  \n",
    "  - `n_components=5` (2 pour visualisation, 5 pour clustering stable)\n",
    "- **HDBSCAN**\n",
    "  - `min_cluster_size=4` (tester 2–6 si besoin)  \n",
    "  - `cluster_selection_epsilon=0.0` (augmenter à 0.05–0.2 pour fusionner clusters proches)  \n",
    "  - `metric='euclidean'` (après UMAP)\n",
    "\n",
    "## Checklist pratique pour ABSA\n",
    "1. **Nettoyage** des candidats (filtrer pronoms/stopwords, normaliser, garder NOUN/PROPN).  \n",
    "2. **Encodage** SBERT (`paraphrase-multilingual-MiniLM-L12-v2` recommandé).  \n",
    "3. **Réduction** UMAP (paramètres ci-dessus). Si UMAP pas dispo → **PCA** (n_components=5) en fallback.  \n",
    "4. **Clustering** HDBSCAN (mine de paramètres ci-dessus).  \n",
    "5. **Inspection** manuelle des clusters → créer mapping `cluster_id → canonical_aspect`.  \n",
    "6. **Application** du mapping aux reviews, extraction de spans et bootstrap du sentiment (pipeline HF) + annotation humaine.\n",
    "\n",
    "## Comment interpréter les résultats\n",
    "- Vérifier `Counter(labels)` :  \n",
    "  - beaucoup de `-1` → bruit / paramètres trop stricts → nettoyer plus ou assouplir `min_cluster_size`.  \n",
    "  - clusters cohérents → inspecter membres pour définir un nom canonique.  \n",
    "- Si aspects attendus restent en `-1` → tester `min_cluster_size=2–3`, augmenter `n_neighbors` de UMAP, ou utiliser KMeans (fallback) pour forcer des groupes à inspecter.\n",
    "\n",
    "## Points forts & limites\n",
    "- **Avantages** : pas besoin de fixer *k*, HDBSCAN gère le bruit, UMAP améliore la séparation sémantique.  \n",
    "- **Limites** : sensibles au prétraitement et aux hyperparamètres ; si candidats très bruités, HDBSCAN peut renvoyer beaucoup de `-1`.\n",
    "\n",
    "## Phrase courte pour l’oral\n",
    "« On rapproche d’abord les termes similaires avec UMAP, puis HDBSCAN identifie automatiquement les aspects comme régions denses et filtre le bruit — on ajuste ensuite les seuils ou on nettoie les candidats selon la qualité des clusters. »\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec0f8cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Impossible : variables 'labels' et 'cand_list_use' doivent exister (relancer clustering si besoin).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m cand_list_now = \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mcand_list_use\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mtop_candidates_filtered\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mcand_list\u001b[39m\u001b[33m'\u001b[39m)))\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels_used \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m cand_list_now \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mImpossible : variables \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m\u001b[33m et \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcand_list_use\u001b[39m\u001b[33m'\u001b[39m\u001b[33m doivent exister (relancer clustering si besoin).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Optionnel : counts par candidate si tu as top_candidates_clean_counts\u001b[39;00m\n\u001b[32m     14\u001b[39m freq_map = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mtop_candidates_clean_counts\u001b[39m\u001b[33m'\u001b[39m, [])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtop_candidates_clean_counts\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[31mRuntimeError\u001b[39m: Impossible : variables 'labels' et 'cand_list_use' doivent exister (relancer clustering si besoin)."
     ]
    }
   ],
   "source": [
    "# Cell: construire mapping automatique (most-frequent per cluster), appliquer aux reviews, sauvegarder CSV\n",
    "import joblib, os\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "# variables attendues par cette cellule : cand_list_use (liste candidats), labels (ou labels2 si tu as relancé permissive)\n",
    "labels_used = globals().get('labels', globals().get('labels2'))\n",
    "cand_list_now = globals().get('cand_list_use', globals().get('top_candidates_filtered', globals().get('cand_list')))\n",
    "\n",
    "if labels_used is None or cand_list_now is None:\n",
    "    raise RuntimeError(\"Impossible : variables 'labels' et 'cand_list_use' doivent exister (relancer clustering si besoin).\")\n",
    "\n",
    "# Optionnel : counts par candidate si tu as top_candidates_clean_counts\n",
    "freq_map = dict(globals().get('top_candidates_clean_counts', [])) if 'top_candidates_clean_counts' in globals() else {}\n",
    "# build cluster -> candidates\n",
    "cluster_map = defaultdict(list)\n",
    "for cand, lab in zip(cand_list_now, labels_used):\n",
    "    cluster_map[lab].append(cand)\n",
    "\n",
    "# build canonical name = most common token in cluster (fallback: first)\n",
    "auto_mapping = {}\n",
    "for lab, items in cluster_map.items():\n",
    "    if lab == -1:\n",
    "        continue\n",
    "    # choose candidate with highest overall frequency if available, else first\n",
    "    if freq_map:\n",
    "        candidate_freqs = {cand: freq_map.get(cand, 1) for cand in items}\n",
    "        canonical = sorted(candidate_freqs.items(), key=lambda x: -x[1])[0][0]\n",
    "    else:\n",
    "        canonical = items[0]\n",
    "    # simple normalization (no spaces leading/trailing)\n",
    "    canonical = str(canonical).strip().lower()\n",
    "    auto_mapping[int(lab)] = canonical\n",
    "\n",
    "print(\"Mapping automatique (provisoire) :\")\n",
    "for lab, name in auto_mapping.items():\n",
    "    print(f\"  {lab} -> {name}\")\n",
    "\n",
    "# Apply mapping to dataframe by substring match (case-insensitive)\n",
    "TEXT_COL = globals().get('TEXT_COL', 'review_body')  # adapte si tu as utilisé text_combined\n",
    "def map_review_to_auto_aspects(text):\n",
    "    text_l = str(text).lower()\n",
    "    found = set()\n",
    "    for cand, lab in zip(cand_list_now, labels_used):\n",
    "        if int(lab) == -1: \n",
    "            continue\n",
    "        if cand in text_l:\n",
    "            found.add(auto_mapping.get(int(lab)))\n",
    "    return sorted([a for a in found if a is not None])\n",
    "\n",
    "# create column and aggregate\n",
    "df['absa_aspects_auto'] = df[TEXT_COL].fillna(\"\").astype(str).apply(map_review_to_auto_aspects)\n",
    "agg = df['absa_aspects_auto'].explode().value_counts().dropna()\n",
    "print(\"\\nTop auto-aspects found (sample):\")\n",
    "display(agg.head(30))\n",
    "\n",
    "# Save artifacts for manual validation\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump({'auto_mapping': auto_mapping, 'cluster_map': dict(cluster_map), 'labels_used': list(set(labels_used))},\n",
    "            \"artifacts/absa_auto_mapping.joblib\")\n",
    "df[['review_body', 'review_title', TEXT_COL, 'absa_aspects_auto']].to_csv(\"artifacts/absa_reviews_with_auto_aspects.csv\", index=False)\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\" - artifacts/absa_auto_mapping.joblib\")\n",
    "print(\" - artifacts/absa_reviews_with_auto_aspects.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f006e3",
   "metadata": {},
   "source": [
    "# Pourquoi les counts sont si grands ?\n",
    "Le mapping automatique actuel fait du **substring matching** simple : si la chaîne `'camera'` apparaît quelque part dans le texte, on compte la review pour l’aspect `camera`.  \n",
    "Problèmes courants :\n",
    "- matching dans des mots plus longs (ex. `scam camera?`), ou parties de mots ;  \n",
    "- mentions hors-contexte (ex. `I bought a camera for my friend` → peut-être OK, mais parfois `camera` apparait dans une phrase générique) ;  \n",
    "- multi-occurrences et doublons non contrôlés.  \n",
    "\n",
    "Solution : utiliser un matching *plus strict* (mots entiers / PhraseMatcher), capturer la phrase contenant l’expression, échantillonner les résultats pour validation humaine, puis ajuster la règle si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d1eecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Precompile patterns per candidate to avoid repeated cost\u001b[39;00m\n\u001b[32m     11\u001b[39m patterns = []\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cand, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_list_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_used\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(lab) == -\u001b[32m1\u001b[39m: \n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 1) Matching strict par regex (mots entiers) — remplace le substring naïf\n",
    "# Regex whole-word matching (utiliser \\b pour délimiter mots)\n",
    "import re\n",
    "\n",
    "# auto_mapping = {lab: canonical} existant\n",
    "mapping = globals().get('auto_mapping', {})  # {lab: 'price', ...}\n",
    "labels_used = globals().get('labels', globals().get('labels2'))\n",
    "cand_list_now = globals().get('cand_list_use', globals().get('top_candidates_filtered', globals().get('cand_list')))\n",
    "\n",
    "# Precompile patterns per candidate to avoid repeated cost\n",
    "patterns = []\n",
    "for cand, lab in zip(cand_list_now, labels_used):\n",
    "    if int(lab) == -1: \n",
    "        continue\n",
    "    # escape and require word boundaries\n",
    "    pat = re.compile(r'\\b' + re.escape(cand) + r'\\b', flags=re.IGNORECASE)\n",
    "    patterns.append((cand, int(lab), pat))\n",
    "\n",
    "def map_review_regex(text):\n",
    "    text = str(text)\n",
    "    found = set()\n",
    "    for cand, lab, pat in patterns:\n",
    "        if pat.search(text):\n",
    "            found.add(mapping.get(int(lab)))\n",
    "    return sorted([f for f in found if f is not None])\n",
    "\n",
    "# apply (on a sample if dataset large)\n",
    "df['absa_aspects_regex'] = df['review_body'].fillna(\"\").astype(str).apply(map_review_regex)\n",
    "df['absa_aspects_regex'].explode().value_counts().head(40)\n",
    "\n",
    "# Regex whole-word matching (utiliser \\b pour délimiter mots)\n",
    "import re\n",
    "\n",
    "# auto_mapping = {lab: canonical} existant\n",
    "mapping = globals().get('auto_mapping', {})  # {lab: 'price', ...}\n",
    "labels_used = globals().get('labels', globals().get('labels2'))\n",
    "cand_list_now = globals().get('cand_list_use', globals().get('top_candidates_filtered', globals().get('cand_list')))\n",
    "\n",
    "# Precompile patterns per candidate to avoid repeated cost\n",
    "patterns = []\n",
    "for cand, lab in zip(cand_list_now, labels_used):\n",
    "    if int(lab) == -1: \n",
    "        continue\n",
    "    # escape and require word boundaries\n",
    "    pat = re.compile(r'\\b' + re.escape(cand) + r'\\b', flags=re.IGNORECASE)\n",
    "    patterns.append((cand, int(lab), pat))\n",
    "\n",
    "def map_review_regex(text):\n",
    "    text = str(text)\n",
    "    found = set()\n",
    "    for cand, lab, pat in patterns:\n",
    "        if pat.search(text):\n",
    "            found.add(mapping.get(int(lab)))\n",
    "    return sorted([f for f in found if f is not None])\n",
    "\n",
    "# apply (on a sample if dataset large)\n",
    "df['absa_aspects_regex'] = df['review_body'].fillna(\"\").astype(str).apply(map_review_regex)\n",
    "df['absa_aspects_regex'].explode().value_counts().head(40)\n",
    "\n",
    "# Regex whole-word matching (utiliser \\b pour délimiter mots)\n",
    "import re\n",
    "\n",
    "# auto_mapping = {lab: canonical} existant\n",
    "mapping = globals().get('auto_mapping', {})  # {lab: 'price', ...}\n",
    "labels_used = globals().get('labels', globals().get('labels2'))\n",
    "cand_list_now = globals().get('cand_list_use', globals().get('top_candidates_filtered', globals().get('cand_list')))\n",
    "\n",
    "# Precompile patterns per candidate to avoid repeated cost\n",
    "patterns = []\n",
    "for cand, lab in zip(cand_list_now, labels_used):\n",
    "    if int(lab) == -1: \n",
    "        continue\n",
    "    # escape and require word boundaries\n",
    "    pat = re.compile(r'\\b' + re.escape(cand) + r'\\b', flags=re.IGNORECASE)\n",
    "    patterns.append((cand, int(lab), pat))\n",
    "\n",
    "def map_review_regex(text):\n",
    "    text = str(text)\n",
    "    found = set()\n",
    "    for cand, lab, pat in patterns:\n",
    "        if pat.search(text):\n",
    "            found.add(mapping.get(int(lab)))\n",
    "    return sorted([f for f in found if f is not None])\n",
    "\n",
    "# apply (on a sample if dataset large)\n",
    "df['absa_aspects_regex'] = df['review_body'].fillna(\"\").astype(str).apply(map_review_regex)\n",
    "df['absa_aspects_regex'].explode().value_counts().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8325f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Build patterns per canonical aspect (group synonyms first)\u001b[39;00m\n\u001b[32m      9\u001b[39m cluster_to_cands = {}\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cand, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_list_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_used\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     11\u001b[39m     lab = \u001b[38;5;28mint\u001b[39m(lab)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lab == -\u001b[32m1\u001b[39m: \n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 2) PhraseMatcher spaCy (robuste pour multi-word phrases + capture de spans)\n",
    "# PhraseMatcher approach (recommended) — handles multi-word and gives spans\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # ou fr_core_news_md si FR\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "# Build patterns per canonical aspect (group synonyms first)\n",
    "cluster_to_cands = {}\n",
    "for cand, lab in zip(cand_list_now, labels_used):\n",
    "    lab = int(lab)\n",
    "    if lab == -1: \n",
    "        continue\n",
    "    cluster_to_cands.setdefault(lab, []).append(cand)\n",
    "\n",
    "# create PhraseMatcher entries using synonyms (candidates) per cluster\n",
    "for lab, cands in cluster_to_cands.items():\n",
    "    patterns = [nlp.make_doc(c) for c in cands]\n",
    "    matcher.add(f\"CL_{lab}\", patterns)\n",
    "\n",
    "def map_and_spans(text):\n",
    "    doc = nlp(text)\n",
    "    found = {}\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        label = nlp.vocab.strings[match_id]  # \"CL_2\"\n",
    "        lab = int(label.split(\"_\",1)[1])\n",
    "        canonical = mapping.get(lab)\n",
    "        span_text = doc[start:end].text\n",
    "        # store set of spans per canonical aspect\n",
    "        found.setdefault(canonical, []).append(span_text)\n",
    "    return found\n",
    "\n",
    "# Apply on a small sample first (performance)\n",
    "sample = df.sample(min(5000, len(df)), random_state=42)\n",
    "sample['absa_spans'] = sample['review_body'].fillna(\"\").astype(str).apply(map_and_spans)\n",
    "sample[['review_body','absa_spans']].head(20)\n",
    "# When happy, apply on full df:\n",
    "# df['absa_spans'] = df['review_body'].fillna(\"\").astype(str).apply(map_and_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c6742b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m rows = []\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# mapping: auto_mapping {lab: canonical} ou mapping variable que tu as\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m canons = \u001b[38;5;28mlist\u001b[39m(auto_mapping.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mauto_mapping\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m([a \u001b[38;5;28;01mfor\u001b[39;00m lst \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mabsa_aspects_regex\u001b[39m\u001b[33m'\u001b[39m].dropna() \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m (lst \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lst, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [lst])]))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# iterate per aspect and sample\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m asp \u001b[38;5;129;01min\u001b[39;00m canons:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# select rows where aspect appears (PhraseMatcher output may be in column 'absa_spans' as dict)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Sauver un échantillon pour annotation manuelle (up to 100 ex par aspect)\n",
    "import pandas as pd, random, os\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# Utilise la colonne produite par PhraseMatcher qui contient les spans dict: sample['absa_spans']\n",
    "# On prendra un échantillon sur les reviews où cet aspect a été trouvé.\n",
    "SAMPLE_PER_ASP = 100\n",
    "rows = []\n",
    "# mapping: auto_mapping {lab: canonical} ou mapping variable que tu as\n",
    "canons = list(auto_mapping.values()) if 'auto_mapping' in globals() else sorted(set([a for lst in df['absa_aspects_regex'].dropna() for a in (lst if isinstance(lst, list) else [lst])]))\n",
    "\n",
    "# iterate per aspect and sample\n",
    "for asp in canons:\n",
    "    # select rows where aspect appears (PhraseMatcher output may be in column 'absa_spans' as dict)\n",
    "    sel = df[df.get('absa_spans', pd.Series()).apply(lambda d: isinstance(d, dict) and asp in d)]\n",
    "    if sel.shape[0] == 0:\n",
    "        # fallback: column absa_aspects_regex or absa_aspects_auto\n",
    "        for col in ['absa_aspects_regex','absa_aspects_auto','absa_aspects_refined','absa_aspects_strict']:\n",
    "            if col in df.columns:\n",
    "                sel = df[df[col].apply(lambda lst: asp in lst if isinstance(lst, (list, set)) else False)]\n",
    "                if sel.shape[0]>0:\n",
    "                    break\n",
    "    if sel.shape[0] == 0:\n",
    "        continue\n",
    "    n = min(SAMPLE_PER_ASP, len(sel))\n",
    "    sample_rows = sel.sample(n, random_state=42)\n",
    "    for _, r in sample_rows.iterrows():\n",
    "        spans = r.get('absa_spans', {}) or {}\n",
    "        # take first span if available\n",
    "        span_examples = spans.get(asp, []) if isinstance(spans, dict) else []\n",
    "        span_text = span_examples[0] if span_examples else \"\"\n",
    "        # get sentence containing span (if you have nlp loaded)\n",
    "        sent = span_text\n",
    "        if span_text and 'nlp' in globals():\n",
    "            doc = nlp(r['review_body'])\n",
    "            for s in doc.sents:\n",
    "                if span_text in s.text:\n",
    "                    sent = s.text.strip()\n",
    "                    break\n",
    "        rows.append({\n",
    "            \"aspect\": asp,\n",
    "            \"span\": span_text,\n",
    "            \"sentence\": sent,\n",
    "            \"review_body\": r['review_body'],\n",
    "            \"review_id\": r.name\n",
    "        })\n",
    "\n",
    "# save CSV to annotate (label column to be added manually: 1=correct, 0=incorrect)\n",
    "df_qc = pd.DataFrame(rows)\n",
    "df_qc.to_csv(\"artifacts/absa_manual_qc_sample.csv\", index=False)\n",
    "print(\"Saved artifacts/absa_manual_qc_sample.csv with\", len(df_qc), \"rows. Annotate column 'label' (1=correct,0=wrong).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b091c7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '’' (U+2019) (4290730335.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mMaintenant l’objectif pratique est de valider la qualité (estimer la précision), corriger / canonicaliser le mapping, puis bootstrapper des labels de sentiment par aspect pour créer un jeu d’entraînement ABSA.\u001b[39m\n                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '’' (U+2019)\n"
     ]
    }
   ],
   "source": [
    "Maintenant l’objectif pratique est de valider la qualité (estimer la précision), corriger / canonicaliser le mapping, puis bootstrapper des labels de sentiment par aspect pour créer un jeu d’entraînement ABSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "494ed07f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# TEXT_COL detection\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m TEXT_COL = \u001b[33m'\u001b[39m\u001b[33mreview_body\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mreview_body\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m.columns \u001b[38;5;28;01melse\u001b[39;00m df.columns[\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing TEXT_COL =\u001b[39m\u001b[33m\"\u001b[39m, TEXT_COL)\n\u001b[32m     11\u001b[39m span_col = \u001b[33m'\u001b[39m\u001b[33mabsa_spans\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mabsa_spans\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# 1) Exporter un échantillon pour annotation humaine (100 ex. / aspect)\n",
    "# === générer artifacts/absa_manual_qc_sample.csv ===\n",
    "import pandas as pd, os, re, random, numpy as np\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "random.seed(42)\n",
    "\n",
    "# TEXT_COL detection\n",
    "TEXT_COL = 'review_body' if 'review_body' in df.columns else df.columns[0]\n",
    "print(\"Using TEXT_COL =\", TEXT_COL)\n",
    "\n",
    "span_col = 'absa_spans' if 'absa_spans' in df.columns else None\n",
    "list_aspect_cols = [c for c in ['absa_aspects_regex','absa_aspects_auto','absa_aspects_refined','absa_aspects_strict','absa_aspects'] if c in df.columns]\n",
    "print(\"span_col:\", span_col, \"list_aspect_cols:\", list_aspect_cols)\n",
    "\n",
    "# --- robust membership tester ---\n",
    "def contains_aspect(v, asp):\n",
    "    \"\"\"\n",
    "    Return True if aspect `asp` appears in value `v`.\n",
    "    Handles: None/NaN, dict (keys), list/tuple/set/np.ndarray, str, other (fallback str).\n",
    "    Uses whole-word regex for string matching.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if v is None:\n",
    "            return False\n",
    "        if isinstance(v, float) and np.isnan(v):\n",
    "            return False\n",
    "        if isinstance(v, dict):\n",
    "            # check keys (common pattern for spans dict)\n",
    "            return asp in v\n",
    "        if isinstance(v, (list, tuple, set, np.ndarray)):\n",
    "            try:\n",
    "                return asp in v\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return asp in list(v)\n",
    "                except Exception:\n",
    "                    return False\n",
    "        if isinstance(v, str):\n",
    "            return bool(re.search(r'\\b' + re.escape(str(asp)) + r'\\b', v, flags=re.IGNORECASE))\n",
    "        # fallback: convert to string\n",
    "        return str(asp).lower() in str(v).lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# determine canonical aspects (canons)\n",
    "if 'auto_mapping' in globals() and auto_mapping:\n",
    "    canons = sorted(set(auto_mapping.values()))\n",
    "else:\n",
    "    s=set()\n",
    "    for col in list_aspect_cols:\n",
    "        # sample to inspect types and values\n",
    "        for v in df[col].dropna().head(20000):\n",
    "            if isinstance(v, (list, tuple, set)):\n",
    "                s.update(v)\n",
    "            elif isinstance(v, dict):\n",
    "                s.update(list(v.keys()))\n",
    "            elif isinstance(v, str):\n",
    "                if v.strip().startswith('[') and len(v) < 500:\n",
    "                    try:\n",
    "                        import ast\n",
    "                        parsed = ast.literal_eval(v)\n",
    "                        if isinstance(parsed, (list,tuple,set)):\n",
    "                            s.update(parsed)\n",
    "                            continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                s.add(v)\n",
    "    canons = sorted(x for x in s if x)\n",
    "\n",
    "print(\"Aspects to sample (example):\", canons[:20])\n",
    "if not canons:\n",
    "    raise RuntimeError(\"Aucun aspect trouvé pour sampling. Vérifie auto_mapping ou colonnes d'aspects.\")\n",
    "\n",
    "# sampling loop (build index set safely)\n",
    "SAMPLE_PER_ASP = 100\n",
    "rows = []\n",
    "\n",
    "for asp in canons:\n",
    "    idxs = set()\n",
    "\n",
    "    # 1) spans dict column if present (fast)\n",
    "    if span_col:\n",
    "        mask = df[span_col].apply(lambda d: isinstance(d, dict) and (asp in d) if pd.notnull(d) else False)\n",
    "        idxs.update(df.index[mask].tolist())\n",
    "\n",
    "    # 2) search in list-like / string aspect columns using contains_aspect\n",
    "    for col in list_aspect_cols:\n",
    "        # avoid evaluating ambiguous arrays directly: use contains_aspect for each cell\n",
    "        try:\n",
    "            mask_idx = df.index[df[col].apply(lambda v: contains_aspect(v, asp))]\n",
    "            idxs.update(mask_idx.tolist())\n",
    "        except Exception:\n",
    "            # fallback to safe string-based search if apply fails\n",
    "            try:\n",
    "                mask = df[col].astype(str).str.contains(re.escape(str(asp)), na=False, case=False)\n",
    "                idxs.update(df.index[mask].tolist())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if not idxs:\n",
    "        continue\n",
    "\n",
    "    # sample indices\n",
    "    idxs_list = list(idxs)\n",
    "    if len(idxs_list) > SAMPLE_PER_ASP:\n",
    "        idxs_sampled = random.sample(idxs_list, SAMPLE_PER_ASP)\n",
    "    else:\n",
    "        idxs_sampled = idxs_list\n",
    "\n",
    "    # build sample rows\n",
    "    for idx in idxs_sampled:\n",
    "        r = df.loc[idx]\n",
    "        spans = {}\n",
    "        if span_col and isinstance(r.get(span_col), dict):\n",
    "            spans = r.get(span_col) or {}\n",
    "        span_list = spans.get(asp, []) if isinstance(spans, dict) else []\n",
    "        span_text = span_list[0] if span_list else \"\"\n",
    "        sent = span_text\n",
    "        if span_text and 'nlp' in globals():\n",
    "            try:\n",
    "                doc = nlp(r[TEXT_COL])\n",
    "                for s in doc.sents:\n",
    "                    if span_text in s.text:\n",
    "                        sent = s.text.strip()\n",
    "                        break\n",
    "            except Exception:\n",
    "                sent = span_text\n",
    "        rows.append({\n",
    "            \"aspect\": asp,\n",
    "            \"span\": span_text,\n",
    "            \"sentence\": sent,\n",
    "            \"review\": r[TEXT_COL],\n",
    "            \"review_id\": idx\n",
    "        })\n",
    "\n",
    "# save result\n",
    "df_qc = pd.DataFrame(rows)\n",
    "out_path = \"artifacts/absa_manual_qc_sample.csv\"\n",
    "df_qc.to_csv(out_path, index=False)\n",
    "print(f\"Saved {out_path} with {len(df_qc)} rows.\")\n",
    "display(df_qc.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78cd941",
   "metadata": {},
   "source": [
    "# Résultats intermédiaires — extraction d'aspects (ABSA) et justification\n",
    "\n",
    "## Résumé des résultats observés\n",
    "- **Aspects détectés automatiquement** (exemples) : `cable`, `camera`, `price`.  \n",
    "- **Échantillon exporté pour QA** : `artifacts/absa_manual_qc_sample.csv` — chaque ligne contient l’`aspect` prédit, le `span` détecté (si présent), la `sentence` extraite (contexte) et la `review`.  \n",
    "- Aperçu (extrait) : beaucoup d’exemples pour `cable` avec `span` parfois vide (le span n’est pas toujours identifié si la détection provient d’un index listé plutôt que d’un span exact).\n",
    "\n",
    "## Pourquoi on a procédé ainsi (méthodologie & motivation)\n",
    "1. **But** : passer d’une simple détection de sentiment global (pos/neg) à une extraction d’**aspects** (ex. prix, batterie, écran, son) puis à une annotation de sentiment *par aspect*.  \n",
    "2. **Pipeline choisi** :\n",
    "   - extraire des *candidats* (n-grams / chunks) à partir des reviews ;  \n",
    "   - encoder ces candidats (SBERT) → réduire la dimension (UMAP) → clusteriser (HDBSCAN) pour obtenir groupes d’aspects naturels ;  \n",
    "   - construire un mapping `cluster -> canonical_aspect` automatique (fréquence ou représentant du cluster) ;  \n",
    "   - **appliquer un matching strict** (PhraseMatcher / regex mot-entier) pour repérer les mentions dans les reviews et extraire la phrase-contexte.\n",
    "3. **Pourquoi matching strict / PhraseMatcher** :\n",
    "   - l’approche par simple substring produisait des *comptages énormes* et beaucoup de faux-positifs ;  \n",
    "   - PhraseMatcher + mot-entier réduit les faux positifs (mots inclus dans d’autres mots, pronoms, etc.) et permet d’extraire le **span exact** pour contextualiser la mention.\n",
    "\n",
    "## Interprétation du tableau d’exemple\n",
    "- Les lignes montrent des reviews où l’aspect `cable` a été détecté.  \n",
    "- `span` vide ⇢ il y a plusieurs causes possibles :\n",
    "  - le matching a été fait sur une colonne liste (aspects détectés globalement) mais aucun span précis n’a été extrait ;  \n",
    "  - PhraseMatcher n’a pas trouvé la phrase exacte (par ex. tokenisation différente) ;  \n",
    "  - ou le token apparaissait dans une forme non couverte par les patterns (abréviations, erreurs OCR, balises HTML).\n",
    "- **Conséquence pratique** : les lignes avec `sentence` vide nécessitent plus d’attention (manuellement ou par règles supplémentaires) pour vérifier la qualité.\n",
    "\n",
    "## Objectif immédiat (raison de l’échantillonnage)\n",
    "- **Contrôle qualité** : annoter manuellement un échantillon (≈100 ex / aspect) pour estimer la *précision* (TP / prédits).  \n",
    "- **Décision opérationnelle** :\n",
    "  - Si précision ≥ ~0.8 → on applique le mapping sur tout le dataset et on bootstrappe des pseudo-labels de sentiment.  \n",
    "  - Si précision 0.5–0.8 → améliorer les règles (POS-check, PhraseMatcher enrichi, two-hits, blacklist), recluster si nécessaire.  \n",
    "  - Si précision < 0.5 → revoir la génération de candidats et le clustering.\n",
    "\n",
    "## Prochaine étape (pipeline recommandé)\n",
    "1. **Annoter** `artifacts/absa_manual_qc_sample.csv` (colonne `label`: 1 correct / 0 faux-positif).  \n",
    "2. Calculer la **precision_est** par aspect (script fourni).  \n",
    "3. Si satisfait : extraire toutes les phrases (contexte) pour chaque aspect, lancer un pipeline `sentiment-analysis` (bootstrap) et créer un dataset pseudo-étiqueté.  \n",
    "4. Entraîner un modèle ABSA (ex. SBERT → LR / small classifier) sur ces pseudo-labels, puis itérer avec annotation humaine sur les erreurs les plus fréquentes (active learning).\n",
    "\n",
    "## Limitations et points d’attention\n",
    "- Les méthodes automatiques produisent des **bruits** : il faut valider par humain avant d’entraîner un modèle final.  \n",
    "- Les tokens vagues (ex. `thing`, `item`, pronoms) sont souvent regroupés dans le bruit ; il faut les **blacklister** ou les retraiter.  \n",
    "- L’extraction de spans dépend fortement de la qualité du `PhraseMatcher` et des variantes textuelles (HTML, fautes, majuscules).\n",
    "\n",
    "## Phrases courtes pour la soutenance (oral)\n",
    "- « Nous utilisons d’abord SBERT → UMAP → HDBSCAN pour découvrir des groupes d’aspects sans supervision. Ensuite, un matching strict (PhraseMatcher / regex) permet d’extraire le span et la phrase-contexte. Enfin, nous validons par échantillonnage humain avant de générer des pseudo-labels de sentiment. »  \n",
    "- « Cette méthode équipe l’entreprise d’un pipeline interprétable : clusters compréhensibles, mapping manuel/automatique, et possibilité d’itérer rapidement sur la précision via annotation ciblée. »\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f84cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ensemble labels: artifacts/absa_auto_ensemble_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Snippet B: ensemble auto-label (requires sentence-transformers & transformers)\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import re, pandas as pd, numpy as np\n",
    "\n",
    "df = pd.read_csv(\"artifacts/absa_manual_qc_sample.csv\")  # or full dataset\n",
    "texts = df['sentence'].fillna('').astype(str)\n",
    "texts = texts.where(texts.str.strip()!='', df['review'].fillna('').astype(str))\n",
    "aspects = df['aspect'].astype(str)\n",
    "\n",
    "# encode SBERT\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "text_emb = model.encode(texts.tolist(), convert_to_tensor=True, show_progress_bar=False)\n",
    "asp_emb = model.encode(aspects.tolist(), convert_to_tensor=True, show_progress_bar=False)\n",
    "sims = util.cos_sim(text_emb, asp_emb).diagonal().cpu().numpy()\n",
    "\n",
    "# zero-shot\n",
    "zs = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
    "\n",
    "def whole_word(text, token):\n",
    "    return bool(re.search(r'\\b' + re.escape(token.lower()) + r'\\b', text.lower()))\n",
    "\n",
    "labels = []\n",
    "conf = []\n",
    "reasons = []\n",
    "for i, row in df.iterrows():\n",
    "    asp = str(row['aspect'])\n",
    "    text = texts.iloc[i]\n",
    "    span = row.get('span','') or \"\"\n",
    "    vote = 0\n",
    "    reason_list = []\n",
    "    # rule: span\n",
    "    if str(span).strip() != \"\":\n",
    "        vote += 2; reason_list.append(\"span\")\n",
    "    # rule: regex / synonym\n",
    "    if whole_word(text, asp):\n",
    "        vote += 1; reason_list.append(\"word\")\n",
    "    # sbert\n",
    "    sim = float(sims[i])\n",
    "    if sim >= 0.62:\n",
    "        vote += 1; reason_list.append(f\"sbert:{sim:.2f}\")\n",
    "    # zero-shot\n",
    "    try:\n",
    "        out = zs(text[:512], candidate_labels=[asp, \"other\"], hypothesis_template=\"This sentence is about {}.\")\n",
    "        sc = out['scores'][0] if out['labels'][0]==asp else out['scores'][1]\n",
    "    except Exception:\n",
    "        sc = 0.0\n",
    "    if sc >= 0.65:\n",
    "        vote += 1; reason_list.append(f\"zs:{sc:.2f}\")\n",
    "    # decision: accept if votes >=2 OR span present\n",
    "    lab = 1 if (vote >= 2) else 0\n",
    "    labels.append(lab); conf.append(vote); reasons.append(\";\".join(reason_list))\n",
    "df['ensemble_label'] = labels\n",
    "df['ensemble_conf_votes'] = conf\n",
    "df['ensemble_reasons'] = reasons\n",
    "df.to_csv(\"artifacts/absa_auto_ensemble_labels.csv\", index=False)\n",
    "print(\"Saved ensemble labels: artifacts/absa_auto_ensemble_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae113d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Rows: 300\n",
      "\n",
      "Distribution des labels (ensemble_label):\n",
      "ensemble_label\n",
      "1    300\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "aspect",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "span",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sentence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_conf_votes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_reasons",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b5e54371-863a-4796-b32b-c069f73c07a4",
       "rows": [
        [
         "0",
         "cable",
         null,
         null,
         "These charging cords are excellent. You can’t beat the price for three of them. They are way long enough to allow to be on the phone and reach most any outlet. Highly recommended",
         "794443",
         "1",
         "2",
         "span"
        ],
        [
         "1",
         "cable",
         null,
         null,
         "This product says its water proof.<br />Since it rained one night it dont work at all. But other then that it's really good n preferable inside the house",
         "95248",
         "1",
         "2",
         "span"
        ],
        [
         "2",
         "cable",
         null,
         null,
         "Returned the same day received. I have had many garmins in the past and still use those. My current one touch is starting to miss recognize touches so I got the drivesmart 65 looking for a better everything. Turns out the other reviewers were right. This thing overheats like it's going supernova. Additionally, anytime you restart the unit the screen turns into a garbled mess of unusableness. Not sure what happened to their product this time around, but I'm severely disappointed. I will skip this model year all together and probably will be way more cautious on their future products. They need to rework their quality control and/or engineering design.",
         "23006",
         "1",
         "2",
         "span"
        ],
        [
         "3",
         "cable",
         null,
         null,
         "worked well for a couple months, ended up replacing unit after nvr became corrupt.  worked with tech support every time.  support was slow and difficult, never able to solve problems.  ran 7 sites with Zosi and will now be changing systems out.",
         "962905",
         "1",
         "2",
         "span"
        ],
        [
         "4",
         "cable",
         null,
         null,
         "I actually just bought this for the Nintendo switch. It seems to work well and I'm able to fit quite a few games in, especially many of the large games like Legend of Zelda: breath of the wild and xenoblade chronicles 2 and downloaded content with several more games. At this rate the card is a little more than halfway full.  I may not buy a larger card for a year or so if I feel the need to keep the games locally. Loading games and starting them is fast enough. This far I'm happy with this purchase.",
         "231771",
         "1",
         "2",
         "span"
        ],
        [
         "5",
         "cable",
         null,
         null,
         "I have used and own lots of wireless mice but the Logitech B100 is my all time favorite mouse. I like everything about it. Its a very accurate mouse. Its lightweight, smooth scrolling and just an around great mouse. Now some say its not very durable and I can understand that. They probably can't take a lot of beating. But I just love the feel of this mouse. The scroll wheel has very little resistance which means I can use it for a long time without index finger fatigue (if you know what I mean). For example I own the much more expensive Logitech MX Master 3 and I like it but not only is it heavy but for some reasons the scroll wheel is not accurate as the B100. When scrolling the page keeps moving all over the place. The B100 remains my favorite mouse.",
         "206658",
         "1",
         "2",
         "span"
        ],
        [
         "6",
         "cable",
         null,
         null,
         "Works just fine. No problems so far. Right price for the right product. Worked as intended. Not much to write here.",
         "189000",
         "1",
         "2",
         "span"
        ],
        [
         "7",
         "cable",
         null,
         null,
         "I was very excited to purchase this item being the price was amazing and two because the reviews seemed amazing.  I was definitely WRONG.  The biggest thing is do not use the bubble level that is in the mount.  I used it to level wall plate and got the bubble as perfect as you can get it for a bubble level.  I then stood back and it looked crooked so I check the bubble again and it was dead center.  I got out my level I have at home and it was completely off.  I then used the small level that came with the package and same thing.  The mount was totally crooked even though the bubble was dead center.<br /><br />After removing the mount and calling it a night out of frustration with all the useless very large holes in my wall I looked at the plate where the tv is screwed onto the mount and it is also crooked, slanting downward on the right side.  So even if I would have mounted the wall plate correctly, once the tv was put on it would have been crooked.<br /><br />I am very displeased with this product.  I would give it zero stars if I could.  I have emailed the seller with pictures and I will update on the response.<br /><br />BEWARE OF PRODUCT<br /><br />UPDATE<br /><br />They called me the very next day after I sent them a message through amazon.  So great customer service there.  They told me many of their customers who purchased this product had the exact same problem and they have fixed it.  The reason this product came with an extra bubble level is because the one on the mount doesn't work correctly.  I was able to fix the drooping on the tv plate and it has been up for a couple days without any problem.  I own a 55 inch samsung tv.  Other than the bubble level the mount itself is a good product.  Its a little hard moving it in and out, but I put some oil on the hinges and it slides a lot smoother.<br /><br />The seller offered me 20 dollars towards my next purchase with them.  I wish I could have gotten a discount, but they said that was not possible since the product was fulfilled through Amazon and not directly through them.  Customer service was quick and the product does what it is supposed to.  Just be careful of installation leveling with a separate level.<br /><br />UPDATE: Three months later and I am pretty happy with the wall mount.  After the fiasco the mount has functioned and held up to what it is supposed to do.  The arms seem really strong.  We pull the tv off the wall constantly to turn it towards the dining area.  It is a little difficult to pull the tv out, but I just have to keep putting grease on it and it fixes the problem.",
         "118895",
         "1",
         "2",
         "span"
        ],
        [
         "8",
         "cable",
         null,
         null,
         "I received this as a gift about a month ago. Its a neat little machine. It doesn't work like a regular external HD, it actually serves as a storage server on your network. Any of your computers can access it; I suggest mapping it as drive Z on all of your PCs.<br /><br />What I don't like about this device is the software which comes with it, \"Mionet.\" Basically, Mionet is a company that makes remote access software. A stripped down version of their remote access program comes with the My Book.<br /><br />So, to use the remote access feature of this product, you have to have a mionet account. Its free, but its constantly pandering to you to upgrade to 'premium.' Its quite annoying.<br /><br />This is what I did, and its kept me happy with this product.<br />1. Connect MyBook to network<br />2. Install drivers and Mionet (you have to) on each computer<br />3. create basic Mionet account<br />4. Uninstall Mionet<br /><br />This way, I don't have the software running on my PC at all times. If I want to access the Mybook while away from home, I just go to [...], sign in, and use the Java interface there.<br /><br />Overall, 4 stars b/c its great having this device on my network. The remote access feature could use some serious work",
         "954591",
         "1",
         "2",
         "span"
        ],
        [
         "9",
         "cable",
         null,
         null,
         "I got this adapter so I could view and copy the video clips from my Garmin dash cams micro SD card on / to my phone. The Garmin app requires a Wi-Fi connection to the dash cam in order to view or copy videos to your phone. This adapter avoids that nuisance.<br />Once you place the micro SD card in the adapter then insert it into your phone's charging port a notification appears asking how you want to use this storage space.<br />I found it relatively easy to use, use your native file explorer or photo/media player or any app that performs the same function. I was able to watch the recorded clips off of the SD card and also able to copy them to my phone's storage and didn't see any abnormal battery drain while in use.  Android 10.",
         "1136692",
         "1",
         "2",
         "span"
        ],
        [
         "10",
         "cable",
         null,
         null,
         "I bought 4 of these cases, one for each of my Kindle Fire 7. I have had a great experience with these for previous iPad's, so this was my first choice for the Kindles. Overall, it is a great case for children. It offers both protection and function. This would probably earn 5 stars if it weren't for the terrible fit and upfront inconvenience. I did not experience this with the iPad versions. As you will see in the picture attached, 3 of the 4 cases covered one of the volume buttons. This created all kinds of problems! I had to cut the the case opening for the volume buttons with a utility knife to move freely. The green case has such a tight fit that it's almost distorted, making it difficult to properly plug in the charger. The blue case provided the best and truest fit, not needing any alterations.",
         "855817",
         "1",
         "2",
         "span"
        ],
        [
         "11",
         "cable",
         null,
         null,
         "This is my second identical Toshiba, now  purchsed for my wife.  Obviously I am quite happy with the<br />one I have owned for around two years already.  This 2nd. one is also working quite well and giving<br />a brilliant picture. The only shortcoming is (was - now not really needed) that there is no HDMI out on<br />this particular model.  This requirement is now more or less redundant as I am now using CHROMECAST<br />to cast to my tv or projector from the Toshiba.",
         "961510",
         "1",
         "2",
         "span"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>span</th>\n",
       "      <th>sentence</th>\n",
       "      <th>review</th>\n",
       "      <th>review_id</th>\n",
       "      <th>ensemble_label</th>\n",
       "      <th>ensemble_conf_votes</th>\n",
       "      <th>ensemble_reasons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These charging cords are excellent. You can’t ...</td>\n",
       "      <td>794443</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This product says its water proof.&lt;br /&gt;Since ...</td>\n",
       "      <td>95248</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Returned the same day received. I have had man...</td>\n",
       "      <td>23006</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worked well for a couple months, ended up repl...</td>\n",
       "      <td>962905</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I actually just bought this for the Nintendo s...</td>\n",
       "      <td>231771</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have used and own lots of wireless mice but ...</td>\n",
       "      <td>206658</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Works just fine. No problems so far. Right pri...</td>\n",
       "      <td>189000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was very excited to purchase this item being...</td>\n",
       "      <td>118895</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I received this as a gift about a month ago. I...</td>\n",
       "      <td>954591</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I got this adapter so I could view and copy th...</td>\n",
       "      <td>1136692</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I bought 4 of these cases, one for each of my ...</td>\n",
       "      <td>855817</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is my second identical Toshiba, now  purc...</td>\n",
       "      <td>961510</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aspect  span  sentence                                             review  \\\n",
       "0   cable   NaN       NaN  These charging cords are excellent. You can’t ...   \n",
       "1   cable   NaN       NaN  This product says its water proof.<br />Since ...   \n",
       "2   cable   NaN       NaN  Returned the same day received. I have had man...   \n",
       "3   cable   NaN       NaN  worked well for a couple months, ended up repl...   \n",
       "4   cable   NaN       NaN  I actually just bought this for the Nintendo s...   \n",
       "5   cable   NaN       NaN  I have used and own lots of wireless mice but ...   \n",
       "6   cable   NaN       NaN  Works just fine. No problems so far. Right pri...   \n",
       "7   cable   NaN       NaN  I was very excited to purchase this item being...   \n",
       "8   cable   NaN       NaN  I received this as a gift about a month ago. I...   \n",
       "9   cable   NaN       NaN  I got this adapter so I could view and copy th...   \n",
       "10  cable   NaN       NaN  I bought 4 of these cases, one for each of my ...   \n",
       "11  cable   NaN       NaN  This is my second identical Toshiba, now  purc...   \n",
       "\n",
       "    review_id  ensemble_label  ensemble_conf_votes ensemble_reasons  \n",
       "0      794443               1                    2             span  \n",
       "1       95248               1                    2             span  \n",
       "2       23006               1                    2             span  \n",
       "3      962905               1                    2             span  \n",
       "4      231771               1                    2             span  \n",
       "5      206658               1                    2             span  \n",
       "6      189000               1                    2             span  \n",
       "7      118895               1                    2             span  \n",
       "8      954591               1                    2             span  \n",
       "9     1136692               1                    2             span  \n",
       "10     855817               1                    2             span  \n",
       "11     961510               1                    2             span  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Aperçu + distribution\n",
    "import os, pandas as pd\n",
    "p = \"artifacts/absa_auto_ensemble_labels.csv\"\n",
    "print(\"Exists:\", os.path.exists(p))\n",
    "df = pd.read_csv(p)\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"\\nDistribution des labels (ensemble_label):\")\n",
    "print(df['ensemble_label'].value_counts(dropna=False))\n",
    "display(df.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc31610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to: c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\notebooks\\data\\absa_auto_ensemble_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# 2) Copier vers /mnt/data pour download (ou remplacer si existant)\n",
    "import shutil, os\n",
    "src = \"artifacts/absa_auto_ensemble_labels.csv\"\n",
    "dst = \"data/absa_auto_ensemble_labels.csv\"\n",
    "shutil.copy2(src, dst)\n",
    "print(\"Copied to:\", os.path.abspath(dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ac59896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disagreements count: 256  saved -> artifacts/absa_auto_ensemble_disagreements.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "aspect",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "span",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sentence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_conf_votes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_reasons",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "simple_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "disagree",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "66be9594-799b-4fd5-bf16-765a233f044b",
       "rows": [
        [
         "0",
         "cable",
         null,
         null,
         "These charging cords are excellent. You can’t beat the price for three of them. They are way long enough to allow to be on the phone and reach most any outlet. Highly recommended",
         "794443",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "1",
         "cable",
         null,
         null,
         "This product says its water proof.<br />Since it rained one night it dont work at all. But other then that it's really good n preferable inside the house",
         "95248",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "2",
         "cable",
         null,
         null,
         "Returned the same day received. I have had many garmins in the past and still use those. My current one touch is starting to miss recognize touches so I got the drivesmart 65 looking for a better everything. Turns out the other reviewers were right. This thing overheats like it's going supernova. Additionally, anytime you restart the unit the screen turns into a garbled mess of unusableness. Not sure what happened to their product this time around, but I'm severely disappointed. I will skip this model year all together and probably will be way more cautious on their future products. They need to rework their quality control and/or engineering design.",
         "23006",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "3",
         "cable",
         null,
         null,
         "worked well for a couple months, ended up replacing unit after nvr became corrupt.  worked with tech support every time.  support was slow and difficult, never able to solve problems.  ran 7 sites with Zosi and will now be changing systems out.",
         "962905",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "4",
         "cable",
         null,
         null,
         "I actually just bought this for the Nintendo switch. It seems to work well and I'm able to fit quite a few games in, especially many of the large games like Legend of Zelda: breath of the wild and xenoblade chronicles 2 and downloaded content with several more games. At this rate the card is a little more than halfway full.  I may not buy a larger card for a year or so if I feel the need to keep the games locally. Loading games and starting them is fast enough. This far I'm happy with this purchase.",
         "231771",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "5",
         "cable",
         null,
         null,
         "I have used and own lots of wireless mice but the Logitech B100 is my all time favorite mouse. I like everything about it. Its a very accurate mouse. Its lightweight, smooth scrolling and just an around great mouse. Now some say its not very durable and I can understand that. They probably can't take a lot of beating. But I just love the feel of this mouse. The scroll wheel has very little resistance which means I can use it for a long time without index finger fatigue (if you know what I mean). For example I own the much more expensive Logitech MX Master 3 and I like it but not only is it heavy but for some reasons the scroll wheel is not accurate as the B100. When scrolling the page keeps moving all over the place. The B100 remains my favorite mouse.",
         "206658",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "6",
         "cable",
         null,
         null,
         "Works just fine. No problems so far. Right price for the right product. Worked as intended. Not much to write here.",
         "189000",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "7",
         "cable",
         null,
         null,
         "I was very excited to purchase this item being the price was amazing and two because the reviews seemed amazing.  I was definitely WRONG.  The biggest thing is do not use the bubble level that is in the mount.  I used it to level wall plate and got the bubble as perfect as you can get it for a bubble level.  I then stood back and it looked crooked so I check the bubble again and it was dead center.  I got out my level I have at home and it was completely off.  I then used the small level that came with the package and same thing.  The mount was totally crooked even though the bubble was dead center.<br /><br />After removing the mount and calling it a night out of frustration with all the useless very large holes in my wall I looked at the plate where the tv is screwed onto the mount and it is also crooked, slanting downward on the right side.  So even if I would have mounted the wall plate correctly, once the tv was put on it would have been crooked.<br /><br />I am very displeased with this product.  I would give it zero stars if I could.  I have emailed the seller with pictures and I will update on the response.<br /><br />BEWARE OF PRODUCT<br /><br />UPDATE<br /><br />They called me the very next day after I sent them a message through amazon.  So great customer service there.  They told me many of their customers who purchased this product had the exact same problem and they have fixed it.  The reason this product came with an extra bubble level is because the one on the mount doesn't work correctly.  I was able to fix the drooping on the tv plate and it has been up for a couple days without any problem.  I own a 55 inch samsung tv.  Other than the bubble level the mount itself is a good product.  Its a little hard moving it in and out, but I put some oil on the hinges and it slides a lot smoother.<br /><br />The seller offered me 20 dollars towards my next purchase with them.  I wish I could have gotten a discount, but they said that was not possible since the product was fulfilled through Amazon and not directly through them.  Customer service was quick and the product does what it is supposed to.  Just be careful of installation leveling with a separate level.<br /><br />UPDATE: Three months later and I am pretty happy with the wall mount.  After the fiasco the mount has functioned and held up to what it is supposed to do.  The arms seem really strong.  We pull the tv off the wall constantly to turn it towards the dining area.  It is a little difficult to pull the tv out, but I just have to keep putting grease on it and it fixes the problem.",
         "118895",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "8",
         "cable",
         null,
         null,
         "I received this as a gift about a month ago. Its a neat little machine. It doesn't work like a regular external HD, it actually serves as a storage server on your network. Any of your computers can access it; I suggest mapping it as drive Z on all of your PCs.<br /><br />What I don't like about this device is the software which comes with it, \"Mionet.\" Basically, Mionet is a company that makes remote access software. A stripped down version of their remote access program comes with the My Book.<br /><br />So, to use the remote access feature of this product, you have to have a mionet account. Its free, but its constantly pandering to you to upgrade to 'premium.' Its quite annoying.<br /><br />This is what I did, and its kept me happy with this product.<br />1. Connect MyBook to network<br />2. Install drivers and Mionet (you have to) on each computer<br />3. create basic Mionet account<br />4. Uninstall Mionet<br /><br />This way, I don't have the software running on my PC at all times. If I want to access the Mybook while away from home, I just go to [...], sign in, and use the Java interface there.<br /><br />Overall, 4 stars b/c its great having this device on my network. The remote access feature could use some serious work",
         "954591",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "9",
         "cable",
         null,
         null,
         "I got this adapter so I could view and copy the video clips from my Garmin dash cams micro SD card on / to my phone. The Garmin app requires a Wi-Fi connection to the dash cam in order to view or copy videos to your phone. This adapter avoids that nuisance.<br />Once you place the micro SD card in the adapter then insert it into your phone's charging port a notification appears asking how you want to use this storage space.<br />I found it relatively easy to use, use your native file explorer or photo/media player or any app that performs the same function. I was able to watch the recorded clips off of the SD card and also able to copy them to my phone's storage and didn't see any abnormal battery drain while in use.  Android 10.",
         "1136692",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "10",
         "cable",
         null,
         null,
         "I bought 4 of these cases, one for each of my Kindle Fire 7. I have had a great experience with these for previous iPad's, so this was my first choice for the Kindles. Overall, it is a great case for children. It offers both protection and function. This would probably earn 5 stars if it weren't for the terrible fit and upfront inconvenience. I did not experience this with the iPad versions. As you will see in the picture attached, 3 of the 4 cases covered one of the volume buttons. This created all kinds of problems! I had to cut the the case opening for the volume buttons with a utility knife to move freely. The green case has such a tight fit that it's almost distorted, making it difficult to properly plug in the charger. The blue case provided the best and truest fit, not needing any alterations.",
         "855817",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "11",
         "cable",
         null,
         null,
         "This is my second identical Toshiba, now  purchsed for my wife.  Obviously I am quite happy with the<br />one I have owned for around two years already.  This 2nd. one is also working quite well and giving<br />a brilliant picture. The only shortcoming is (was - now not really needed) that there is no HDMI out on<br />this particular model.  This requirement is now more or less redundant as I am now using CHROMECAST<br />to cast to my tv or projector from the Toshiba.",
         "961510",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "12",
         "cable",
         null,
         null,
         "It's fun and pretty accurate. However, the mount isn't strong, and the sensor flew off from my racket multiple times until it broke. I probably recorded 10-20 sessions  erode it broke.",
         "640861",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "13",
         "cable",
         null,
         null,
         "The Logitech Ultrathin Keyboard Cover was shipped to me by Amazon in a padded envelope.  When I opened the package, I noticed that the product packaging was slightly dented, but nothing serious.  But when I took the keyboard cover out of the box and attached it to my iPad, I found that the keyboard was warped significantly.  When attached, the end opposite the hinge was separated from the iPad by about a quarter of an inch.  I immediately submitted for a return, and Amazon quickly shipped a replacement.  I received the replacement today, and while the product box was not damaged, the keyboard again was warped, and even had a very slight dent on the cover.  Shipping these things in just a padded envelope is probably not the best idea.<br /><br />I've asked for a refund, as I don't want to risk  getting a third damaged keyboard.<br /><br />I was able to  pair the keyboard with the iPad, and while typing did seem to be enjoyable, the aluminum cover seems very flimsy.  I'm sure this contributed to the fact that both keyboards I received were warped.  For something that costs 1/5 to 1/8 the cost of the actual iPad (depending on which iPad you own), this thing should be sturdier.  Not impressed.",
         "74869",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "14",
         "cable",
         null,
         null,
         "My daughter thought the Star War Yoda earbuds were great.  She absolutely loves them.  She uses them during her turn at dishes",
         "714395",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "15",
         "cable",
         null,
         null,
         "My wife recently asked me about getting a camera for around the house and wanted to upgrade from our old point and shoot. Being an amateur photo enthusiast for as long as I have been and hanging with some real camera nuts I can honestly say I never hesitated. The Canon SX40 HS is the answer for many who want to get more serious, but maybe the bulk or price of a true dSLR doesn't fit your needs. In many (and most cases) this camera not only fills the gap, it will leave you not even wanting a true dSLR. This coming from a man that owns a Nikon D7000 with 3 detachable lenses and hundreds of dollars in accessories. I also own a Canon SX20is which is 2 years old as of this writing, but a camera that I still grab if I want to not worry to much about technical shooting and want to worry more about composition and maybe add some artistic flare with it's many cool functions.<br /><br />No, the Canon is not a detachable lens dSLR.  It is a full body \"bridge camera\" that does more in many instances of what a dSLR camera can do for the average person but maybe a bit less than what a professional would want done to their pics.  Of course a professional would need thousands of dollars in lenses and a photoshop program to match some of the functions this camera can do.  This camera is top in it's class amongst bridge style camera's in picture quality and shooting side by side with the Nikon Cool PIx direct competition proved what I had thought all along. (Again this coming from a Nikon d7000 owner). Canon plain and simple owns the high end bridge camera market in my opinion.<br /><br />Zoom is a big seller and many are disappointed when they hit the dSLR market and see the price of even a modest Zoom lens. Considering a low end 200mm lens for a dSLR is about the price of this entire camera, I can see why some skip the dSLR market altogether and would want something like this.  The average person would never be able to afford the zoom this camera offers on a dSLR.<br /><br />Quality usually goes down when zoom is involved as well. Ask any camera buff to test this baby out.  They will be amazed. Im not saying it's the equivalent of a f2.8 200mm or 500mm lens professionals use, but it's not bad and this entire camera weighs less than most 300mm dSLR lenses do and a 10th of what a 500mm lens would.  The detractors of my comparison, I'm sure, will mutter some mumbo jumbo about how the f-stop is too high on these super zoom cameras and that there is no way they can get as much quality as their $5000-$10000 outfit.  It may be so when comparing many technical aspects, but what I like is flat out results, not what some technical guru is telling you. And I LOVE the results of the testing I've put this camera through.  For the money, for the weight of this unit and all else, I will stand behind my claim that<br /><br />This camera has a 35X optical zoom (WITH an incredible 140X digital zoom) that produces images better than I could ever have imagined (even compared to my 80x zoom on my sx20...so big improvements in technology there!). It has an optical stabilization of which I am totally impressed with considering you can freehand a zoomed in pic at 140 digital zoom (35x optical) and get impressive pics. The zoom doesn't seem to suffer from lack of resolution or light gathering capabilities either in daylight hours.. On one particular extremely overcast day my wife took a picture of the trans-allegheny lunatic asylum in Weston, WV clock tower from the fence at the street and it filling the frame completely (and then some) with no motion and beautiful resolution. Considering at full zoom the 35mm equivilant is 840mm (can you imagine the price of THAT lens for a dSLR) I found this impressive. You could see the paint peeling off the clock face she got so close! We were a good 300 yards from it.<br /><br />Size is manageable as well. Smaller than any dSLR. It's not slim like a pocket camera, but it isn't bulky like a dSLR either. Its a \"best of both worlds hybrid camera\" that any camera enthusiast would be proud to have in their arsenal. In addition the video on this camera rivals any consumer HD camera available. I use my sx20 for my youtube videos and consistently get complimented on image quality. The sx40 has now further 'upped the ante' (from my sx20) and bumped up quality to a full 1080HD video WITH autofocus (which even the best dSLR cameras cannot rival to this date). Considering video cameras have typically low res photo taking quality, it would make sense to buy this as your video camera AND you get awesome 12MP picture quality in addition.<br /><br />OK, so thus far you may think I have elevated this to God status. I realize it's faults. I know it's shortcomings. Thus the reason I own my Nikon d7000.<br /><br />A true dSLR has advantages. Manually focusing pictures is so much easier(I wouldn't even bother with manual focus with this camera), lower f-stops (with the correct lens, which btw are very expensive), wide angle shots (again, expensive lense) and the ability to crank out 3200ISO pictures with decent quality are the 3 that come to mind immediatly. The art of pulling together a beautiful picture that you can truly call your own, with a combination of technical expertise and composing can be more fully realized with a dSLR detachable lens type camera. But for the average person looking at your pictures and seeing the end result, most all but the most arogant camera snobs would say \"who cares\" [lol]. Again, this coming from a man that owns an amazing Nikon dSLR.<br /><br />For the amateur enthusiast who wants to 'up their game' (without the expense of a magnum camera), dabble with manual settings and experiment with various picture options like fish eye lens, color swapping/enhancing, super-vivid color saturation option in addition to having the capability to get high quality video all within one carry around camera (without the bag) and so much more...I will HIGHLY recommend this camera to anybody that is looking for a camera but not quite wanting to reach into the expensive dSLR market.<br /><br />Highly recommended accessory ($10 in a few stores here on Amazon) would be the conversion ring that allows the use of 58mm filters so that you can dabble with polarizing and ND filters to further enhance your photography.  This camera does not allow the use of standard dSLR camera filters, you need this conversion ring to allow filters to be screwed on properly.",
         "439782",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "16",
         "cable",
         null,
         null,
         "Doesn't fit tight and fall out of the computer if moved.  Fixed that problem.  Just don't move the computer.",
         "1077570",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "17",
         "cable",
         null,
         null,
         "Bought this to replace my daughter's failing laptop battery.  The OEM battery is twice, if not three times the price and experience proves that after a model is no longer current, even the OEM batteries can't be trusted.  This one seemed to have good reviews.  (However, it is worth noting that by writing a review, CPY will offer an upgrade from 12 months to 24 months warranty.)  So I took a chance, especially since within 30 days, CPY will refund the price, no questions asked.<br />After charging the battery to 100%, I ran the battery report utility built into Windows 10 and was pleased to see that the 40Wh rated battery was actually almost 39Wh. Historically, it reported her original battery was down at 16Wh capacity after a couple years of use.<br />Surprisingly, the report did show the design capacity to be approx. 56Wh and the manufacturer to be Panasonic!  Not sure what is going on there, but being a 40Wh rated battery, to get 39Wh capacity is a good sign.  I've had replacement cell phone batteries that were 60% of their rated capacity on arrival!  It fitted perfectly - no problems there.<br />Only time will tell how well this battery holds up",
         "1075910",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "18",
         "cable",
         null,
         null,
         "They hurt my ears sometimes. Wish they had the durability and noise cancellation quality with more comfort. The mic isn’t great per people I talk to.<br /><br />***UPDATE 8/7/2020***<br />Randomly disconnects even when my phone is in my pocket, sound quality very patchy and staticky sounding. Not worth the money.",
         "1129520",
         "1",
         "2",
         "span",
         "0",
         "True"
        ],
        [
         "19",
         "cable",
         null,
         null,
         "I was expecting the usual problems with getting a new piece of hardware to work with my laptop/computer.  Surprise, surprise this worked right out of the box.  I just installed the software that came with the unit plugged in the stereo input and usb port and voila it worked like a charm.  I can't recommend this highly enough.  There was some minimal assembly of the turntable but nothing too difficult.  I got mine at an excellent price and am extremely satisfied.<br /><br />Doreen",
         "1233775",
         "1",
         "2",
         "span",
         "0",
         "True"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>span</th>\n",
       "      <th>sentence</th>\n",
       "      <th>review</th>\n",
       "      <th>review_id</th>\n",
       "      <th>ensemble_label</th>\n",
       "      <th>ensemble_conf_votes</th>\n",
       "      <th>ensemble_reasons</th>\n",
       "      <th>simple_label</th>\n",
       "      <th>disagree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These charging cords are excellent. You can’t ...</td>\n",
       "      <td>794443</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This product says its water proof.&lt;br /&gt;Since ...</td>\n",
       "      <td>95248</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Returned the same day received. I have had man...</td>\n",
       "      <td>23006</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worked well for a couple months, ended up repl...</td>\n",
       "      <td>962905</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I actually just bought this for the Nintendo s...</td>\n",
       "      <td>231771</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have used and own lots of wireless mice but ...</td>\n",
       "      <td>206658</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Works just fine. No problems so far. Right pri...</td>\n",
       "      <td>189000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was very excited to purchase this item being...</td>\n",
       "      <td>118895</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I received this as a gift about a month ago. I...</td>\n",
       "      <td>954591</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I got this adapter so I could view and copy th...</td>\n",
       "      <td>1136692</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I bought 4 of these cases, one for each of my ...</td>\n",
       "      <td>855817</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is my second identical Toshiba, now  purc...</td>\n",
       "      <td>961510</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's fun and pretty accurate. However, the mou...</td>\n",
       "      <td>640861</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Logitech Ultrathin Keyboard Cover was ship...</td>\n",
       "      <td>74869</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My daughter thought the Star War Yoda earbuds ...</td>\n",
       "      <td>714395</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My wife recently asked me about getting a came...</td>\n",
       "      <td>439782</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn't fit tight and fall out of the computer...</td>\n",
       "      <td>1077570</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bought this to replace my daughter's failing l...</td>\n",
       "      <td>1075910</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They hurt my ears sometimes. Wish they had the...</td>\n",
       "      <td>1129520</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was expecting the usual problems with gettin...</td>\n",
       "      <td>1233775</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>span</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aspect  span  sentence                                             review  \\\n",
       "0   cable   NaN       NaN  These charging cords are excellent. You can’t ...   \n",
       "1   cable   NaN       NaN  This product says its water proof.<br />Since ...   \n",
       "2   cable   NaN       NaN  Returned the same day received. I have had man...   \n",
       "3   cable   NaN       NaN  worked well for a couple months, ended up repl...   \n",
       "4   cable   NaN       NaN  I actually just bought this for the Nintendo s...   \n",
       "5   cable   NaN       NaN  I have used and own lots of wireless mice but ...   \n",
       "6   cable   NaN       NaN  Works just fine. No problems so far. Right pri...   \n",
       "7   cable   NaN       NaN  I was very excited to purchase this item being...   \n",
       "8   cable   NaN       NaN  I received this as a gift about a month ago. I...   \n",
       "9   cable   NaN       NaN  I got this adapter so I could view and copy th...   \n",
       "10  cable   NaN       NaN  I bought 4 of these cases, one for each of my ...   \n",
       "11  cable   NaN       NaN  This is my second identical Toshiba, now  purc...   \n",
       "12  cable   NaN       NaN  It's fun and pretty accurate. However, the mou...   \n",
       "13  cable   NaN       NaN  The Logitech Ultrathin Keyboard Cover was ship...   \n",
       "14  cable   NaN       NaN  My daughter thought the Star War Yoda earbuds ...   \n",
       "15  cable   NaN       NaN  My wife recently asked me about getting a came...   \n",
       "16  cable   NaN       NaN  Doesn't fit tight and fall out of the computer...   \n",
       "17  cable   NaN       NaN  Bought this to replace my daughter's failing l...   \n",
       "18  cable   NaN       NaN  They hurt my ears sometimes. Wish they had the...   \n",
       "19  cable   NaN       NaN  I was expecting the usual problems with gettin...   \n",
       "\n",
       "    review_id  ensemble_label  ensemble_conf_votes ensemble_reasons  \\\n",
       "0      794443               1                    2             span   \n",
       "1       95248               1                    2             span   \n",
       "2       23006               1                    2             span   \n",
       "3      962905               1                    2             span   \n",
       "4      231771               1                    2             span   \n",
       "5      206658               1                    2             span   \n",
       "6      189000               1                    2             span   \n",
       "7      118895               1                    2             span   \n",
       "8      954591               1                    2             span   \n",
       "9     1136692               1                    2             span   \n",
       "10     855817               1                    2             span   \n",
       "11     961510               1                    2             span   \n",
       "12     640861               1                    2             span   \n",
       "13      74869               1                    2             span   \n",
       "14     714395               1                    2             span   \n",
       "15     439782               1                    2             span   \n",
       "16    1077570               1                    2             span   \n",
       "17    1075910               1                    2             span   \n",
       "18    1129520               1                    2             span   \n",
       "19    1233775               1                    2             span   \n",
       "\n",
       "    simple_label  disagree  \n",
       "0              0      True  \n",
       "1              0      True  \n",
       "2              0      True  \n",
       "3              0      True  \n",
       "4              0      True  \n",
       "5              0      True  \n",
       "6              0      True  \n",
       "7              0      True  \n",
       "8              0      True  \n",
       "9              0      True  \n",
       "10             0      True  \n",
       "11             0      True  \n",
       "12             0      True  \n",
       "13             0      True  \n",
       "14             0      True  \n",
       "15             0      True  \n",
       "16             0      True  \n",
       "17             0      True  \n",
       "18             0      True  \n",
       "19             0      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) Générer disagreements (petit fichier à vérifier manuellement si besoin)\n",
    "import pandas as pd, os\n",
    "df = pd.read_csv(\"artifacts/absa_auto_ensemble_labels.csv\")\n",
    "# simple_label: span present OR whole-word aspect present\n",
    "import re\n",
    "def whole_word(text, token):\n",
    "    if not isinstance(text, str) or not token:\n",
    "        return False\n",
    "    return bool(re.search(r'\\b' + re.escape(token.lower()) + r'\\b', str(text).lower()))\n",
    "simple = []\n",
    "for _, r in df.iterrows():\n",
    "    span = r.get('span','')\n",
    "    if isinstance(span, float) and pd.isna(span): span = \"\"\n",
    "    if str(span).strip():\n",
    "        simple.append(1)\n",
    "    else:\n",
    "        txt = str(r.get('sentence','') or \"\") + \" \" + str(r.get('review','') or \"\")\n",
    "        simple.append(1 if whole_word(txt, r.get('aspect','')) else 0)\n",
    "df['simple_label'] = simple\n",
    "df['disagree'] = df['ensemble_label'] != df['simple_label']\n",
    "dis = df[df['disagree']].copy()\n",
    "out = \"artifacts/absa_auto_ensemble_disagreements.csv\"\n",
    "dis.to_csv(out, index=False)\n",
    "print(\"Disagreements count:\", len(dis), \" saved ->\", out)\n",
    "display(dis.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e9494ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "aspect",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_sample",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_pos",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision_est_proxy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "148f9275-b0fd-4fd0-a183-aaefde69321d",
       "rows": [
        [
         "cable",
         "100",
         "100",
         "1.0"
        ],
        [
         "camera",
         "100",
         "100",
         "1.0"
        ],
        [
         "price",
         "100",
         "100",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sample</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>precision_est_proxy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cable</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camera</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_sample  n_pos  precision_est_proxy\n",
       "aspect                                      \n",
       "cable        100    100                  1.0\n",
       "camera       100    100                  1.0\n",
       "price        100    100                  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) Résumé par aspect\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"artifacts/absa_auto_ensemble_labels.csv\")\n",
    "summary = df.groupby('aspect')['ensemble_label'].agg(['count','sum']).rename(columns={'sum':'n_pos','count':'n_sample'})\n",
    "summary['precision_est_proxy'] = (summary['n_pos'] / summary['n_sample']).round(3)\n",
    "display(summary.sort_values('n_pos', ascending=False).head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f03fef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count span non-empty: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "aspect",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "span",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sentence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_conf_votes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ensemble_reasons",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a81ee120-c035-4c88-a65c-14a6652fd2f8",
       "rows": [],
       "shape": {
        "columns": 8,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>span</th>\n",
       "      <th>sentence</th>\n",
       "      <th>review</th>\n",
       "      <th>review_id</th>\n",
       "      <th>ensemble_label</th>\n",
       "      <th>ensemble_conf_votes</th>\n",
       "      <th>ensemble_reasons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [aspect, span, sentence, review, review_id, ensemble_label, ensemble_conf_votes, ensemble_reasons]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "df = pd.read_csv(\"artifacts/absa_auto_ensemble_labels.csv\")\n",
    "# afficher quelques exemples où span non-nul selon le code\n",
    "mask_span = df['span'].notna() & (df['span'].astype(str).str.strip() != \"\") \n",
    "print(\"Count span non-empty:\", mask_span.sum())\n",
    "display(df[mask_span].head(20))\n",
    "# voir valeurs particulières (strings “Missing value” etc.)\n",
    "print(df['span'].unique()[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba555280",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sbert_sim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'sbert_sim'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m display(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msbert_sim\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.describe())\n\u001b[32m      2\u001b[39m display(df[\u001b[33m'\u001b[39m\u001b[33mzs_score\u001b[39m\u001b[33m'\u001b[39m].describe())\n\u001b[32m      3\u001b[39m display(df[\u001b[33m'\u001b[39m\u001b[33mvotes\u001b[39m\u001b[33m'\u001b[39m].value_counts().sort_index())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'sbert_sim'"
     ]
    }
   ],
   "source": [
    "display(df['sbert_sim'].describe())\n",
    "display(df['zs_score'].describe())\n",
    "display(df['votes'].value_counts().sort_index())\n",
    "# show low/medium scores that were labelled 1\n",
    "display(df[(df['ensemble_label']==1) & (df['sbert_sim']<0.6)].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a34ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer une règle conservative (span present OR exact whole-word match)\n",
    "import os, re, pandas as pd, math\n",
    "\n",
    "src = \"artifacts/absa_auto_ensemble_labels.csv\"\n",
    "if not os.path.exists(src):\n",
    "    raise FileNotFoundError(f\"{src} introuvable (vérifie le chemin)\")\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "def whole_word(text, token):\n",
    "    if not isinstance(text, str) or not token:\n",
    "        return False\n",
    "    return bool(re.search(r'\\b' + re.escape(str(token).lower()) + r'\\b', str(text).lower()))\n",
    "\n",
    "def is_span_present(span):\n",
    "    if span is None:\n",
    "        return False\n",
    "    if isinstance(span, float) and pd.isna(span):\n",
    "        return False\n",
    "    s = str(span).strip().lower()\n",
    "    if s in [\"\", \"nan\", \"none\", \"missing value\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def conservative_label(row):\n",
    "    # 1) span explicit -> accept\n",
    "    if is_span_present(row.get('span', \"\")):\n",
    "        return 1\n",
    "    # 2) exact whole-word aspect in sentence or review -> accept\n",
    "    text = str(row.get('sentence', '') or \"\") + \" \" + str(row.get('review', '') or \"\")\n",
    "    if whole_word(text, row.get('aspect', '')):\n",
    "        return 1\n",
    "    # 3) otherwise reject\n",
    "    return 0\n",
    "\n",
    "# apply and save\n",
    "df['cons_label'] = df.apply(conservative_label, axis=1)\n",
    "df['cons_agree'] = df['cons_label'] == df['ensemble_label']\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "out_art = \"artifacts/absa_auto_ensemble_conservative.csv\"\n",
    "out_mnt = \"data/absa_auto_ensemble_conservative.csv\"\n",
    "df.to_csv(out_art, index=False)\n",
    "df.to_csv(out_mnt, index=False)\n",
    "\n",
    "# disagreements (cases to inspect quickly)\n",
    "dis = df[df['cons_agree']==False].copy()\n",
    "dis_path = \"artifacts/absa_auto_ensemble_conservative_disagreements.csv\"\n",
    "dis_path_mnt = \"data/absa_auto_ensemble_conservative_disagreements.csv\"\n",
    "dis.to_csv(dis_path, index=False)\n",
    "dis.to_csv(dis_path_mnt, index=False)\n",
    "\n",
    "# summary\n",
    "summary = {\n",
    "    \"total_rows\": len(df),\n",
    "    \"ensemble_ones\": int((df['ensemble_label']==1).sum()),\n",
    "    \"cons_ones\": int((df['cons_label']==1).sum()),\n",
    "    \"agreements\": int(df['cons_agree'].sum()),\n",
    "    \"disagreements\": int((df['cons_agree']==False).sum())\n",
    "}\n",
    "print(\"Saved conservative CSV ->\", os.path.abspath(out_art))\n",
    "print(\"Saved copy ->\", os.path.abspath(out_mnt))\n",
    "print(\"Saved disagreements ->\", os.path.abspath(dis_path))\n",
    "print(\"Summary:\", summary)\n",
    "display(df[['aspect','span','sentence','review','ensemble_label','cons_label','ensemble_reasons']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re, math, pandas as pd, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 0) Chercher automatiquement un fichier d'entrée possible\n",
    "candidates = [\n",
    "    \"artifacts/absa_auto_ensemble_labels.csv\",\n",
    "    \"artifacts/absa_auto_ensemble_conservative.csv\",\n",
    "    \"artifacts/absa_manual_qc_sample.csv\",\n",
    "    \"notebooks/artifacts/absa_auto_ensemble_labels.csv\",\n",
    "    \"notebooks/artifacts/absa_manual_qc_sample.csv\",\n",
    "    \"/mnt/data/absa_auto_ensemble_labels.csv\",\n",
    "    \"/mnt/data/absa_manual_qc_sample.csv\",\n",
    "    \"absa_manual_qc_sample.csv\"\n",
    "]\n",
    "src = None\n",
    "for p in candidates:\n",
    "    if p and os.path.exists(p):\n",
    "        src = p\n",
    "        break\n",
    "if src is None:\n",
    "    # fallback: try to find any file that looks like 'absa' in the tree\n",
    "    found = glob.glob(\"**/*absa*.csv\", recursive=True)\n",
    "    found = [f for f in found if \"ml\" not in f and \"ml_labels\" not in f]\n",
    "    if found:\n",
    "        src = found[0]\n",
    "if src is None:\n",
    "    raise FileNotFoundError(\"Aucun fichier source trouvé (regarder artifacts/). Essayez de préciser le chemin ou exécutez la cellule depuis le dossier racine du projet.\")\n",
    "\n",
    "print(\"Using source file:\", os.path.abspath(src))\n",
    "df = pd.read_csv(src)\n",
    "print(\"Rows in source:\", len(df))\n",
    "print(\"Cols:\", df.columns.tolist())\n",
    "\n",
    "# 1) Préparer textes et aspects\n",
    "# prefer 'sentence' else 'review'\n",
    "if 'sentence' in df.columns:\n",
    "    texts = df['sentence'].fillna('').astype(str).where(lambda s: s.str.strip()!='', df['review'].fillna('').astype(str))\n",
    "else:\n",
    "    texts = df['review'].fillna('').astype(str)\n",
    "aspects = df['aspect'].fillna('').astype(str).tolist()\n",
    "texts_list = texts.tolist()\n",
    "\n",
    "# 2) Chargement SBERT (peut télécharger le modèle -> quelques dizaines de Mo)\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "print(\"Loading SBERT model...\")\n",
    "sbert = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "print(\"Encoding texts + aspects (SBERT) ...\")\n",
    "text_embs = sbert.encode(texts_list, convert_to_tensor=True, show_progress_bar=True)\n",
    "asp_embs = sbert.encode(aspects, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# 3) Similarité diagonale (text_i vs aspect_i)\n",
    "sims = util.cos_sim(text_embs, asp_embs).diagonal().cpu().numpy()\n",
    "df['sbert_sim'] = sims\n",
    "print(\"sbert_sim computed: min %.3f / med %.3f / max %.3f\" % (sims.min(), pd.Series(sims).median(), sims.max()))\n",
    "\n",
    "# 4) Zero-shot (facebook/bart-large-mnli) -> CPU par défaut\n",
    "from transformers import pipeline\n",
    "print(\"Loading zero-shot classifier (this may download ~700MB)...\")\n",
    "zs = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
    "\n",
    "zs_scores = []\n",
    "for i, txt in enumerate(texts_list):\n",
    "    asp = aspects[i]\n",
    "    try:\n",
    "        out = zs(str(txt)[:512], candidate_labels=[asp, \"other\"], hypothesis_template=\"This sentence is about {}.\", multi_label=False)\n",
    "        sc = 0.0\n",
    "        for lbl, scv in zip(out['labels'], out['scores']):\n",
    "            if lbl == asp:\n",
    "                sc = scv; break\n",
    "    except Exception as e:\n",
    "        sc = 0.0\n",
    "    zs_scores.append(float(sc))\n",
    "df['zs_score'] = zs_scores\n",
    "print(\"zs_score computed: min %.3f / med %.3f / max %.3f\" % (min(zs_scores), pd.Series(zs_scores).median(), max(zs_scores)))\n",
    "\n",
    "# 5) Appliquer la règle ML stricte\n",
    "def is_span_present(span):\n",
    "    if span is None: return False\n",
    "    if isinstance(span, float) and pd.isna(span): return False\n",
    "    s = str(span).strip().lower()\n",
    "    if s in [\"\", \"nan\", \"none\", \"missing value\"]: return False\n",
    "    return True\n",
    "\n",
    "def ml_rule(row, sbert_th=0.68, zs_th=0.68):\n",
    "    if is_span_present(row.get('span', \"\")):\n",
    "        return 1\n",
    "    if float(row.get('sbert_sim', 0.0)) >= sbert_th and float(row.get('zs_score', 0.0)) >= zs_th:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df['ml_label'] = df.apply(ml_rule, axis=1)\n",
    "df['ml_agree_with_ensemble'] = (df['ml_label'] == df.get('ensemble_label', -1))\n",
    "\n",
    "# 6) Sauver résultats (artifacts + /mnt/data pour download)\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "out_art = \"artifacts/absa_auto_ensemble_ml_labels.csv\"\n",
    "out_mnt = \"data/absa_auto_ensemble_ml_labels.csv\"\n",
    "df.to_csv(out_art, index=False)\n",
    "df.to_csv(out_mnt, index=False)\n",
    "\n",
    "# disagreements (pour QA rapide)\n",
    "dis = df[df['ml_agree_with_ensemble'] == False].copy()\n",
    "dis_path = \"artifacts/absa_auto_ensemble_ml_disagreements.csv\"\n",
    "dis.to_csv(dis_path, index=False)\n",
    "dis.to_csv(\"data/absa_auto_ensemble_ml_disagreements.csv\", index=False)\n",
    "\n",
    "# 7) Résumé\n",
    "summary = {\n",
    "    \"total_rows\": len(df),\n",
    "    \"ensemble_ones\": int((df.get('ensemble_label', pd.Series([]))==1).sum()) if 'ensemble_label' in df.columns else None,\n",
    "    \"ml_ones\": int((df['ml_label']==1).sum()),\n",
    "    \"agreements\": int(df['ml_agree_with_ensemble'].sum()) if 'ensemble_label' in df.columns else None,\n",
    "    \"disagreements\": int(len(dis))\n",
    "}\n",
    "print(\"Saved ML CSV ->\", os.path.abspath(out_art))\n",
    "print(\"Saved download copy ->\", os.path.abspath(out_mnt))\n",
    "print(\"Saved ML disagreements ->\", os.path.abspath(dis_path))\n",
    "print(\"Summary:\", summary)\n",
    "\n",
    "# 8) Inspect: distribution and some example disagreements\n",
    "display(df[['aspect','span','sentence','review','ensemble_label','sbert_sim','zs_score','ml_label']].head(20))\n",
    "print(\"\\nSbert stats:\")\n",
    "display(pd.Series(df['sbert_sim']).describe())\n",
    "print(\"\\nZero-shot stats:\")\n",
    "display(pd.Series(df['zs_score']).describe())\n",
    "print(\"\\nml_label distribution:\")\n",
    "display(pd.Series(df['ml_label']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95be597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
