{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf1fce7",
   "metadata": {},
   "source": [
    "\n",
    "# üß± C5.2.1 ‚Äî Construire des variables (Bloc 5)\n",
    "\n",
    "**Objectif de la comp√©tence (√©liminatoire)** : √† partir des avis texte, **construire des variables** pertinentes et reproductibles\n",
    "pour la mod√©lisation (sentiment binaire/√©motions/ABSA), et produire un **jeu de donn√©es exploitable**.\n",
    "\n",
    "Ce notebook :\n",
    "- lit le **dataset normalis√©** produit en C5.1.1 (`data/interim/amazon_electronics_normalized.parquet|csv`),\n",
    "- applique des **pr√©traitements contr√¥l√©s**,\n",
    "- g√©n√®re plusieurs **familles de features** (TF‚ÄëIDF mots, TF‚ÄëIDF caract√®res, *hand‚Äëcrafted*),\n",
    "- sauvegarde les **artefacts de vectorisation** et un **√©chantillon de features** r√©utilisable,\n",
    "- documente les **choix** et pourquoi la comp√©tence est **valid√©e**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a5fa2",
   "metadata": {},
   "source": [
    "## üéôÔ∏è Discours rapide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf56a8",
   "metadata": {},
   "source": [
    "\n",
    "> ¬´ Nous construisons d'abord des **features textuelles robustes** et peu co√ªteuses : **TF‚ÄëIDF mots** pour la s√©mantique directe\n",
    "> et **TF‚ÄëIDF caract√®res** pour la morphologie/ponctuation (utile pour fautes/sarcasme).  \n",
    "> Nous ajoutons quelques **indicateurs** (longueur, !, ? , MAJUSCULES) et conservons des **m√©tadonn√©es** utiles (*verified*, *helpful*).  \n",
    "> Les recettes sont **param√©tr√©es** (ngram, min_df, max_features), **reproductibles** (seeds), et **s√©rialis√©es** (`joblib`).\n",
    "> On produit un **jeu de donn√©es exploitable** (X, y) et les **artefacts** pour l'entra√Ænement (C5.2.3). ¬ª\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245e950",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 0) Config & chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbff6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PARQUET_NORM = Path(\"data/interim/amazon_electronics_normalized.parquet\")\n",
    "CSV_NORM     = Path(\"data/interim/amazon_electronics_normalized.csv\")\n",
    "\n",
    "TEXT_COL   = \"review_body\"\n",
    "LABEL_COL  = \"star_rating\"   # utilis√© pour fabriquer y binaire >=4\n",
    "DATE_COL   = \"review_date\"   # optionnel\n",
    "\n",
    "# Sampler pour it√©rer vite (None = tout)\n",
    "SAMPLE_N = 200_000   # tu peux augmenter/r√©duire selon ta RAM\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Recette TF-IDF mots\n",
    "WORD_MAX_FEATURES = 60_000\n",
    "WORD_NGRAM_RANGE  = (1, 2)\n",
    "WORD_MIN_DF       = 3\n",
    "\n",
    "# Recette TF-IDF caract√®res (d√©sactivable ici)\n",
    "USE_CHAR_TFIDF    = True\n",
    "CHAR_MAX_FEATURES = 120_000\n",
    "CHAR_NGRAM_RANGE  = (3, 5)\n",
    "CHAR_MIN_DF       = 3\n",
    "\n",
    "# Dossiers sortie\n",
    "ART_DIR = Path(\"models\")\n",
    "DATA_PROC_DIR = Path(\"data/processed\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROC_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67b991",
   "metadata": {},
   "source": [
    "## üì• 1) Chargement du dataset normalis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d7fdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Charg√© depuis : data\\interim\\amazon_electronics_normalized.parquet\n",
      "Shape: (200000, 12)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_len",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "0591e269-5964-4ca8-a0f7-7345f382613b",
       "rows": [
        [
         "145322",
         "This Hub was clearly designed for Apple computers.  The design, quality and materials used are all top notch.  The only 2 short comings I can think of are 1) the USB cable is a very short and 2) it's not USB 3.0 compatible.<br /><br />Hopefully they will make a new version that has USB 3.0 and maybe include a few Thunderbolt ports.  For some reason this product is not listed on the manufacturers website.  It may be a specially designed product for the seller (Battery World) who sells them here on Amazon.",
         "509"
        ],
        [
         "1110436",
         "Works as advertised",
         "19"
        ],
        [
         "301223",
         "Good so far! Only had for 1 week. Clips for straps are hard to put on.",
         "70"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145322</th>\n",
       "      <td>This Hub was clearly designed for Apple comput...</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110436</th>\n",
       "      <td>Works as advertised</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301223</th>\n",
       "      <td>Good so far! Only had for 1 week. Clips for st...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  review_len\n",
       "145322   This Hub was clearly designed for Apple comput...         509\n",
       "1110436                                Works as advertised          19\n",
       "301223   Good so far! Only had for 1 week. Clips for st...          70"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "if PARQUET_NORM.exists():\n",
    "    df = pd.read_parquet(PARQUET_NORM)\n",
    "    print(\"‚úì Charg√© depuis :\", PARQUET_NORM)\n",
    "elif CSV_NORM.exists():\n",
    "    df = pd.read_csv(CSV_NORM)\n",
    "    print(\"‚úì Charg√© depuis :\", CSV_NORM)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Dataset normalis√© introuvable. Ex√©cute C5.1.1 (cellule de sauvegarde).\")\n",
    "\n",
    "# √©chantillonnage (option)\n",
    "if SAMPLE_N is not None and len(df) > SAMPLE_N:\n",
    "    df = df.sample(SAMPLE_N, random_state=RANDOM_STATE)\n",
    "\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "df[\"review_len\"] = df[TEXT_COL].str.len()\n",
    "print(\"Shape:\", df.shape)\n",
    "df[[TEXT_COL, \"review_len\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d3e59",
   "metadata": {},
   "source": [
    "## üßº 2) Pr√©traitements contr√¥l√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7257391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bang_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ques_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "upper_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "10cd7037-fdb8-4de7-a878-12a573a7d3f4",
       "rows": [
        [
         "145322",
         "this hub was clearly designed for apple computers. the design, quality and materials used are all top notch. the only 2 short comings i can think of are 1) the usb cable is a very short and 2) it's not usb 3.0 compatible.<br /><br />hopefully they will make a new version that has usb 3.0 and maybe include a few thunderbolt ports. for some reason this product is not listed on the manufacturers website. it may be a specially designed product for the seller (battery world) who sells them here on amazon.",
         "92",
         "0",
         "0",
         "0.043222003929273084"
        ],
        [
         "1110436",
         "works as advertised",
         "3",
         "0",
         "0",
         "0.05263157894736842"
        ],
        [
         "301223",
         "good so far! only had for 1 week. clips for straps are hard to put on.",
         "16",
         "1",
         "0",
         "0.04285714285714286"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>ques_count</th>\n",
       "      <th>upper_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145322</th>\n",
       "      <td>this hub was clearly designed for apple comput...</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110436</th>\n",
       "      <td>works as advertised</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301223</th>\n",
       "      <td>good so far! only had for 1 week. clips for st...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text_clean  word_count  \\\n",
       "145322   this hub was clearly designed for apple comput...          92   \n",
       "1110436                                works as advertised           3   \n",
       "301223   good so far! only had for 1 week. clips for st...          16   \n",
       "\n",
       "         bang_count  ques_count  upper_ratio  \n",
       "145322            0           0     0.043222  \n",
       "1110436           0           0     0.052632  \n",
       "301223            1           0     0.042857  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def basic_clean(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Exemple : ne pas supprimer la ponctuation ici (laissons TF‚ÄëIDF char l'exploiter)\n",
    "df[\"text_clean\"] = df[TEXT_COL].map(basic_clean)\n",
    "df[\"word_count\"] = df[\"text_clean\"].str.split().str.len()\n",
    "df[\"bang_count\"] = df[TEXT_COL].str.count(\"!\")\n",
    "df[\"ques_count\"] = df[TEXT_COL].str.count(\"\\?\")\n",
    "df[\"upper_ratio\"] = df[TEXT_COL].apply(lambda x: (sum(1 for ch in x if ch.isupper()) / max(1,len(x))))\n",
    "df[[\"text_clean\",\"word_count\",\"bang_count\",\"ques_count\",\"upper_ratio\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192f86e",
   "metadata": {},
   "source": [
    "## üéØ 3) Label binaire (‚â•4 = positif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d4e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives ratio: 0.761405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "y = (pd.to_numeric(df[LABEL_COL], errors=\"coerce\") >= 4).astype(int).values\n",
    "print(\"Positives ratio:\", y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c70f840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: neg=23.86% | pos=76.14%\n",
      "class_weight: {0: 2.0956013328024476, 1: 0.6566807415238933}\n",
      "sample_weight shape: (200000,) | min/max: 0.6566807415238933 2.0956013328024476\n"
     ]
    }
   ],
   "source": [
    "# 3) Statistiques de classes + weights pour l'entra√Ænement\n",
    "import numpy as np, json\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "\n",
    "print(f\"Distribution: neg={((y==0).mean()):.2%} | pos={((y==1).mean()):.2%}\")\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# Vecteur de poids par √©chantillon (align√© sur y)\n",
    "sample_weight = np.where(y == 1, class_weight[1], class_weight[0])\n",
    "print(\"sample_weight shape:\", sample_weight.shape, \"| min/max:\", sample_weight.min(), sample_weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35b9d0",
   "metadata": {},
   "source": [
    "## üî§ 4) TF‚ÄëIDF mots ‚Äî vectorisation principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47c09f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 60000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) TF-IDF mots ‚Äî vectorisation principale (sans fonction pickl√©e)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec_word = TfidfVectorizer(\n",
    "    lowercase=False,                 # d√©j√† en minuscules via text_clean\n",
    "    stop_words=\"english\",\n",
    "    max_features=WORD_MAX_FEATURES,\n",
    "    ngram_range=WORD_NGRAM_RANGE,\n",
    "    min_df=WORD_MIN_DF,\n",
    ")\n",
    "\n",
    "X_word = vec_word.fit_transform(df[\"text_clean\"])  # ‚úÖ on utilise le texte nettoy√©\n",
    "X_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230f671",
   "metadata": {},
   "source": [
    "## üî° 5) (Option) TF‚ÄëIDF caract√®res ‚Äî compl√©ment morpho/ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d24cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape: (200000, 180000) (word + char)\n"
     ]
    }
   ],
   "source": [
    "# 5) TF-IDF caract√®res ‚Äî compl√©ment morpho/ponctuation (sans fonction pickl√©e)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "if USE_CHAR_TFIDF:\n",
    "    vec_char = TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        lowercase=False,                # d√©j√† en minuscules via text_clean\n",
    "        max_features=CHAR_MAX_FEATURES,\n",
    "        ngram_range=CHAR_NGRAM_RANGE,\n",
    "        min_df=CHAR_MIN_DF,\n",
    "    )\n",
    "    X_char = vec_char.fit_transform(df[\"text_clean\"])  # ‚úÖ texte nettoy√©\n",
    "    # Concat√©ner horizontalement (word | char)\n",
    "    X_all = sparse.hstack([X_word, X_char], format=\"csr\")\n",
    "    print(\"X_all shape:\", X_all.shape, \"(word + char)\")\n",
    "else:\n",
    "    vec_char = None\n",
    "    X_all = X_word\n",
    "    print(\"X_all shape:\", X_all.shape, \"(word only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f1092",
   "metadata": {},
   "source": [
    "## üßÆ 6) Features *hand‚Äëcrafted* (indicateurs simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17f00fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajout meta: ['word_count', 'review_len', 'bang_count', 'ques_count', 'upper_ratio', 'helpful_vote', 'verified_purchase'] | X_all: (200000, 180007)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Metadonn√©es disponibles (optionnelles)\n",
    "meta_cols = [c for c in [\"word_count\",\"review_len\",\"bang_count\",\"ques_count\",\"upper_ratio\",\n",
    "                         \"helpful_vote\",\"verified_purchase\"] if c in df.columns]\n",
    "meta = df[meta_cols].copy() if meta_cols else None\n",
    "\n",
    "if meta is not None:\n",
    "    # Remplissage / standardisation l√©g√®re\n",
    "    for c in meta_cols:\n",
    "        if meta[c].dtype == \"bool\":\n",
    "            meta[c] = meta[c].astype(int)\n",
    "        else:\n",
    "            meta[c] = pd.to_numeric(meta[c], errors=\"coerce\").fillna(0.0)\n",
    "    M = sparse.csr_matrix(meta.values)\n",
    "    X_all = sparse.hstack([X_all, M], format=\"csr\")\n",
    "    print(\"Ajout meta:\", meta_cols, \"| X_all:\", X_all.shape)\n",
    "else:\n",
    "    print(\"Pas de colonnes meta ajout√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc685411",
   "metadata": {},
   "source": [
    "## üíæ 7) Sauvegarde des artefacts de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0225fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefact TF-IDF ‚Üí models\\features_tfidf_20250912_125553.joblib\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "import time, json\n",
    "\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "art = {\n",
    "    \"vectorizer_word\": vec_word,\n",
    "    \"vectorizer_char\": vec_char,\n",
    "    \"feature_meta_cols\": meta_cols,\n",
    "    \"created\": stamp,\n",
    "    \"params\": {\n",
    "        \"word_max_features\": WORD_MAX_FEATURES,\n",
    "        \"word_ngram_range\": WORD_NGRAM_RANGE,\n",
    "        \"word_min_df\": WORD_MIN_DF,\n",
    "        \"use_char_tfidf\": USE_CHAR_TFIDF,\n",
    "        \"char_max_features\": CHAR_MAX_FEATURES,\n",
    "        \"char_ngram_range\": CHAR_NGRAM_RANGE,\n",
    "        \"char_min_df\": CHAR_MIN_DF,\n",
    "    }\n",
    "}\n",
    "art_path = ART_DIR / f\"features_tfidf_{stamp}.joblib\"\n",
    "dump(art, art_path)\n",
    "print(\"Artefact TF-IDF ‚Üí\", art_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1741c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âcrit ‚Üí data\\processed\\binary_label_metadata.json\n",
      "√âcrit ‚Üí data\\processed\\class_weight.json\n",
      "√âcrit ‚Üí data\\processed\\sample_weight_binary.joblib\n"
     ]
    }
   ],
   "source": [
    "# 7) Sauvegarde des poids et m√©ta pour la suite (C5.2.3)\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "import time, json\n",
    "\n",
    "stamp2 = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "META_PATH = DATA_PROC_DIR / \"binary_label_metadata.json\"\n",
    "W_VEC_PATH = DATA_PROC_DIR / \"sample_weight_binary.joblib\"\n",
    "CW_JSON   = DATA_PROC_DIR / \"class_weight.json\"\n",
    "\n",
    "meta = {\n",
    "    \"n_samples\": int(len(y)),\n",
    "    \"prevalence_pos\": float((y==1).mean()),\n",
    "    \"class_weight\": {k: float(v) for k, v in class_weight.items()},\n",
    "    \"created\": stamp2,\n",
    "}\n",
    "\n",
    "META_PATH.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "(Path(CW_JSON)).write_text(json.dumps(meta[\"class_weight\"], indent=2), encoding=\"utf-8\")\n",
    "dump(sample_weight, W_VEC_PATH)\n",
    "\n",
    "print(\"√âcrit ‚Üí\", META_PATH)\n",
    "print(\"√âcrit ‚Üí\", CW_JSON)\n",
    "print(\"√âcrit ‚Üí\", W_VEC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8fb4c",
   "metadata": {},
   "source": [
    "## üì¶ 8) Export d‚Äôun √©chantillon (X, y) exploitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9cbb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âcrit : data\\processed\\X_tfidf_sample.npz et data\\processed\\y_binary_sample.joblib | shape: (120000, 180007)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import sparse\n",
    "from joblib import dump\n",
    "\n",
    "X_path = DATA_PROC_DIR / \"X_tfidf_sample.npz\"\n",
    "y_path = DATA_PROC_DIR / \"y_binary_sample.joblib\"\n",
    "\n",
    "# On exporte un √©chantillon compact (pour entra√Ænements rapides / d√©mos)\n",
    "sample_k = min(X_all.shape[0], 120_000)\n",
    "X_sample = X_all[:sample_k]\n",
    "y_sample = y[:sample_k]\n",
    "\n",
    "sparse.save_npz(X_path, X_sample, compressed=True)\n",
    "dump(y_sample, y_path)\n",
    "print(\"√âcrit :\", X_path, \"et\", y_path, \"| shape:\", X_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004171b7",
   "metadata": {},
   "source": [
    "## üîç 9) Aper√ßu top n‚Äëgrammes (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa85f0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 2.4375084485141327),\n",
       " ('good', 2.8355909727561093),\n",
       " ('works', 2.951440385085651),\n",
       " ('use', 2.983627400496733),\n",
       " ('just', 3.082731932658865),\n",
       " ('br', 3.0985885871957026),\n",
       " ('like', 3.1066944161604733),\n",
       " ('product', 3.1845848597906405),\n",
       " ('quality', 3.264090793830936),\n",
       " ('easy', 3.2964589270513054),\n",
       " ('work', 3.3317614702818257),\n",
       " ('price', 3.435378904245009),\n",
       " ('love', 3.440186880995992),\n",
       " ('sound', 3.5449164637125294),\n",
       " ('br br', 3.5450438928302552),\n",
       " ('time', 3.5651893850658634),\n",
       " ('bought', 3.5656445956085965),\n",
       " ('really', 3.635688028963809),\n",
       " ('don', 3.7053837725345513),\n",
       " ('case', 3.7701965976275598)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "vocab = {i:t for t,i in vec_word.vocabulary_.items()}\n",
    "idf = vec_word.idf_\n",
    "top_idx = np.argsort(idf)[:20]  # plus fr√©quents (idf bas)\n",
    "[(vocab[i], float(idf[i])) for i in top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d7ad5",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Rappel √† la comp√©tence & validation\n",
    "\n",
    "**Comp√©tence :** *C5.2.1 ‚Äî Construire des variables* (**√©liminatoire**)  \n",
    "**Pourquoi c‚Äôest valid√© :**\n",
    "- Les **recettes de features** sont **claires et param√©tr√©es** (TF‚ÄëIDF mots/char, meta), reproductibles.\n",
    "- Les **artefacts** sont **sauvegard√©s** (`models/features_tfidf_*.joblib`) et un **jeu de donn√©es exploitable** est g√©n√©r√©\n",
    "  (`data/processed/X_tfidf_sample.npz`, `y_binary_sample.joblib`).\n",
    "- Les **choix** sont justifi√©s (mots = s√©mantique, caract√®res = orthographe/ponctuation/sarcasme l√©ger, indicateurs = style/qualit√©).\n",
    "- La d√©marche est **scalable** (SAMPLE_N contr√¥lable) et **compatible** avec C5.2.3 (entra√Ænement du mod√®le)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255bd7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
