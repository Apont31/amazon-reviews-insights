{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204e613e",
   "metadata": {},
   "source": [
    "# üß† C5.2.3 ‚Äî Entra√Æner les mod√®les (global + √©motions + sarcasme + ABSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a3255",
   "metadata": {},
   "source": [
    "> Objectif : entra√Æner des mod√®les robuste sur X/y, choisir ses hyperparam√®tres, fixer un seuil d√©cisionnel, mesurer la g√©n√©ralisation sur un jeu de test tenu √† part, et s√©rialiser les mod√®les pour l‚Äôinf√©rence sur de nouvelles donn√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1c12d",
   "metadata": {},
   "source": [
    "On n‚Äôa pas ‚Äúr√©√©quilibr√© physiquement‚Äù (over/under-sampling) parce que :\n",
    "\n",
    "On a d√©j√† compens√© le d√©s√©quilibre dans l‚Äôapprentissage avec\n",
    "class_weight=\"balanced\" dans la LogReg.\n",
    "‚Üí Pour une r√©gression logistique, c‚Äôest √©quivalent √† r√©pliquer/pond√©rer les exemples des classes rares sans dupliquer la data en m√©moire.\n",
    "\n",
    "On a d√©coup√© en stratifi√© (train/val/test) ‚Üí les proportions sont stables et comparables.\n",
    "\n",
    "On r√®gle le seuil (t*‚âà0.27) pour atteindre le compromis m√©tier (pr√©cision/recall) plut√¥t que de bricoler la distribution.\n",
    "\n",
    "La donn√©e est tr√®s grande : faire de l‚Äôoversampling gonfle le temps et la RAM et peut sur-apprendre des duplicatas.\n",
    "L‚Äôundersampling, lui, jette de l‚Äôinformation.\n",
    "\n",
    "Nos m√©triques (PR/AP, F1, confusion matrices) montrent d√©j√† un mod√®le tr√®s performant malgr√© ~76% de positifs.\n",
    "\n",
    "Bref : class_weight + stratified split + choix du seuil = la mani√®re propre et scalable de g√©rer le d√©s√©quilibre ici."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0adf75",
   "metadata": {},
   "source": [
    "## 0) Setup & chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e14468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# 0) Setup\n",
    "from pathlib import Path\n",
    "import time, json, numpy as np, pandas as pd, torch\n",
    "from joblib import load, dump\n",
    "from scipy import sparse\n",
    "\n",
    "DATA_DIR = Path(\"data/processed\")\n",
    "MODELS_DIR = Path(\"models\"); MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PSEUDO_DIR = Path(\"data/pseudo\"); PSEUDO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_GPU else -1\n",
    "RNG = 42\n",
    "\n",
    "print(\"CUDA:\", USE_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d173b1f",
   "metadata": {},
   "source": [
    "## A) Sentiment global (binaire) ‚Äî r√©utilisation rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "430080f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dispo: ['X_tfidf_sample.npz']\n",
      "y dispo: ['y_binary_sample.joblib']\n"
     ]
    }
   ],
   "source": [
    "# Imports & chemins\n",
    "from pathlib import Path\n",
    "import time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             average_precision_score, precision_recall_curve, roc_curve,\n",
    "                             f1_score, PrecisionRecallDisplay)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import load, dump\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RNG = 42\n",
    "ART_DIR = Path(\"models\")             # l√† o√π tu as sauvegard√© sel / sfm / svd_pipe\n",
    "DATA_DIR = Path(\"data/processed\")    # l√† o√π sont X_tfidf_*.npz et y_*.joblib si existants\n",
    "OUT_DIR = Path(\"models\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Trouver les derniers X/y produits en 5.2.1/5.2.2\n",
    "X_CAND = sorted(DATA_DIR.glob(\"X_tfidf*.npz\"))\n",
    "Y_CAND = sorted(DATA_DIR.glob(\"y_*binary*.joblib\"))\n",
    "print(\"X dispo:\", [p.name for p in X_CAND][-3:])\n",
    "print(\"y dispo:\", [p.name for p in Y_CAND][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ec2c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total textes: 1,314,720  |  colonne utilis√©e: review_body\n",
      "Fit TF-IDF sur 200,000 textes‚Ä¶\n",
      "Vocab sizes -> word: 200,000 | char: 100,000 | elapsed: 93.2s\n",
      "Stacked sample shape: (2000, 300000)\n",
      "‚úî TF-IDF runtime sauvegard√© -> models/features_tfidf_runtime_full_20250922_035127.joblib\n"
     ]
    }
   ],
   "source": [
    "# === TF-IDF : fit sur un √©chantillon de 200k avis et sauvegarde en runtime ===\n",
    "from pathlib import Path\n",
    "from joblib import dump\n",
    "import numpy as np, pandas as pd, time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "DATA_DIR  = Path(\"data/processed\")\n",
    "MODELS_DIR = Path(\"models\"); MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Charger le texte brut (chemins possibles)\n",
    "candidates = [\n",
    "    DATA_DIR / \"reviews_norm.parquet\",  # ton export normalis√© si dispo\n",
    "    DATA_DIR / \"amazon_electronics_normalized.parquet\",  # fallback\n",
    "]\n",
    "PARQUET = None\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        PARQUET = p\n",
    "        break\n",
    "assert PARQUET is not None, f\"Aucun parquet texte trouv√© dans {candidates}\"\n",
    "\n",
    "df = pd.read_parquet(PARQUET)\n",
    "col = \"review_body\" if \"review_body\" in df.columns else df.columns[0]\n",
    "texts_all = df[col].astype(str).to_numpy()\n",
    "n_total = len(texts_all)\n",
    "print(f\"Total textes: {n_total:,}  |  colonne utilis√©e: {col}\")\n",
    "\n",
    "# 2) √âchantillonner 200k (ou tout si <200k)\n",
    "N_FIT = min(200_000, n_total)\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.choice(n_total, size=N_FIT, replace=False)\n",
    "texts = texts_all[idx]\n",
    "print(f\"Fit TF-IDF sur {len(texts):,} textes‚Ä¶\")\n",
    "\n",
    "# 3) Param√®tres raisonnables (16 Go RAM)\n",
    "params_word = dict(\n",
    "    analyzer=\"word\", ngram_range=(1, 2), lowercase=True, strip_accents=\"unicode\",\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\", min_df=2, max_df=0.95, max_features=200_000,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "params_char = dict(\n",
    "    analyzer=\"char\", ngram_range=(3, 5), lowercase=False, strip_accents=None,\n",
    "    min_df=5, max_df=0.95, max_features=100_000, sublinear_tf=True\n",
    ")\n",
    "\n",
    "# 4) Fit des deux vectorizers\n",
    "t0 = time.time()\n",
    "vw = TfidfVectorizer(**params_word).fit(texts)\n",
    "vc = TfidfVectorizer(**params_char).fit(texts)\n",
    "print(f\"Vocab sizes -> word: {len(vw.vocabulary_):,} | char: {len(vc.vocabulary_):,} | elapsed: {time.time()-t0:.1f}s\")\n",
    "\n",
    "# (sanity) transformer un mini batch pour v√©rifier le stack\n",
    "Xs = hstack([vw.transform(texts[:2000]), vc.transform(texts[:2000])], format=\"csr\")\n",
    "print(\"Stacked sample shape:\", Xs.shape)\n",
    "\n",
    "# 5) Sauvegarde artefact runtime unifi√©\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "outp = MODELS_DIR / f\"features_tfidf_runtime_full_{stamp}.joblib\"\n",
    "dump({\n",
    "    \"vectorizer_word\": vw,\n",
    "    \"vectorizer_char\": vc,\n",
    "    \"created\": stamp,\n",
    "    \"params\": {\"word\": params_word, \"char\": params_char, \"n_fit\": int(N_FIT), \"source\": PARQUET.name}\n",
    "}, outp, compress=3)\n",
    "print(\"‚úî TF-IDF runtime sauvegard√© ->\", outp.as_posix())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3df9c",
   "metadata": {},
   "source": [
    "## üì• 1) Charger X/y (ou √©chouer clairement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d776b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement: X_tfidf_sample.npz | y_binary_sample.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((120000, 180007), (120000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert X_CAND and Y_CAND, \"Pas de features X / labels y trouv√©s. Ex√©cute 5.2.1 pour les g√©n√©rer.\"\n",
    "X_path = X_CAND[-1]; y_path = Y_CAND[-1]\n",
    "print(\"Chargement:\", X_path.name, \"|\", y_path.name)\n",
    "\n",
    "X_all = sparse.load_npz(X_path)\n",
    "y_all = load(y_path).astype(int)\n",
    "X_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94543b15",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 2) Split train / val / test (stratifi√©, stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53378ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (96000, 180007) | val: (12000, 180007) | test: (12000, 180007) | pos ratio train: 0.7614583333333333\n"
     ]
    }
   ],
   "source": [
    "# 80/10/10 (ou adapte selon tes besoins)\n",
    "X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "    X_all, y_all, test_size=0.20, stratify=y_all, random_state=RNG\n",
    ")\n",
    "X_va, X_te, y_va, y_te = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.50, stratify=y_tmp, random_state=RNG\n",
    ")\n",
    "print(\"train:\", X_tr.shape, \"| val:\", X_va.shape, \"| test:\", X_te.shape, \"| pos ratio train:\", y_tr.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b2125",
   "metadata": {},
   "source": [
    "## üß∞ 3) Charger les s√©lecteurs (chi¬≤, L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e82a72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi¬≤ charg√©: selector_chi2_k50000_20250912_215349.joblib\n",
      "L1 charg√©: selector_l1_after_chi2_k50000_kept25000_20250912_215349.joblib\n",
      "SVD charg√©: reducer_svd_k300_20250912_215349.joblib\n",
      "Apr√®s chi¬≤: (96000, 50000)\n"
     ]
    }
   ],
   "source": [
    "# Essaie de charger le dernier chi¬≤ et le dernier L1 (optionnels)\n",
    "sel = None; sfm = None; svd_pipe = None\n",
    "\n",
    "CHI = sorted(ART_DIR.glob(\"selector_chi2_*.joblib\"))\n",
    "if CHI:\n",
    "    sel = load(CHI[-1])[\"chi2\"]\n",
    "    print(\"chi¬≤ charg√©:\", CHI[-1].name)\n",
    "\n",
    "L1S = sorted(ART_DIR.glob(\"selector_l1_after_chi2_*.joblib\"))\n",
    "if L1S:\n",
    "    sfm = load(L1S[-1])[\"l1_sfm\"]\n",
    "    print(\"L1 charg√©:\", L1S[-1].name)\n",
    "\n",
    "SVD = sorted(ART_DIR.glob(\"reducer_svd_*.joblib\"))\n",
    "if SVD:\n",
    "    svd_pipe = load(SVD[-1])[\"svd_pipe\"]\n",
    "    print(\"SVD charg√©:\", SVD[-1].name)\n",
    "\n",
    "# Appliquer chi¬≤ (pipeline retenu dans ton analyse)\n",
    "if sel is not None:\n",
    "    X_tr_ = sel.transform(X_tr); X_va_ = sel.transform(X_va); X_te_ = sel.transform(X_te)\n",
    "    print(\"Apr√®s chi¬≤:\", X_tr_.shape)\n",
    "else:\n",
    "    X_tr_, X_va_, X_te_ = X_tr, X_va, X_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301fb5e",
   "metadata": {},
   "source": [
    "## üéØ 4) Entra√Ænement : GridSearch (rapide) + choix du seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db5a54c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     21\u001b[39m gs = GridSearchCV(\n\u001b[32m     22\u001b[39m     base, param_grid=param_grid, scoring=\u001b[33m\"\u001b[39m\u001b[33maverage_precision\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     cv=cv, n_jobs=\u001b[32m1\u001b[39m, pre_dispatch=\u001b[32m1\u001b[39m, refit=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m1\u001b[39m  \u001b[38;5;66;03m# <- pas de parall√©lisme pour √©viter les copies g√©antes\u001b[39;00m\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m, gs.best_params_, \u001b[33m\"\u001b[39m\u001b[33m| best AP (CV):\u001b[39m\u001b[33m\"\u001b[39m, gs.best_score_, \u001b[33m\"\u001b[39m\u001b[33m| fit:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-t0\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 2) Refit final sur train+val avec le meilleur C\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:859\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    857\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    863\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1384\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1382\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1410\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:459\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    455\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    456\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    457\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    458\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_get_additional_lbfgs_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    474\u001b[39m     solver,\n\u001b[32m    475\u001b[39m     opt_res,\n\u001b[32m    476\u001b[39m     max_iter,\n\u001b[32m    477\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    479\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:784\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    781\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    782\u001b[39m                              **options)\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    787\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    788\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:403\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:353\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\_lib\\_util.py:590\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:312\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    309\u001b[39m n_dof = n_features + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.fit_intercept)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raw_prediction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     weights, intercept, raw_prediction = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_intercept_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    314\u001b[39m     weights, intercept = \u001b[38;5;28mself\u001b[39m.weight_intercept(coef)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:200\u001b[39m, in \u001b[36mLinearModelLoss.weight_intercept_raw\u001b[39m\u001b[34m(self, coef, X)\u001b[39m\n\u001b[32m    197\u001b[39m weights, intercept = \u001b[38;5;28mself\u001b[39m.weight_intercept(coef)\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_loss.is_multiclass:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     raw_prediction = \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m + intercept\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# weights has shape (n_classes, n_dof)\u001b[39;00m\n\u001b[32m    203\u001b[39m     raw_prediction = X @ weights.T + intercept  \u001b[38;5;66;03m# ndarray, likely C-contiguous\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:908\u001b[39m, in \u001b[36m_spbase.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScalar operands are not allowed, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    907\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33muse \u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matmul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:793\u001b[39m, in \u001b[36m_spbase._matmul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m other.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m np.ndarray:\n\u001b[32m    791\u001b[39m     \u001b[38;5;66;03m# Fast path for the most common case\u001b[39;00m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m other.shape == (N,):\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matmul_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m other.shape == (N, \u001b[32m1\u001b[39m):\n\u001b[32m    795\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._matmul_vector(other.ravel())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:395\u001b[39m, in \u001b[36m_cs_matrix._matmul_vector\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# csr_matvec or csc_matvec\u001b[39;00m\n\u001b[32m    394\u001b[39m fn = \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m.format + \u001b[33m'\u001b[39m\u001b[33m_matvec\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Petite grille sur C (r√©gularisation), solver lbfgs (le plus performant)\n",
    "from scipy import sparse\n",
    "import numpy as np, time\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 0) Formats m√©moire (idempotent : tu peux relancer sans risque)\n",
    "X_tr_ = X_tr_.tocsr().astype(np.float32)\n",
    "X_va_ = X_va_.tocsr().astype(np.float32)\n",
    "y_tr  = np.asarray(y_tr, dtype=np.int8)\n",
    "y_va  = np.asarray(y_va, dtype=np.int8)\n",
    "\n",
    "# 1) Petite grille sur C (lbfgs le plus performant chez toi)\n",
    "param_grid = {\"C\": [0.25, 0.5, 1.0, 2.0]}\n",
    "base = LogisticRegression(\n",
    "    solver=\"lbfgs\", penalty=\"l2\", class_weight=\"balanced\",\n",
    "    max_iter=5000, tol=1e-3\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RNG)\n",
    "gs = GridSearchCV(\n",
    "    base, param_grid=param_grid, scoring=\"average_precision\",\n",
    "    cv=cv, n_jobs=1, pre_dispatch=1, refit=True, verbose=1  # <- pas de parall√©lisme pour √©viter les copies g√©antes\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_tr_, y_tr)\n",
    "print(\"Best params:\", gs.best_params_, \"| best AP (CV):\", gs.best_score_, \"| fit:\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "# 2) Refit final sur train+val avec le meilleur C\n",
    "best = LogisticRegression(\n",
    "    solver=\"lbfgs\", penalty=\"l2\", class_weight=\"balanced\",\n",
    "    C=gs.best_params_[\"C\"], max_iter=5000, tol=1e-3\n",
    ").fit(sparse.vstack([X_tr_, X_va_]), np.hstack([y_tr, y_va]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8afc1",
   "metadata": {},
   "source": [
    "## üìè 5) √âvaluation finale sur test (jamais vu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuil optimal (validation) t* = 0.301\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "setting",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "threshold",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ROC_AUC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BalancedAcc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Specificity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NPV",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TP",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FP",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FN",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TN",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cc1d056c-6a70-434d-bb94-1ee5f938d4d3",
       "rows": [
        [
         "0",
         "Test @0.5",
         "0.5",
         "0.9845602979739532",
         "0.9565505576215143",
         "0.8968333333333334",
         "0.8937593387206222",
         "0.962416578854935",
         "0.8996388311261903",
         "0.9299694535580948",
         "0.887879846314744",
         "0.7348944781726698",
         "8220",
         "321",
         "917",
         "2542"
        ],
        [
         "1",
         "Test @t*",
         "0.3014459013938904",
         "0.9845602979739532",
         "0.9565505576215143",
         "0.9079166666666667",
         "0.8637424968646823",
         "0.9320137693631669",
         "0.9482324614205976",
         "0.9400531655183638",
         "0.7792525323084949",
         "0.8250739644967363",
         "8664",
         "632",
         "473",
         "2231"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting</th>\n",
       "      <th>threshold</th>\n",
       "      <th>AP</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>BalancedAcc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>NPV</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test @0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.98456</td>\n",
       "      <td>0.956551</td>\n",
       "      <td>0.896833</td>\n",
       "      <td>0.893759</td>\n",
       "      <td>0.962417</td>\n",
       "      <td>0.899639</td>\n",
       "      <td>0.929969</td>\n",
       "      <td>0.887880</td>\n",
       "      <td>0.734894</td>\n",
       "      <td>8220</td>\n",
       "      <td>321</td>\n",
       "      <td>917</td>\n",
       "      <td>2542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test @t*</td>\n",
       "      <td>0.301446</td>\n",
       "      <td>0.98456</td>\n",
       "      <td>0.956551</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.863742</td>\n",
       "      <td>0.932014</td>\n",
       "      <td>0.948232</td>\n",
       "      <td>0.940053</td>\n",
       "      <td>0.779253</td>\n",
       "      <td>0.825074</td>\n",
       "      <td>8664</td>\n",
       "      <td>632</td>\n",
       "      <td>473</td>\n",
       "      <td>2231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setting  threshold       AP   ROC_AUC  Accuracy  BalancedAcc  Precision  \\\n",
       "0  Test @0.5   0.500000  0.98456  0.956551  0.896833     0.893759   0.962417   \n",
       "1   Test @t*   0.301446  0.98456  0.956551  0.907917     0.863742   0.932014   \n",
       "\n",
       "     Recall        F1  Specificity       NPV    TP   FP   FN    TN  \n",
       "0  0.899639  0.929969     0.887880  0.734894  8220  321  917  2542  \n",
       "1  0.948232  0.940053     0.779253  0.825074  8664  632  473  2231  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix ‚Äî Test @0.5 (thr=0.50)\n",
      "[[2542  321]\n",
      " [ 917 8220]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.735     0.888     0.804      2863\n",
      "           1      0.962     0.900     0.930      9137\n",
      "\n",
      "    accuracy                          0.897     12000\n",
      "   macro avg      0.849     0.894     0.867     12000\n",
      "weighted avg      0.908     0.897     0.900     12000\n",
      "\n",
      "\n",
      "Confusion matrix ‚Äî Test @t* (thr=0.30)\n",
      "[[2231  632]\n",
      " [ 473 8664]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.825     0.779     0.802      2863\n",
      "           1      0.932     0.948     0.940      9137\n",
      "\n",
      "    accuracy                          0.908     12000\n",
      "   macro avg      0.879     0.864     0.871     12000\n",
      "weighted avg      0.906     0.908     0.907     12000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAHHCAYAAAAoIIjLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABat0lEQVR4nO3dd3hT1f8H8HeSNmm6KV0UCmWXvSmlAo5KBURRkFWgbFSW9IsCsmdFFFFkKCoIPxQEWTKKUECmimwoIKOlZbSU0b2T8/uj9kLoIClJ05L363nymHvuOJ8caT65955zj0wIIUBERGRh5OYOgIiIyByYAImIyCIxARIRkUViAiQiIovEBEhERBaJCZCIiCwSEyAREVkkJkAiIrJITIBERGSRmADJog0cOBA+Pj4G7XPgwAHIZDIcOHDAJDE9LwprJ0PaW6vVomHDhpg7d65pAiyB5cuXo2rVqsjKyjJ3KGQETIBUqlatWgWZTCa9bGxsUKdOHYwaNQrx8fHmDq9cGThwoE5bqlQq1KlTB9OmTUNmZqa5w3tmP//8M2JjYzFq1Cip7OjRo5gxYwYSExNNWve8efOwZcuWAuUDBw5EdnY2vvnmG5PWT6XDytwBkGWaNWsWqlevjszMTBw+fBjLli3Dzp07cf78edja2pZaHCtWrIBWqzVon/bt2yMjIwNKpdJEUelPpVLhu+++AwAkJSVh69atmD17Nq5du4a1a9eaObpns2DBAvTu3RtOTk5S2dGjRzFz5kwMHDgQzs7OJqt73rx56NGjB7p166ZTbmNjg5CQECxcuBCjR4+GTCYzWQxkejwDJLPo1KkT+vXrh6FDh2LVqlX44IMPEBUVha1btxa5T1pamtHjsLa2hkqlMmgfuVwOGxsbyOXm//OxsrJCv3790K9fP4wcORK7d+9GmzZt8PPPP5frM+pTp07hzJkz6Nmzp7lDKaBnz564ceMG9u/fb+5Q6BmZ/y+YCMDLL78MAIiKigKQd6nJ3t4e165dQ+fOneHg4IDg4GAAefeGFi1ahAYNGsDGxgYeHh4YMWIEHj58WOC4u3btQocOHeDg4ABHR0e0atUKP/30k7S+sHtS69atQ4sWLaR9GjVqhC+//FJaX9Q9wA0bNqBFixZQq9VwdXVFv379cOvWLZ1t8j/XrVu30K1bN9jb28PNzQ3jx4+HRqMpcfvlk8lkeOGFFyCEwPXr1wu0Rbt27WBnZwcHBwd06dIFFy5cKHCMS5cuoWfPnnBzc4NarUbdunUxefJkaf2NGzfw/vvvo27dulCr1ahYsSLeeecdREdHP3P8+bZs2QKlUon27dtLZTNmzMCHH34IAKhevbp06ffxev/v//5P+n/g4uKC3r17IzY2VufYV65cQffu3eHp6QkbGxtUqVIFvXv3RlJSEoC8NkxLS8OPP/4o1TFw4EBp/xYtWsDFxaXYH2tUPvASKJUJ165dAwBUrFhRKsvNzUVQUBBeeOEFfPbZZ9Kl0REjRmDVqlUYNGgQxowZg6ioKHz99dc4deoUjhw5AmtrawB59xsHDx6MBg0aYNKkSXB2dsapU6cQHh6Ovn37FhrHnj170KdPH7zyyiuYP38+AODixYs4cuQIxo4dW2T8+fG0atUKYWFhiI+Px5dffokjR47g1KlTOpfrNBoNgoKC4Ofnh88++wx79+7F559/jpo1a+K99957pnYEICWEChUqSGVr1qxBSEgIgoKCMH/+fKSnp2PZsmV44YUXcOrUKelHwNmzZ9GuXTtYW1tj+PDh8PHxwbVr1/Dbb79JnVGOHz+Oo0ePonfv3qhSpQqio6OxbNkyvPjii4iMjDTKJeyjR4+iYcOG0v9LAHj77bfx77//4ueff8YXX3wBV1dXAICbmxsAYO7cuZg6dSp69uyJoUOHIiEhAYsXL0b79u2l/wfZ2dkICgpCVlYWRo8eDU9PT9y6dQvbt29HYmIinJycsGbNGgwdOhStW7fG8OHDAQA1a9bUia958+Y4cuTIM39OMjNBVIpWrlwpAIi9e/eKhIQEERsbK9atWycqVqwo1Gq1uHnzphBCiJCQEAFATJw4UWf/Q4cOCQBi7dq1OuXh4eE65YmJicLBwUH4+fmJjIwMnW21Wq30PiQkRFSrVk1aHjt2rHB0dBS5ublFfob9+/cLAGL//v1CCCGys7OFu7u7aNiwoU5d27dvFwDEtGnTdOoDIGbNmqVzzGbNmokWLVoUWWdhQkJChJ2dnUhISBAJCQni6tWr4rPPPhMymUw0bNhQ+pwpKSnC2dlZDBs2TGf/uLg44eTkpFPevn174eDgIG7cuKGz7eNtlp6eXiCWY8eOCQBi9erVUtmT7ZQf8+PtXZQqVaqI7t27FyhfsGCBACCioqJ0yqOjo4VCoRBz587VKT937pywsrKSyk+dOiUAiA0bNhRbv52dnQgJCSly/fDhw4VarX7q56CyjZdAySwCAwPh5uYGb29v9O7dG/b29ti8eTMqV66ss92TZ0QbNmyAk5MTXn31Vdy7d096tWjRAvb29tJ9mT179iAlJQUTJ06EjY2NzjGK67jg7OyMtLQ07NmzR+/P8s8//+Du3bt4//33derq0qULfH19sWPHjgL7vPvuuzrL7dq1K3DJUh9paWlwc3ODm5sbatWqhfHjxyMgIABbt26VPueePXuQmJiIPn366LSZQqGAn5+f1GYJCQk4ePAgBg8ejKpVq+rU83ibqdVq6X1OTg7u37+PWrVqwdnZGSdPnjT4MxTm/v37OmewT7Np0yZotVr07NlT5zN6enqidu3a0mfM71Cze/dupKenlzi+ChUqICMj45mOQebHS6BkFkuWLEGdOnVgZWUFDw8P1K1bt0CnEisrK1SpUkWn7MqVK0hKSoK7u3uhx7179y6AR5dUGzZsaFBc77//Pn755Rd06tQJlStXRseOHdGzZ0+89tprRe5z48YNAEDdunULrPP19cXhw4d1ymxsbKTLdvkqVKigcw8zISGh0HuCCoVCZ18bGxv89ttvAICbN2/i008/xd27d3WS1JUrVwA8us/6JEdHRwCQEvDT2iwjIwNhYWFYuXIlbt26BSGEtC7/PpoxPH7cp7ly5QqEEKhdu3ah6/MvpVavXh2hoaFYuHAh1q5di3bt2uGNN95Av379dHqb6hsbe4GWb0yAZBatW7dGy5Yti91GpVIVSIparRbu7u5FdvF/MrEYyt3dHadPn8bu3buxa9cu7Nq1CytXrsSAAQPw448/PtOx8ykUiqdu06pVKymxPq5atWo6nT4UCgUCAwOl5aCgIPj6+mLEiBHYtm0bAEjDPNasWQNPT88Cx7SyMuxrYPTo0Vi5ciU++OAD+Pv7w8nJCTKZDL179zZ4SElRKlasWGinpqJotVrIZDLs2rWr0Pa1t7eX3n/++ecYOHAgtm7dit9//x1jxoxBWFgY/vzzzwI/uIry8OFD2Nra6vzQoPKHCZDKlZo1a2Lv3r0ICAgo9ssnv9PC+fPnUatWLYPqUCqV6Nq1K7p27QqtVov3338f33zzDaZOnVrosapVqwYAuHz5coGzrMuXL0vrDbF27VpkZGQUKH/aF26lSpUwbtw4zJw5E3/++SfatGkjtYW7u7tOsnxSjRo1AOS1WXE2btyIkJAQfP7551JZZmamUQen+/r6Sj2CH1fUGVfNmjUhhED16tVRp06dpx6/UaNGaNSoEaZMmYKjR48iICAAy5cvx5w5c4qtJ19UVBTq1aunxyehsoz3AKlc6dmzJzQaDWbPnl1gXW5urvQl3LFjRzg4OCAsLKzAU1GKu7R2//59nWW5XI7GjRsDQJGPv2rZsiXc3d2xfPlynW127dqFixcvokuXLnp9tscFBAQgMDCwwCsgIOCp+44ePRq2trb45JNPAOSdFTo6OmLevHnIyckpsH1CQgKAvLPn9u3b44cffkBMTIzONo+3mUKhKNCGixcvNsowjnz+/v44f/58gTa3s7MDgALJ9u2334ZCocDMmTMLxCaEkP6/JicnIzc3V2d9o0aNIJfLdeqys7MrNqGfPHkSbdu2NfRjURnDM0AqVzp06IARI0YgLCwMp0+fRseOHWFtbY0rV65gw4YN+PLLL9GjRw84Ojriiy++wNChQ9GqVSv07dsXFSpUwJkzZ5Cenl7k5cyhQ4fiwYMHePnll1GlShXcuHEDixcvRtOmTYv8xW9tbY358+dj0KBB6NChA/r06SMNg/Dx8cG4ceNM2SQFVKxYEYMGDcLSpUtx8eJF1KtXD8uWLUP//v3RvHlz9O7dG25uboiJicGOHTsQEBCAr7/+GgDw1Vdf4YUXXkDz5s0xfPhwVK9eHdHR0dixYwdOnz4NAHj99dexZs0aODk5oX79+jh27Bj27t2rM4TlWb355puYPXs2/vjjD3Ts2FEqb9GiBQBg8uTJ6N27N6ytrdG1a1fUrFkTc+bMwaRJkxAdHY1u3brBwcEBUVFR2Lx5M4YPH47x48dj3759GDVqFN555x3UqVMHubm5WLNmDRQKBbp3765Tz969e7Fw4UJ4eXmhevXq8PPzAwCcOHECDx48wJtvvmm0z0tmYq7up2SZ8odBHD9+vNjt8rv4F+Xbb78VLVq0EGq1Wjg4OIhGjRqJjz76SNy+fVtnu23btom2bdsKtVotHB0dRevWrcXPP/+sU8/j3fI3btwoOnbsKNzd3YVSqRRVq1YVI0aMEHfu3JG2Kax7vxBCrF+/XjRr1kyoVCrh4uIigoODpWEdT/tc06dPF4b+ORbXRteuXRMKhUKnK//+/ftFUFCQcHJyEjY2NqJmzZpi4MCB4p9//tHZ9/z58+Ktt94Szs7OwsbGRtStW1dMnTpVWv/w4UMxaNAg4erqKuzt7UVQUJC4dOmSqFatWoH6nmwnfYdBCCFE48aNxZAhQwqUz549W1SuXFnI5fICQyJ+/fVX8cILLwg7OzthZ2cnfH19xciRI8Xly5eFEEJcv35dDB48WNSsWVPY2NgIFxcX8dJLL4m9e/fq1HHp0iXRvn17oVarBQCdzzVhwgRRtWpVnaEhVD7JhDCgqxURUSlZs2YNRo4ciZiYGJM+99MQWVlZ8PHxwcSJE4t9MAKVD7wHSERlUnBwMKpWrYolS5aYOxTJypUrYW1tXWAcJ5VPPAMkIiKLxDNAIiKySEyARERkkZgAiYjIIjEBEhGRRTLrQPiDBw9iwYIFOHHiBO7cuYPNmzejW7duxe5z4MABhIaG4sKFC/D29saUKVN0Jqt8Gq1Wi9u3b8PBwYEPsiUiKoeEEEhJSYGXl1eB5wUbwqwJMC0tDU2aNMHgwYPx9ttvP3X7qKgodOnSBe+++y7Wrl2LiIgIDB06FJUqVUJQUJBedd6+fRve3t7PGjoREZlZbGys3g8wL0yZGQYhk8meegY4YcIE7NixQ+dhvb1790ZiYiLCw8P1qicpKQnOzs6IjY2VpoEhIqLyIzk5Gd7e3khMTDRoGqsnlatngR47dqzA0+yDgoLwwQcf6H2M/Muejo6OsLKxxbFrug8/ruPhAG8X22eOlYiITOtZb2OVqwQYFxcHDw8PnTIPDw8kJycjIyOj0KlisrKydJ7ynpycLL2/m5yFIT/+o7O92lqBf6YEwk5VsGmEEEjKyMGdpEzEPkhHRo4GWTlapGblIjUrF+nZGmTmaGCtkGHwC9VRwVaJ1KxcpP23Lj1bg6wcDTJzNcjI1iIrV4PsXC2yNVrpv7kagRzNo/cabd6yRiuQq81bjk/OxIO0bKwe3BqQATkagZxcrc5+uVptXrlGm7ef5tH+udpHx76flo06HvZwc1A9Wv/fuqxcDbyc1XBSW0vH1GgFNCJvfYGXEMjO1aKOhwOs5Ka5v6oVgPa/+oUANEJAKwS0WgGtADTavGUhADuVAk5qa8hlMshkeX8sMhnyloHHygsr4/1houdduUqAJREWFoaZM2cWuk5pJUeTKo9On8/cTEJGjgaJGTmwsVbgwu0kHL12HzvO3kFaVi5uJmYgO1e/CT9XHCo4l5mxtZ4XYfI6LFklJxvI8Chx3nyYNz9fLXd7XL2bCgAI8a8GmUwGuUwGuQyQyx97/99+eQn7UeLOT9ia/8qFENh08hZqutvjZV+3//bLO4YMj46bn6gfHVuG2AfpaFjZqUCs+e/l/yX49GwNfCraQWUth8pKAVulAtYKdgIny1auEqCnpyfi4+N1yuLj4+Ho6FjkRKGTJk1CaGiotJx/7RgAvJzV2DrqBWldnSm7kJ2rRej60/gr6kGRcTiprVHRXgkPBxvYWMthp7KCg40VbJVW2HH2DuKSdeefs1XmfeGolQqorfNeKivFf19Gciit5FAq5LBWyGFtJYe1XAZrhRxWCjmsFTIo5DJYyWWwUshhJZdhzo6LOsdXWsmhUshhpcjbz/q/91bScWSwkuftq5DLYKWQQSGX42FaNs7dSgIAVHZWS/Uo5DJohcC1hDTp+Pnlj2+jkMkg/29ZLpfhdmIGMnPyfiAo9DwDNPQWtPy/OuUy5NX/+LL8UeKITy587j5D3EnKLLQ8P/kBwI/HCs7aXlIX7yTj4p3kp29oRGprBfJPdqUEmr/ysURa1DY5Gi2SMx/Nr1fJyQa1PRz+S96Pts3b/1FCfpCWjePRD9GmhgtquNlLSV3+X0X5if5heg62nbmFF+u64+1mlZGt0cJWaQVPRxvI5YCVXA65DPB2sYWNdcGZ4ImKU64SoL+/P3bu3KlTtmfPHvj7+xe5j0qlgkql0uv4+Wd3jye/F+u6oXV1F1R2VqNxFWdUcrIp9g9t6uv1cfNhOmQyGRxtrGCntILcyJcDh7argaxcDeSyvOTDy3VFE/9dDtUKAQFIl0eFAASEdEk1ryzvv5F3kuFoY523/3/bCCGQkJKFbWdu42Vfd3x78DrikzPRv0016RjSf7WPvRdC+mJXyPPO5BSyvB8Qj5d/9vu/sFMq8E5L77w48OiYQghotbrLAkByRg7+ufEQSRk5qONh/99nerQe/8UQfT+9yPbJyDHeJLZA3o+Gon44FObP6w/w5/Wif2zm2xMZjz2R8U/dztHGSmpjWX6bSz+UgNgHGTrbe7uoYWOlQF1PB7g72KBZVWcoreR4kJaNmm72sFMpoLKSw0ouRwU7JdTWCiiteOb8vDBrL9DU1FRcvXoVANCsWTMsXLgQL730ElxcXFC1alVMmjQJt27dwurVqwHkDYNo2LAhRo4cicGDB2Pfvn0YM2YMduzYofcwiOTkZDg5OSEpKalAL9D608KRnp33hdCtqRfmvNUI9oXcCyQqr4QQyNZoEfsgHSorxWPleck+/9vg8UT66BtCSEn28X3O3kyCRiuw7fRtdG7kCbXSSicJP/oR8aiOuylZ+CriCnq0qIKqLra6yV760QIc/DcBkf+dFTeu4oSzN/OuWHg52SBXK3A35dnP9J+Fs601Zr7RACorOe4kZSKglitqudkb/Ucv6Srue9wQZk2ABw4cwEsvvVSgPCQkBKtWrcLAgQMRHR2NAwcO6Owzbtw4REZGokqVKpg6dapBA+GLa7jzt5Lw5/X7aFTZCX41jDe7NRGZjhAC8clZeJieDaWVvNAOUfmdpSJvJyMxPRvXEtJw4XYSrtxNhRBAYD0PnI5NRC13O5yJTZLOjN0dVEjJzDXKmfLggOpoV8cVzmpr6dZEBVslPBxtnvnYlua5SIDmYKyGIyLLkX/m/DAtByuPRMHBxgqf/f4vAMC/RkWkZ+fizH9np89iYFsfuDuqYKe0QgMvR9Sr5Ai1tYJnlE9gAiwhJkAiMpXMHA1SMnORmaPBqqPR+OV4LFKycnW28XZRIydXFOgspy8HGysE1vPAoAAfNKrsZJF9AJgAS4gJkIjKiqSMHETdS8Os3y7gZEyizhAbQzT1dkZWrhZCCFR1sUXXJl5o6VMBlZwK7x1f3jEBlhATIBGVB0IIpGTlIlcjcDclE68tOlSi4wwOqI7BL/jAUW0t9W4u75gAS4gJkIieB5k5Gmw/ewdWchlO3HgIhVyGVUej9d5/5cBWeMnX3XQBmhATYAkxARKRJZi7I1KvJ1KFvd0IdT0d0LxqhVKIyjiYAEuICZCILNG91Cws2X8VK49EF7vd/w3xwwu1XUsnqBJiAiwhJkAisnRCCHx/OAoLdl9GVjHPN145qBVeqlv2LpMyAZYQEyARka7rCal4+fM/it1m26gANK7iXDoBPQUTYAkxARIRFe1eahZaztlb5PqPO/tiePuapRhRQUyAJcQESESkn9gH6Viw+zK2nbldYF3/NtUw680GZhmIzwRYQkyARESGC9t5Ed8cvF6g/K+PXyn155kyAZYQEyARUclF3UvDS58d0Cl7rYEnlvdvUWoxGOt7nBNbERGR3qq72iH6ky4Y2NZHKgu/EAefiTtw8N8E8wVWAjwDJCKiEnmYlo1ms/folDX1dsaWkQEmrZdngEREZFYV7JSI/qQLPnuniVR2OjYRPhN3mDEq/TEBEhHRM+nRogr+ndNJp8xn4g5otWX7AiMTIBERPTOllRxnpnfUKavx8U4zRaMfJkAiIjIKJ7U1osI665T1//4vM0XzdEyARERkNDKZDNGfdJGWD125h57fHDNjREVjAiQiIqM7PjlQev931AO8vfSIGaMpHBMgEREZnZuDCn9PfkVaPhmTiJ3n7pgxooKYAImIyCTcHWxwec5r0vL7a09izvZIM0akiwmQiIhMRmWlwIIejaXl7w5HYdmBa2aM6BEmQCIiMql3WnrjwswgaXl++CUkpGSZMaI8TIBERGRydiorbHjXX1puNXcvUjJzzBgREyAREZWSVj4uGBxQXVpuNON3M0bDBEhERKVoWtf66NO6qrQc+yDdbLEwARIRUama+UYD6X27T/fDXJMSMQESEVGpUlrJMevNR0lw8pbzZomDCZCIiErdAH8f6f1Pf8UgM0dT6jEwARIRkVkcGP+i9H78hjOlXj8TIBERmYWPqx0aeOXN6L79bOk/Jo0JkIiIzGb0y7Wl9/9EPyjVupkAiYjIbDrW95De91heutMmMQESEZHZyOUyjOhQQ1q+nZhRenWXWk1ERESFmPiar/T+iz3/llq9TIBERGRWMpkMNVztAAAbTtwstXqZAImIyOzmPzZlUtS9tFKpkwmQiIjMrpWPi/T+pc8OlEqdTIBERFQmBNSqWKr1MQESEVGZ8Mnbjy6DXktINXl9TIBERFQmeLvYSu/Dz8eZvD4mQCIiKjP6tcmbK/Dbg9dNXhcTIBERlRnBftUAAEkZOcjINu0MEUyARERUZtSr5AgntTUA4PztJJPWxQRIRERlSg23vEHxq45Gm7QeJkAiIipTsnK0AIAdJp4iiQmQiIjKlGHtq5dKPUyARERUprSp8WhA/MO0bJPVwwRIRERliqejjfQ+4tJdk9XDBEhERGWKTCaDr6cDAODK3RST1cMESEREZY69ygoAcD3BdDNDMAESEVGZU61i3lCIa3dN90xQJkAiIipz/KrnTY903YRzAzIBEhFRmdPSp4L0XqMVJqmDCZCIiMqc/EuggOmmRmICJCKiMkchl0nvL95JNkkdTIBERFQmNfV2BgDsN9FYQCZAIiIqk67E540B3HL6tkmOzwRIRERl0geBdUx6fCZAIiIqk6q72j19o2fABEhERGVSbQ976b0Qxh8KYfYEuGTJEvj4+MDGxgZ+fn74+++/i91+0aJFqFu3LtRqNby9vTFu3DhkZmaWUrRERFRaPB57KPbNhxlGP75ZE+D69esRGhqK6dOn4+TJk2jSpAmCgoJw927hPX5++uknTJw4EdOnT8fFixfx/fffY/369fj4449LOXIiIjI1G2sFrBV5wyGOXL1n9OObNQEuXLgQw4YNw6BBg1C/fn0sX74ctra2+OGHHwrd/ujRowgICEDfvn3h4+ODjh07ok+fPk89ayQiovLJWpGXpm4nGf9Kn9kSYHZ2Nk6cOIHAwMBHwcjlCAwMxLFjxwrdp23btjhx4oSU8K5fv46dO3eic+fORdaTlZWF5ORknRcREZUP+dMiJaUbf2JcK6MfUU/37t2DRqOBh4eHTrmHhwcuXbpU6D59+/bFvXv38MILL0AIgdzcXLz77rvFXgINCwvDzJkzjRo7ERGVDqv/zgCPXrtv9GObvROMIQ4cOIB58+Zh6dKlOHnyJDZt2oQdO3Zg9uzZRe4zadIkJCUlSa/Y2NhSjJiIiJ5FVRdbAMAVE0yLZLYzQFdXVygUCsTHx+uUx8fHw9PTs9B9pk6div79+2Po0KEAgEaNGiEtLQ3Dhw/H5MmTIZcXzOcqlQoqlcr4H4CIiEyuXW1XbDxx0yTHNtsZoFKpRIsWLRARESGVabVaREREwN/fv9B90tPTCyQ5hUIBwDRjRIiIyLycbZUmO7bZzgABIDQ0FCEhIWjZsiVat26NRYsWIS0tDYMGDQIADBgwAJUrV0ZYWBgAoGvXrli4cCGaNWsGPz8/XL16FVOnTkXXrl2lREhERM8Pn4p5l0Btlcb/jjdrAuzVqxcSEhIwbdo0xMXFoWnTpggPD5c6xsTExOic8U2ZMgUymQxTpkzBrVu34Obmhq5du2Lu3Lnm+ghERGRC+cMgcjXGv8onExZ27TA5ORlOTk5ISkqCo6OjucMhIqJi3E3JROu5ebfKosI6QyaTGe17vFz1AiUiIsuiVDxKUxk5GqMemwmQiIjKrMc7wSRl5Bj12EyARERUpqmt8zrAGPs+IBMgERGVaVbyvAdi52qZAImIyIIo/psRIlejNepxmQCJiKhMS0zPu/d3K9G4cwIyARIRkUViAiQiojKtqbczAON3gjHrk2CIiIieZk63hsjM0aCWu71Rj8sESEREZVrDyk4mOa7BCTArKwt//fUXbty4gfT0dLi5uaFZs2aoXr26KeIjIiIyCb0T4JEjR/Dll1/it99+Q05ODpycnKBWq/HgwQNkZWWhRo0aGD58ON599104ODiYMmYiIqJnplcnmDfeeAO9evWCj48Pfv/9d6SkpOD+/fu4efMm0tPTceXKFUyZMgURERGoU6cO9uzZY+q4iYiInoleZ4BdunTBr7/+Cmtr60LX16hRAzVq1EBISAgiIyNx584dowZJRERkbJwOiYiIyhVOh0RERPQMjJYAz5w5A4XC+FPWExERmYJRzwAt7GoqERGVY3oPg3j77beLXZ+UlASZTPbMAREREZUGvRPgb7/9hldffRUeHh6FrtdojDtVPRERkSnpnQDr1auH7t27Y8iQIYWuP336NLZv3260wIiIiExJ73uALVq0wMmTJ4tcr1KpULVqVaMERUREZGp6jwPMysqCRqOBra2tqWMyKY4DJCIq34z1Pa73JVCVSlXiSoiIiMoaDoQnIiKLxARIREQWiQmQiIgsEhMgERFZJCZAIiKySCVKgKtXr8bWrVt1yrZu3YrVq1cbJSgiIiJTK9F8gHK5HL6+voiMjJTKfH19ceXKlTL/SDSOAyQiKt9KfRzg47RabYGyS5culTgIIiKi0sZ7gEREZJH0OgNMTk7W+4C8rEhEROWBXgnQ2dn5qXP9CSEgk8nK/D1AIiIiQM8EuH//flPHQUREVKr0SoAdOnQwdRxERESlqkSdYA4dOoR+/fqhbdu2uHXrFgBgzZo1OHz4sFGDIyIiMhWDE+Cvv/6KoKAgqNVqnDx5EllZWQCApKQkzJs3z+gBEhERmYLBCXDOnDlYvnw5VqxYAWtra6k8ICCg2BnjiYiIyhKDE+Dly5fRvn37AuVOTk5ITEw0RkxEREQmZ3AC9PT0xNWrVwuUHz58GDVq1DBKUERERKZmcAIcNmwYxo4di7/++gsymQy3b9/G2rVrMX78eLz33numiJGIiMjoDH4W6MSJE6HVavHKK68gPT0d7du3h0qlwvjx4zF69GhTxEhERGR0JZoNAgCys7Nx9epVpKamon79+rC3tzd2bCbB2SCIiMo3s84GAQBKpRIODg5wcHAoN8mPiIgon8H3AHNzczF16lQ4OTnBx8cHPj4+cHJywpQpU5CTk2OKGImIiIzO4DPA0aNHY9OmTfj000/h7+8PADh27BhmzJiB+/fvY9myZUYPkoiIyNgMvgfo5OSEdevWoVOnTjrlO3fuRJ8+fZCUlGTUAI2N9wCJiMo3Y32PG3wJVKVSwcfHp0B59erVoVQqSxwIERFRaTI4AY4aNQqzZ8+WngEKAFlZWZg7dy5GjRpl1OCIiIhMRa97gG+//bbO8t69e1GlShU0adIEAHDmzBlkZ2fjlVdeMX6EREREJqBXAnRyctJZ7t69u86yt7e38SIiIiIqBXolwJUrV5o6DiIiolJVoglxiYiIyrsSPQlm48aN+OWXXxATE4Ps7GyddZwTkIiIygODzwC/+uorDBo0CB4eHjh16hRat26NihUr4vr16wXGBhIREZVVBifApUuX4ttvv8XixYuhVCrx0UcfYc+ePRgzZkyZHwRPRESUz+AEGBMTg7Zt2wIA1Go1UlJSAAD9+/fHzz//bNzoiIiITKREM8I/ePAAAFC1alX8+eefAICoqCiUcGYlIiKiUmdwAnz55Zexbds2AMCgQYMwbtw4vPrqq+jVqxfeeustowdIRERkCgYnwG+//RaTJ08GAIwcORI//PAD6tWrh1mzZpVoJoglS5bAx8cHNjY28PPzw99//13s9omJiRg5ciQqVaoElUqFOnXqYOfOnQbXS0REls3gYRByuRxy+aO82bt3b/Tu3btEla9fvx6hoaFYvnw5/Pz8sGjRIgQFBeHy5ctwd3cvsH12djZeffVVuLu7Y+PGjahcuTJu3LgBZ2fnEtVPRESWS6/pkM6ePav3ARs3bqz3tn5+fmjVqhW+/vprAIBWq4W3tzdGjx6NiRMnFth++fLlWLBgAS5dugRra2u963kcp0MiIirfjPU9rlcClMvlkMlkT+3kIpPJoNFo9Ko4Ozsbtra22LhxI7p16yaVh4SEIDExEVu3bi2wT+fOneHi4gJbW1ts3boVbm5u6Nu3LyZMmACFQqFXvUyARETlm7G+x/W6BBoVFVXiCopy7949aDQaeHh46JR7eHjg0qVLhe5z/fp17Nu3D8HBwdi5cyeuXr2K999/Hzk5OZg+fXqh+2RlZelM3ZScnGy8D0FEROWWXgmwWrVqpo5DL1qtFu7u7vj222+hUCjQokUL3Lp1CwsWLCgyAYaFhWHmzJmlHCkREZV1ZnsYtqurKxQKBeLj43XK4+Pj4enpWeg+lSpVQp06dXQud9arVw9xcXEFnkmab9KkSUhKSpJesbGxxvsQRERUbpktASqVSrRo0QIRERFSmVarRUREBPz9/QvdJyAgAFevXoVWq5XK/v33X1SqVAlKpbLQfVQqFRwdHXVeREREZp0OKTQ0FCtWrMCPP/6Iixcv4r333kNaWhoGDRoEABgwYAAmTZokbf/ee+/hwYMHGDt2LP7991/s2LED8+bNw8iRI831EYiIqJwq0XRIxtKrVy8kJCRg2rRpiIuLQ9OmTREeHi51jImJidEZc+jt7Y3du3dj3LhxaNy4MSpXroyxY8diwoQJ5voIRERUTuk1DOJJiYmJ2LhxI65du4YPP/wQLi4uOHnyJDw8PFC5cmVTxGk0HAZBRFS+leowiMedPXsWgYGBcHJyQnR0NIYNGwYXFxds2rQJMTExWL16dYmDISIiKi0G3wMMDQ3FwIEDceXKFdjY2EjlnTt3xsGDB40aHBERkakYnACPHz+OESNGFCivXLky4uLijBIUERGRqRmcAFUqVaFPU/n333/h5uZmlKCIiIhMzeAE+MYbb2DWrFnIyckBkPf8z5iYGEyYMAHdu3c3eoBERESmYHAC/Pzzz5Gamgp3d3dkZGSgQ4cOqFWrFhwcHDB37lxTxEhERGR0BvcCdXJywp49e3D48GGcPXsWqampaN68OQIDA00RHxERkUkYPA4wNjYW3t7eporH5DgOkIiofDPW97jBl0B9fHzQoUMHrFixAg8fPixxxUREROZkcAL8559/0Lp1a8yaNQuVKlVCt27dsHHjRp0594iIiMo6gxNgs2bNsGDBAsTExGDXrl1wc3PD8OHD4eHhgcGDB5siRiIiIqMr0bNAn3Ty5EkMGTIEZ8+ehUajMUZcJsN7gERE5ZvZ7gHmu3nzJj799FM0bdoUrVu3hr29PZYsWVLiQIiIiEqTwcMgvvnmG/z00084cuQIfH19ERwcjK1bt6JatWqmiI+IiMgkDE6Ac+bMQZ8+ffDVV1+hSZMmpoiJiIjI5AxOgDExMZDJZKaIhYiIqNTolQDPnj2Lhg0bQi6X49y5c8Vu27hxY6MERkREZEp6JcCmTZsiLi4O7u7uaNq0KWQyGR7vPJq/LJPJynwvUCIiIkDPBBgVFSVNdRQVFWXSgIiIiEqDXgnw8R6eN27cQNu2bWFlpbtrbm4ujh49yt6gRERULhg8DvCll17CgwcPCpQnJSXhpZdeMkpQREREpmZwAsy/1/ek+/fvw87OzihBERERmZrewyDefvttAHkdXgYOHAiVSiWt02g0OHv2LNq2bWv8CImIiExA7wTo5OQEIO8M0MHBAWq1WlqnVCrRpk0bDBs2zPgREhERmYDeCXDlypUA8uYDHD9+PC93EhFRuWaU2SDKE84GQURUvhnre1yvM8DmzZsjIiICFSpUQLNmzYp9FNrJkydLHAwREVFp0SsBvvnmm1Knl27dupkyHiIiolLBS6BERFSumG1C3NjYWNy8eVNa/vvvv/HBBx/g22+/LXEQREREpc3gBNi3b1/s378fABAXF4fAwED8/fffmDx5MmbNmmX0AImIiEzB4AR4/vx5tG7dGgDwyy+/oFGjRjh69CjWrl2LVatWGTs+IiIikzA4Aebk5EgdYvbu3Ys33ngDAODr64s7d+4YNzoiIiITMTgBNmjQAMuXL8ehQ4ewZ88evPbaawCA27dvo2LFikYPkIiIyBQMToDz58/HN998gxdffBF9+vRBkyZNAADbtm2TLo0SERGVdSUaBqHRaJCcnIwKFSpIZdHR0bC1tYW7u7tRAzQ2DoMgIirfSvVJME9SKBTIzc3F4cOHAQB169aFj49PiYMgIiIqbQZfAk1LS8PgwYNRqVIltG/fHu3bt4eXlxeGDBmC9PR0U8RIRERkdAYnwNDQUPzxxx/47bffkJiYiMTERGzduhV//PEH/ve//5kiRiIiIqMz+B6gq6srNm7ciBdffFGnfP/+/ejZsycSEhKMGZ/R8R4gEVH5ZrZHoaWnp8PDw6NAubu7Oy+BEhFRuWFwAvT398f06dORmZkplWVkZGDmzJnw9/c3anBERESmYnAv0EWLFiEoKAhVqlSRxgCeOXMGNjY22L17t9EDJCIiMoUSjQNMT0/HTz/9hIsXLwIA6tWrh+DgYKjVaqMHaGy8B0hEVL6ZZRzgn3/+id9++w3Z2dl4+eWXMXTo0BJXTEREZE56J8CNGzeiV69eUKvVsLa2xsKFCzF//nyMHz/elPERERGZhN6dYMLCwjBs2DAkJSXh4cOHmDNnDubNm2fK2IiIiExG73uA9vb2OH36NGrVqgUAyM7Ohp2dHW7dulXmn//5ON4DJCIq30p9HGB6erpORUqlEjY2NkhNTS1x5UREROZiUCeY7777Dvb29tJybm4uVq1aBVdXV6lszJgxxouOiIjIRPS+BOrj4wOZTFb8wWQyXL9+3SiBmQovgRIRlW+lPgwiOjq6xJUQERGVNQY/Co2IiOh5oFcCXLdund4HjI2NxZEjR0ocEBERUWnQKwEuW7YM9erVw6effio9/uxxSUlJ2LlzJ/r27YvmzZvj/v37Rg+UiIjImPS6B/jHH39g27ZtWLx4MSZNmgQ7Ozt4eHjAxsYGDx8+RFxcHFxdXTFw4ECcP3++0OmSiIiIyhKDH4Z97949HD58GDdu3EBGRgZcXV3RrFkzNGvWDHJ52b+lyF6gRETlm1kehg3kzQjfrVu3EldIRERUFpT9UzYiIiITYAIkIiKLxARIREQWiQmQiIgsUplIgEuWLIGPjw9sbGzg5+eHv//+W6/91q1bB5lMxk45RERkMIN7gWo0GqxatQoRERG4e/cutFqtzvp9+/YZdLz169cjNDQUy5cvh5+fHxYtWoSgoCBcvny52HkGo6OjMX78eLRr187Qj0BERGT4GeDYsWMxduxYaDQaNGzYEE2aNNF5GWrhwoUYNmwYBg0ahPr162P58uWwtbXFDz/8UOQ+Go0GwcHBmDlzJmrUqGFwnURERAafAa5btw6//PILOnfu/MyVZ2dn48SJE5g0aZJUJpfLERgYiGPHjhW536xZs+Du7o4hQ4bg0KFDxdaRlZWFrKwsaTk5OfmZ4yYiovLP4DNApVKJWrVqGaXye/fuQaPRFHh0moeHB+Li4grd5/Dhw/j++++xYsUKveoICwuDk5OT9PL29n7muImIqPwzOAH+73//w5dffgkDn6BmFCkpKejfvz9WrFihMwt9cSZNmoSkpCTpFRsba+IoiYioPDD4Eujhw4exf/9+7Nq1Cw0aNIC1tbXO+k2bNul9LFdXVygUCsTHx+uUx8fHw9PTs8D2165dQ3R0NLp27SqV5XfCsbKywuXLl1GzZk2dfVQqFVQqld4xERGRZTA4ATo7O+Ott94ySuVKpRItWrRARESENJRBq9UiIiICo0aNKrC9r68vzp07p1M2ZcoUpKSk4Msvv+TlTSIi0pvBCXDlypVGDSA0NBQhISFo2bIlWrdujUWLFiEtLQ2DBg0CAAwYMACVK1dGWFgYbGxs0LBhQ539nZ2dAaBAORERUXEMToD5EhIScPnyZQBA3bp14ebmVqLj9OrVCwkJCZg2bRri4uLQtGlThIeHSx1jYmJiysU0S0REVL4YPB9gWloaRo8ejdWrV0v33xQKBQYMGIDFixfD1tbWJIEaC+cDJCIq34z1PW7wqVVoaCj++OMP/Pbbb0hMTERiYiK2bt2KP/74A//73/9KHAgREVFpMvgM0NXVFRs3bsSLL76oU75//3707NkTCQkJxozP6HgGSERUvpntDDA9Pb3AwHUAcHd3R3p6eokDISIiKk0GJ0B/f39Mnz4dmZmZUllGRgZmzpwJf39/owZHRERkKgb3Av3yyy8RFBSEKlWqSA+/PnPmDGxsbLB7926jB0hERGQKBt8DBPIug65duxaXLl0CANSrVw/BwcFQq9VGD9DYeA+QiKh8M9b3eInGAdra2mLYsGElrpSIiMjc9EqA27ZtQ6dOnWBtbY1t27YVu+0bb7xhlMCIiIhMSa9LoHK5HHFxcXB3dy/2qSwymQwajcaoARobL4ESEZVvpXoJNP+JL0++JyIiKq+M8pDNxMREYxyGiIio1BicAOfPn4/169dLy++88w5cXFxQuXJlnDlzxqjBERERmYrBCXD58uXSvHt79uzB3r17ER4ejk6dOuHDDz80eoBERESmYPAwiLi4OCkBbt++HT179kTHjh3h4+MDPz8/owdIRERkCgafAVaoUAGxsbEAgPDwcAQGBgIAhBBlvgcoERFRPoPPAN9++2307dsXtWvXxv3799GpUycAwKlTp1CrVi2jB0hERGQKBifAL774Aj4+PoiNjcWnn34Ke3t7AMCdO3fw/vvvGz1AIiIiUyjRs0DLMw6EJyIq30p1IDwfhUZERM8bPgqNiIjKFT4KjYiI6BkY5VFoRERE5Y3BCXDMmDH46quvCpR//fXX+OCDD4wRExERkckZnAB//fVXBAQEFChv27YtNm7caJSgiIiITM3gBHj//n04OTkVKHd0dMS9e/eMEhQREZGpGZwAa9WqhfDw8ALlu3btQo0aNYwSFBERkakZ/CSY0NBQjBo1CgkJCXj55ZcBABEREfj888+xaNEiY8dHRERkEgYnwMGDByMrKwtz587F7NmzAQA+Pj5YtmwZBgwYYPQAiYiITOGZHoWWkJAAtVotPQ+0POBAeCKi8s1Y3+MlGgeYm5uLvXv3YtOmTcjPn7dv30ZqamqJAyEiIipNBl8CvXHjBl577TXExMQgKysLr776KhwcHDB//nxkZWVh+fLlpoiTiIjIqAw+Axw7dixatmyJhw8fQq1WS+VvvfUWIiIijBocERGRqRh8Bnjo0CEcPXoUSqVSp9zHxwe3bt0yWmBERESmZPAZoFarLXTGh5s3b8LBwcEoQREREZmawQmwY8eOOuP9ZDIZUlNTMX36dHTu3NmYsREREZmMwcMgYmNj8dprr0EIgStXrqBly5a4cuUKXF1dcfDgQbi7u5sqVqPgMAgiovLNWN/jJRoHmJubi/Xr1+PMmTNITU1F8+bNERwcrNMppqxiAiQiKt/MkgBzcnLg6+uL7du3o169eiWu1JyYAImIyjezDIS3trZGZmZmiSsjIiIqKwzuBDNy5EjMnz8fubm5poiHiIioVBg8DvD48eOIiIjA77//jkaNGsHOzk5n/aZNm4wWHBERkakYnACdnZ3RvXt3U8RCRERUagxOgCtXrjRFHERERKVK73uAWq0W8+fPR0BAAFq1aoWJEyciIyPDlLERERGZjN4JcO7cufj4449hb2+PypUr48svv8TIkSNNGRsREZHJ6J0AV69ejaVLl2L37t3YsmULfvvtN6xduxZardaU8REREZmE3gkwJiZG51mfgYGBkMlkuH37tkkCIyIiMiW9E2Bubi5sbGx0yqytrZGTk2P0oIiIiExN716gQggMHDgQKpVKKsvMzMS7776rMxaQ4wCJiKg80DsBhoSEFCjr16+fUYMhIiIqLXonQI7/IyKi54nBzwIlIiJ6HjABEhGRRWICJCIii8QESEREFokJkIiILBITIBERWSQmQCIiskhMgEREZJGYAImIyCIxARIRkUUqEwlwyZIl8PHxgY2NDfz8/PD3338Xue2KFSvQrl07VKhQARUqVEBgYGCx2xMRERXG7Alw/fr1CA0NxfTp03Hy5Ek0adIEQUFBuHv3bqHbHzhwAH369MH+/ftx7NgxeHt7o2PHjrh161YpR05EROWZTAghzBmAn58fWrVqha+//hoAoNVq4e3tjdGjR2PixIlP3V+j0aBChQr4+uuvMWDAgKdun5ycDCcnJyQlJcHR0fGZ4yciotJlrO9xs54BZmdn48SJEwgMDJTK5HI5AgMDcezYMb2OkZ6ejpycHLi4uJgqTCIieg7pPR2SKdy7dw8ajQYeHh465R4eHrh06ZJex5gwYQK8vLx0kujjsrKykJWVJS0nJyeXPGAiInpumP0e4LP45JNPsG7dOmzevBk2NjaFbhMWFgYnJyfp5e3tXcpREhFRWWTWBOjq6gqFQoH4+Hid8vj4eHh6eha772effYZPPvkEv//+Oxo3blzkdpMmTUJSUpL0io2NNUrsRERUvpk1ASqVSrRo0QIRERFSmVarRUREBPz9/Yvc79NPP8Xs2bMRHh6Oli1bFluHSqWCo6OjzouIiMis9wABIDQ0FCEhIWjZsiVat26NRYsWIS0tDYMGDQIADBgwAJUrV0ZYWBgAYP78+Zg2bRp++ukn+Pj4IC4uDgBgb28Pe3t7s30OIiIqX8yeAHv16oWEhARMmzYNcXFxaNq0KcLDw6WOMTExMZDLH52oLlu2DNnZ2ejRo4fOcaZPn44ZM2aUZuhERFSOmX0cYGnjOEAiovLtuRgHSEREZC5MgEREZJGYAImIyCIxARIRkUViAiQiIovEBEhERBaJCZCIiCwSEyAREVkkJkAiIrJITIBERGSRmACJiMgiMQESEZFFYgIkIiKLxARIREQWiQmQiIgsEhMgERFZJCZAIiKySEyARERkkZgAiYjIIjEBEhGRRWICJCIii8QESEREFokJkIiILBITIBERWSQmQCIiskhMgEREZJGYAImIyCIxARIRkUViAiQiIovEBEhERBaJCZCIiCwSEyAREVkkJkAiIrJITIBERGSRmACJiMgiMQESEZFFYgIkIiKLZGXuAMoiIQRyc3Oh0WjMHQqRWSkUClhZWUEmk5k7FCKjYwJ8QnZ2Nu7cuYP09HRzh0JUJtja2qJSpUpQKpXmDoXIqJgAH6PVahEVFQWFQgEvLy8olUr+8iWLJYRAdnY2EhISEBUVhdq1a0Mu510Ten4wAT4mOzsbWq0W3t7esLW1NXc4RGanVqthbW2NGzduIDs7GzY2NuYOicho+HOuEPyVS/QI/x7oecV/2UREZJGYAImIyCIxAVoYmUyGLVu2mLyeAwcOQCaTITExUSrbsmULatWqBYVCgQ8++ACrVq2Cs7OzyWK4fPkyPD09kZKSYrI6yrvIyEhUqVIFaWlp5g6FqNQxAT5H4uLiMHr0aNSoUQMqlQre3t7o2rUrIiIiSj2Wtm3b4s6dO3BycpLKRowYgR49eiA2NhazZ89Gr1698O+//5oshkmTJmH06NFwcHAosM7X1xcqlQpxcXEF1r344ouQyWSQyWSwsbFB/fr1sXTpUpPFCQAPHjxAcHAwHB0d4ezsjCFDhiA1NbXYfa5du4a33noLbm5ucHR0RM+ePREfH6+zzb///os333wTrq6ucHR0xAsvvID9+/dL6+vXr482bdpg4cKFJvlcRGUZE+BzIjo6Gi1atMC+ffuwYMECnDt3DuHh4XjppZcwcuTIUo9HqVTC09NTGkaSmpqKu3fvIigoCF5eXnBwcIBarYa7u/sz1ZOTk1NoeUxMDLZv346BAwcWWHf48GFkZGSgR48e+PHHHwvdf9iwYbhz5w4iIyPRs2dPjBw5Ej///PMzxVqc4OBgXLhwAXv27MH27dtx8OBBDB8+vMjt09LS0LFjR8hkMuzbtw9HjhxBdnY2unbtCq1WK233+uuvIzc3F/v27cOJEyfQpEkTvP766zqJf9CgQVi2bBlyc3NN9vmIyiRhYZKSkgQAkZSUVGBdRkaGiIyMFBkZGVKZVqsVaVk5ZnlptVq9P1enTp1E5cqVRWpqaoF1Dx8+lN4DEJs3b5aWP/roI1G7dm2hVqtF9erVxZQpU0R2dra0/vTp0+LFF18U9vb2wsHBQTRv3lwcP35cCCFEdHS0eP3114Wzs7OwtbUV9evXFzt27BBCCLF//34BQDx8+FB6//hr//79YuXKlcLJyUkn1i1btohmzZoJlUolqlevLmbMmCFycnJ04l+6dKno2rWrsLW1FdOnTy+0PRYsWCBatmxZ6LqBAweKiRMnil27dok6deoUWN+hQwcxduxYnbLatWuL3r17F3q8ZxUZGSkASO0qhBC7du0SMplM3Lp1q9B9du/eLeRyuc6/48TERCGTycSePXuEEEIkJCQIAOLgwYPSNsnJyQKAtI0QQmRlZQmVSiX27t1baF2F/V0QmVNx3+OG4DjAp8jI0aD+tN1mqTtyVhBslU//X/TgwQOEh4dj7ty5sLOzK7C+uPtsDg4OWLVqFby8vHDu3DkMGzYMDg4O+OijjwDknZk0a9YMy5Ytg0KhwOnTp2FtbQ0AGDlyJLKzs3Hw4EHY2dkhMjIS9vb2Bepo27YtLl++jLp16+LXX39F27Zt4eLigujoaJ3tDh06hAEDBuCrr75Cu3btcO3aNeksaPr06dJ2M2bMwCeffIJFixbByqrw9jl06BBatmxZoDwlJQUbNmzAX3/9BV9fXyQlJeHQoUNo165dkW0E5I2Hy87OLnJ9gwYNcOPGjSLXt2vXDrt27Sp03bFjx+Ds7KwTb2BgIORyOf766y+89dZbBfbJysqCTCaDSqWSymxsbCCXy3H48GEEBgaiYsWKqFu3LlavXo3mzZtDpVLhm2++gbu7O1q0aCHtp1Qq0bRpUxw6dAivvPJKse1A9DxhAnwOXL16FUII+Pr6GrzvlClTpPc+Pj4YP3481q1bJyXAmJgYfPjhh9Kxa9euLW0fExOD7t27o1GjRgCAGjVqFFqHUqmULnW6uLjA09Oz0O1mzpyJiRMnIiQkRDre7Nmz8dFHH+kkwL59+2LQoEHFfq4bN24UmgDXrVuH2rVro0GDBgCA3r174/vvvy8yAWo0Gvz88884e/ZssZckd+7cWeTlWCAvgRYlLi6uwKVgKysruLi4FHqPEgDatGkDOzs7TJgwAfPmzYMQAhMnToRGo8GdO3cA5HV42rt3L7p16wYHBwfI5XK4u7sjPDwcFSpU0Dmel5dXsQmc6HnEBPgUamsFImcFma1ufQghSlzH+vXr8dVXX+HatWtITU1Fbm4uHB0dpfWhoaEYOnQo1qxZg8DAQLzzzjuoWbMmAGDMmDF477338PvvvyMwMBDdu3dH48aNSxzLmTNncOTIEcydO1cq02g0yMzMRHp6uvR0nsIS25MyMjIKfWrJDz/8gH79+knL/fr1Q4cOHbB48WKdzjJLly7Fd999h+zsbCgUCowbNw7vvfdekfVVq1ZNr89oLG5ubtiwYQPee+89fPXVV5DL5ejTpw+aN28uDVwXQmDkyJFwd3fHoUOHoFar8d1336Fr1644fvw4KlWqJB1PrVbz+bdkcdgJ5ilkMhlslVZmeen7HNLatWtDJpPh0qVLBn22Y8eOITg4GJ07d8b27dtx6tQpTJ48WedS34wZM3DhwgV06dIF+/btQ/369bF582YAwNChQ3H9+nX0798f586dQ8uWLbF48WKDYnhcamoqZs6cidOnT0uvc+fO4cqVKzrJrLDLvE9ydXXFw4cPdcoiIyPx559/4qOPPoKVlRWsrKzQpk0bpKenY926dTrbBgcH4/Tp04iKikJaWhoWLlxY7BNRGjRoAHt7+yJfnTp1KnJfT09P3L17V6csNzcXDx48KPJsGQA6duyIa9eu4e7du7h37x7WrFmDW7duSWfi+/btw/bt27Fu3ToEBASgefPmWLp0KdRqdYHOPw8ePICbm1uRdRE9j3gG+BxwcXFBUFAQlixZgjFjxhRIEImJiYXeBzx69CiqVauGyZMnS2WFXQarU6cO6tSpg3HjxqFPnz5YuXKldF/K29sb7777Lt59911MmjQJK1aswOjRo0v0OZo3b47Lly+jVq1aJdr/cc2aNUNkZKRO2ffff4/27dtjyZIlOuUrV67E999/j2HDhkllTk5OBsXxLJdA/f39kZiYiBMnTkj35vbt2wetVgs/P7+n1u3q6irtc/fuXbzxxhsAIJ3RPZm45XK5Tk9RADh//jx69Ojx1LqInidMgM+JJUuWICAgAK1bt8asWbPQuHFj5ObmYs+ePVi2bBkuXrxYYJ/atWsjJiYG69atQ6tWrbBjxw7p7A7Iu4z44YcfokePHqhevTpu3ryJ48ePo3v37gCADz74AJ06dUKdOnXw8OFD7N+/H/Xq1SvxZ5g2bRpef/11VK1aFT169IBcLseZM2dw/vx5zJkzx6BjBQUFYejQodBoNFAoFMjJycGaNWswa9YsNGzYUGfboUOHYuHChbhw4YJ0b9BQz3IJtF69enjttdcwbNgwLF++HDk5ORg1ahR69+4NLy8vAMCtW7fwyiuvYPXq1WjdujWAvMRdr149uLm54dixYxg7dizGjRuHunXrAshLrBUqVEBISAimTZsGtVqNFStWICoqCl26dJHqj46Oxq1btxAYGFjiz0BULhmhR2q5YugwiPLk9u3bYuTIkaJatWpCqVSKypUrizfeeEPs379f2gZPDIP48MMPRcWKFYW9vb3o1auX+OKLL6ShCVlZWaJ3797C29tbKJVK4eXlJUaNGiW1z6hRo0TNmjWFSqUSbm5uon///uLevXtCCN1hEELkDcXAf8Mf8hU2DCI8PFy0bdtWqNVq4ejoKFq3bi2+/fbbIuMvSk5OjvDy8hLh4eFCCCE2btwo5HK5iIuLK3T7evXqiXHjxgkhCh8GYWr3798Xffr0Efb29sLR0VEMGjRIpKSkSOujoqIKtN+ECROEh4eHsLa2FrVr1xaff/55gaEzx48fFx07dhQuLi7CwcFBtGnTRuzcuVNnm3nz5omgoKAiYyvvfxf0/DHWMAiZEM/Qg6IcSk5OhpOTE5KSknQ6ewBAZmYmoqKiUL16dU778hxYsmQJtm3bht27zTOMpTzIzs5G7dq18dNPPyEgIKDQbfh3QWVNcd/jhuAlUHpujRgxAomJiUhJSSn0cWiUN5Tl448/LjL5ET3PmADpuWVlZaXTwYcKqlWrllE6HRGVRxwGQUREFokJkIiILBITYCEsrF8QUbH490DPKybAx+Q/5JmPhCJ6JP/vIf/vg+h5USY6wSxZsgQLFixAXFwcmjRpgsWLF0uDfQuzYcMGTJ06FdHR0ahduzbmz5+Pzp07P3McCoUCzs7O0mOpbG1t9X4cGdHzRgiB9PR03L17F87OzlAo9Hs2LVF5YfYEuH79eoSGhmL58uXw8/PDokWLEBQUhMuXLxc6WerRo0fRp08fhIWF4fXXX8dPP/2Ebt264eTJkwWe8FES+c9efPLZjESWytnZudhnkhKVV2YfCO/n54dWrVrh66+/BgBotVp4e3tj9OjRmDhxYoHte/XqhbS0NGzfvl0qa9OmDZo2bYrly5c/tT59B1BqNJpin+1IZAmsra155kdlznMxED47OxsnTpzApEmTpDK5XI7AwEAcO3as0H2OHTuG0NBQnbKgoCBs2bKl0O2zsrKQlZUlLScnJ+sVm0Kh4B8+EdFzzKydYO7duweNRgMPDw+dcg8PjyInAo2LizNo+7CwMDg5OUkvb29v4wRPRETl2nPfC3TSpElISkqSXrGxseYOiYiIygCzXgJ1dXWFQqFAfHy8Tnl8fHyRN909PT0N2l6lUkGlUhknYCIiem6YNQEqlUq0aNECERER6NatG4C8TjAREREYNWpUofv4+/sjIiICH3zwgVS2Z88e+Pv761Vnfp8ffe8FEhFR2ZL//f3MfTifbVamZ7du3TqhUqnEqlWrRGRkpBg+fLhwdnaW5m3r37+/mDhxorT9kSNHhJWVlfjss8/ExYsXxfTp04W1tbU4d+6cXvXFxsYKAHzxxRdffJXzV2xs7DPlH7OPA+zVqxcSEhIwbdo0xMXFoWnTpggPD5c6usTExEAuf3Srsm3btvjpp58wZcoUfPzxx6hduza2bNmi9xhALy8vxMbGwsHBATKZDMnJyfD29kZsbOwzdad9XrF9no5tVDy2z9OxjYr3ZPsIIZCSkgIvL69nOq7ZxwGam7HGkzyv2D5PxzYqHtvn6dhGxTNV+zz3vUCJiIgKwwRIREQWyeIToEqlwvTp0zlUoghsn6djGxWP7fN0bKPimap9LP4eIBERWSaLPwMkIiLLxARIREQWiQmQiIgsEhMgERFZJItIgEuWLIGPjw9sbGzg5+eHv//+u9jtN2zYAF9fX9jY2KBRo0bYuXNnKUVqHoa0z4oVK9CuXTtUqFABFSpUQGBg4FPb83lg6L+hfOvWrYNMJpOedfu8MrR9EhMTMXLkSFSqVAkqlQp16tTh39kTFi1ahLp160KtVsPb2xvjxo1DZmZmKUVbug4ePIiuXbvCy8sLMpmsyPldH3fgwAE0b94cKpUKtWrVwqpVqwyv+JkepFYOrFu3TiiVSvHDDz+ICxcuiGHDhglnZ2cRHx9f6PZHjhwRCoVCfPrppyIyMlJMmTLFoGeNljeGtk/fvn3FkiVLxKlTp8TFixfFwIEDhZOTk7h582YpR156DG2jfFFRUaJy5cqiXbt24s033yydYM3A0PbJysoSLVu2FJ07dxaHDx8WUVFR4sCBA+L06dOlHHnpMbSN1q5dK1QqlVi7dq2IiooSu3fvFpUqVRLjxo0r5chLx86dO8XkyZPFpk2bBACxefPmYre/fv26sLW1FaGhoSIyMlIsXrxYKBQKER4eblC9z30CbN26tRg5cqS0rNFohJeXlwgLCyt0+549e4ouXbrolPn5+YkRI0aYNE5zMbR9npSbmyscHBzEjz/+aKoQza4kbZSbmyvatm0rvvvuOxESEvJcJ0BD22fZsmWiRo0aIjs7u7RCNDtD22jkyJHi5Zdf1ikLDQ0VAQEBJo2zLNAnAX700UeiQYMGOmW9evUSQUFBBtX1XF8Czc7OxokTJxAYGCiVyeVyBAYG4tixY4Xuc+zYMZ3tASAoKKjI7cuzkrTPk9LT05GTkwMXFxdThWlWJW2jWbNmwd3dHUOGDCmNMM2mJO2zbds2+Pv7Y+TIkfDw8EDDhg0xb948aDSa0gq7VJWkjdq2bYsTJ05Il0mvX7+OnTt3onPnzqUSc1lnrO9ps88GYUr37t2DRqORZpbI5+HhgUuXLhW6T1xcXKHbx8XFmSxOcylJ+zxpwoQJ8PLyKvCP8XlRkjY6fPgwvv/+e5w+fboUIjSvkrTP9evXsW/fPgQHB2Pnzp24evUq3n//feTk5GD69OmlEXapKkkb9e3bF/fu3cMLL7wAIQRyc3Px7rvv4uOPPy6NkMu8or6nk5OTkZGRAbVarddxnuszQDKtTz75BOvWrcPmzZthY2Nj7nDKhJSUFPTv3x8rVqyAq6urucMpk7RaLdzd3fHtt9+iRYsW6NWrFyZPnozly5ebO7Qy48CBA5g3bx6WLl2KkydPYtOmTdixYwdmz55t7tCeK8/1GaCrqysUCgXi4+N1yuPj4+Hp6VnoPp6engZtX56VpH3yffbZZ/jkk0+wd+9eNG7c2JRhmpWhbXTt2jVER0eja9euUplWqwUAWFlZ4fLly6hZs6Zpgy5FJfk3VKlSJVhbW0OhUEhl9erVQ1xcHLKzs6FUKk0ac2krSRtNnToV/fv3x9ChQwEAjRo1QlpaGoYPH47JkyfrzJFqiYr6nnZ0dNT77A94zs8AlUolWrRogYiICKlMq9UiIiIC/v7+he7j7++vsz0A7Nmzp8jty7OStA8AfPrpp5g9ezbCw8PRsmXL0gjVbAxtI19fX5w7dw6nT5+WXm+88QZeeuklnD59Gt7e3qUZvsmV5N9QQEAArl69Kv0wAIB///0XlSpVeu6SH1CyNkpPTy+Q5PJ/MAg+vtl439OG9c8pf9atWydUKpVYtWqViIyMFMOHDxfOzs4iLi5OCCFE//79xcSJE6Xtjxw5IqysrMRnn30mLl68KKZPn/7cD4MwpH0++eQToVQqxcaNG8WdO3ekV0pKirk+gskZ2kZPet57gRraPjExMcLBwUGMGjVKXL58WWzfvl24u7uLOXPmmOsjmJyhbTR9+nTh4OAgfv75Z3H9+nXx+++/i5o1a4qePXua6yOYVEpKijh16pQ4deqUACAWLlwoTp06JW7cuCGEEGLixImif//+0vb5wyA+/PBDcfHiRbFkyRIOgyjK4sWLRdWqVYVSqRStW7cWf/75p7SuQ4cOIiQkRGf7X375RdSpU0colUrRoEEDsWPHjlKOuHQZ0j7VqlUTAAq8pk+fXvqBlyJD/w097nlPgEIY3j5Hjx4Vfn5+QqVSiRo1aoi5c+eK3NzcUo66dBnSRjk5OWLGjBmiZs2awsbGRnh7e4v3339fPHz4sPQDLwX79+8v9Hslv01CQkJEhw4dCuzTtGlToVQqRY0aNcTKlSsNrpfTIRERkUV6ru8BEhERFYUJkIiILBITIBERWSQmQCIiskhMgEREZJGYAImIyCIxARIRkUViAiQiIovEBEhUCJlMhi1btgAAoqOjIZPJnjq90eXLl+Hp6YmUlBTTBwjAx8cHixYtKnabGTNmoGnTpiaNoyR1PN6+JTVw4EB069btmY5RmDZt2uDXX381+nGp7GECpDJl4MCBkMlkkMlksLa2RvXq1fHRRx8hMzPT3KE91aRJkzB69Gg4ODgAyJvSJv+zyGQyeHh4oHv37rh+/bpR6jt+/DiGDx8uLReWVMaPH1/gocGW7ODBg+jatSu8vLyKTMJTpkzBxIkTdR7WTc8nJkAqc1577TXcuXMH169fxxdffIFvvvmmzE+UGhMTg+3bt2PgwIEF1l2+fBm3b9/Ghg0bcOHCBXTt2tUos5+7ubnB1ta22G3s7e1RsWLFZ67reZGWloYmTZpgyZIlRW7TqVMnpKSkYNeuXaUYGZkDEyCVOSqVCp6envD29ka3bt0QGBiIPXv2SOu1Wi3CwsJQvXp1qNVqNGnSBBs3btQ5xoULF/D666/D0dERDg4OaNeuHa5duwYg78zp1VdfhaurK5ycnNChQwecPHnymWL+5Zdf0KRJE1SuXLnAOnd3d1SqVAnt27fHtGnTEBkZiatXrwIAli1bhpo1a0KpVKJu3bpYs2aNtJ8QAjNmzEDVqlWhUqng5eWFMWPGSOsfvwTq4+MDAHjrrbcgk8mk5ccvT/7++++wsbFBYmKiTnxjx47Fyy+/LC0fPnwY7dq1g1qthre3N8aMGYO0tDS920Lf9r1z5w46deoEtVqNGjVqFPh/GBsbi549e8LZ2RkuLi548803ER0drXcchenUqRPmzJmDt956q8htFAoFOnfujHXr1j1TXVT2MQFSmXb+/HkcPXpUZ564sLAwrF69GsuXL8eFCxcwbtw49OvXD3/88QcA4NatW2jfvj1UKhX27duHEydOYPDgwcjNzQWQN2t7SEgIDh8+jD///BO1a9dG586dn+ne3aFDh/SaGzF/ss7s7Gxs3rwZY8eOxf/+9z+cP38eI0aMwKBBg7B//34AwK+//iqdAV+5cgVbtmxBo0aNCj3u8ePHAQArV67EnTt3pOXHvfLKK3B2dta5v6XRaLB+/XoEBwcDyJvQ97XXXkP37t1x9uxZrF+/HocPH8aoUaP0bgt923fq1Kno3r07zpw5g+DgYPTu3RsXL14EAOTk5CAoKAgODg44dOgQjhw5Ant7e7z22mvIzs4utN5Vq1ZBJpPpHWdxWrdujUOHDhnlWFSGPeMsFkRGFRISIhQKhbCzsxMqlUoAEHK5XGzcuFEIIURmZqawtbUVR48e1dlvyJAhok+fPkIIISZNmiSqV68usrOz9apTo9EIBwcH8dtvv0llAMTmzZuFEEJERUUJAOLUqVNFHqNJkyZi1qxZOmX5U7zkT2Fz+/Zt0bZtW1G5cmWRlZUl2rZtK4YNG6azzzvvvCM6d+4shBDi888/F3Xq1Cnyc1SrVk188cUXhcacb/r06aJJkybS8tixY8XLL78sLe/evVuoVCopxiFDhojhw4frHOPQoUNCLpeLjIyMQuN4so4nFdW+7777rs52fn5+4r333hNCCLFmzRpRt25dodVqpfVZWVlCrVaL3bt3CyEKTjO1adMmUbdu3SLjeFJh7ZVv69atQi6XC41Go/fxqPzhGSCVOfmzp//1118ICQnBoEGD0L17dwDA1atXkZ6ejldffRX29vbSa/Xq1dIlztOnT6Ndu3awtrYu9Pjx8fEYNmwYateuDScnJzg6OiI1NRUxMTEljjkjIwM2NjaFrqtSpQrs7Ozg5eWFtLQ0/Prrr1Aqlbh48SICAgJ0tg0ICJDOgt555x1kZGSgRo0aGDZsGDZv3iydxZZUcHAwDhw4gNu3bwMA1q5diy5dusDZ2RkAcObMGaxatUqnbYOCgqDVahEVFaVXHfq275Ozd/v7+0uf/cyZM7h69SocHBykOFxcXJCZmSn9f37SW2+9hUuXLhnSHEVSq9XQarXIysoyyvGobLIydwBET7Kzs0OtWrUAAD/88AOaNGmC77//HkOGDEFqaioAYMeOHQXut6lUKgCPLjMWJSQkBPfv38eXX36JatWqQaVSwd/fv8hLa/pwdXXFw4cPC1136NAhODo6wt3dXeohqg9vb29cvnwZe/fuxZ49e/D+++9jwYIF+OOPP4pM7k/TqlUr1KxZE+vWrcN7772HzZs3Y9WqVdL61NRUjBgxQudeY76qVavqVYcx2jc1NRUtWrTA2rVrC6xzc3PT+zgl9eDBA9jZ2T313xKVb0yAVKbJ5XJ8/PHHCA0NRd++fVG/fn2oVCrExMSgQ4cOhe7TuHFj/Pjjj8jJySk0URw5cgRLly5F586dAeR1trh3794zxdmsWTNERkYWuq569erSGdbj6tWrhyNHjiAkJEQntvr160vLarUaXbt2RdeuXTFy5Ej4+vri3LlzaN68eYHjWVtb69W7NDg4GGvXrkWVKlUgl8vRpUsXaV3z5s0RGRkp/QApCX3b988//8SAAQN0lps1aybFsX79eri7u8PR0bHEsZTU+fPnpVjo+cVLoFTmvfPOO1AoFFiyZAkcHBwwfvx4jBs3Dj/++COuXbuGkydPYvHixfjxxx8BAKNGjUJycjJ69+6Nf/75B1euXMGaNWtw+fJlAEDt2rWxZs0aXLx4EX/99ReCg4Of+Zd+UFAQjh07ZtDwhg8//BCrVq3CsmXLcOXKFSxcuBCbNm3C+PHjAeR16vj+++9x/vx5XL9+Hf/3f/8HtVqNatWqFXo8Hx8fREREIC4ursizUSAvAZ48eRJz585Fjx49pDNnAJgwYQKOHj2KUaNG4fTp07hy5Qq2bt1qUCcYfdt3w4YN+OGHH/Dvv/9i+vTp+Pvvv6V6goOD4erqijfffBOHDh1CVFQUDhw4gDFjxuDmzZuF1rt582b4+voWG1tqaipOnz4tPdQgKioKp0+fLnB59tChQ+jYsaPen5nKKXPfhCR63JMdG/KFhYUJNzc3kZqaKrRarVi0aJGoW7eusLa2Fm5ubiIoKEj88ccf0vZnzpwRHTt2FLa2tsLBwUG0a9dOXLt2TQghxMmTJ0XLli2FjY2NqF27ttiwYUOxHUr06QSTk5MjvLy8RHh4uFT2ZCeYwixdulTUqFFDWFtbizp16ojVq1dL6zZv3iz8/PyEo6OjsLOzE23atBF79+6V1j8Z87Zt20StWrWElZWVqFatmhCi6A4qrVu3FgDEvn37Cqz7+++/xauvvirs7e2FnZ2daNy4sZg7d26Rn+HJOvRt3yVLlohXX31VqFQq4ePjI9avX69z3Dt37ogBAwYIV1dXoVKpRI0aNcSwYcNEUlKSEKLgv5WVK1eKp32l5f8/efIVEhIibXPz5k1hbW0tYmNjiz0WlX8yIYQwU+4leq4sWbIE27Ztw+7du80dCj2DCRMm4OHDh/j222/NHQqZGO8BEhnJiBEjkJiYiJSUFIM6u1DZ4u7ujtDQUHOHQaWAZ4BERGSR2AmGiIgsEhMgERFZJCZAIiKySEyARERkkZgAiYjIIjEBEhGRRWICJCIii8QESEREFokJkIiILNL/A31YJRMZAKYAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === METRICS-ONLY (pas de r√©-entra√Ænement, pas de GridSearch) ===\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import (precision_recall_curve, accuracy_score, balanced_accuracy_score,\n",
    "                             precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             average_precision_score, confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_proba_in_batches(clf, X, batch=20000):\n",
    "    \"\"\"Evite la surconsommation m√©moire pour de gros X.\"\"\"\n",
    "    n = X.shape[0]\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    for i in range(0, n, batch):\n",
    "        j = min(i+batch, n)\n",
    "        out[i:j] = clf.predict_proba(X[i:j])[:, 1]\n",
    "    return out\n",
    "\n",
    "# 1) Probas validation & seuil optimal F1 (t*) ‚Äî on peut utiliser 'gs.best_estimator_' s‚Äôil existe, sinon 'best'\n",
    "est_for_val = gs.best_estimator_ if 'gs' in globals() else best\n",
    "proba_va = predict_proba_in_batches(est_for_val, X_va_)\n",
    "prec, rec, thr = precision_recall_curve(y_va, proba_va)\n",
    "f1_curve = (2*prec*rec)/(prec+rec+1e-9)\n",
    "idx = int(np.nanargmax(f1_curve))\n",
    "t_star = float(thr[idx]) if idx < len(thr) else 0.5\n",
    "print(f\"Seuil optimal (validation) t* = {t_star:.3f}\")\n",
    "\n",
    "# 2) Probas test avec le mod√®le final 'best'\n",
    "proba_te = predict_proba_in_batches(best, X_te_)\n",
    "\n",
    "def metrics_at_threshold(y_true, proba, thr, label):\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        \"setting\": label, \"threshold\": thr,\n",
    "        \"AP\": average_precision_score(y_true, proba),\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, proba),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"BalancedAcc\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"Specificity\": tn / (tn + fp + 1e-9),\n",
    "        \"NPV\": tn / (tn + fn + 1e-9),\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"FN\": int(fn), \"TN\": int(tn),\n",
    "    }\n",
    "\n",
    "# 3) Tableau de m√©triques (test) @0.5 et @t*\n",
    "rows = [\n",
    "    metrics_at_threshold(y_te, proba_te, 0.5, \"Test @0.5\"),\n",
    "    metrics_at_threshold(y_te, proba_te, t_star, \"Test @t*\"),\n",
    "]\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics)\n",
    "\n",
    "# 4) Matrices & rapports (test)\n",
    "def print_cm(y_true, proba, thr, title):\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    print(f\"\\n{title} (thr={thr:.2f})\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "print_cm(y_te, proba_te, 0.5, \"Confusion matrix ‚Äî Test @0.5\")\n",
    "print_cm(y_te, proba_te, t_star, \"Confusion matrix ‚Äî Test @t*\")\n",
    "\n",
    "# 5) Courbes PR & ROC (test)\n",
    "from sklearn.metrics import roc_curve, PrecisionRecallDisplay\n",
    "disp = PrecisionRecallDisplay.from_predictions(y_te, proba_te)\n",
    "disp.ax_.set_title(\"Precision‚ÄìRecall (test)\")\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_te, proba_te)\n",
    "plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--')\n",
    "plt.title(\"ROC (test)\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33ef97",
   "metadata": {},
   "source": [
    "Ce qui change en passant de 0.5 ‚Üí 0.27\n",
    "\n",
    "Comptes (test, N=12 000) :\n",
    "\n",
    "@0.5 : TN=2537, FP=326, FN=955, TP=8182\n",
    "\n",
    "@t*=0.27 : TN=2170, FP=693, FN=429, TP=8708\n",
    "\n",
    "Diff√©rences : TP +526, FN ‚Äì526 (on rate beaucoup moins de positifs) ; FP +367, TN ‚Äì367 (on se trompe plus souvent sur les n√©gatifs).\n",
    "\n",
    "M√©triques cl√©s :\n",
    "\n",
    "Classe 1 (positif) :\n",
    "\n",
    "Rappel ‚Üë 0.895 ‚Üí 0.953 (on attrape bien plus de positifs)\n",
    "\n",
    "Pr√©cision ‚Üì 0.962 ‚Üí 0.926 (un peu plus de faux positifs)\n",
    "\n",
    "F1 ‚Üë 0.927 ‚Üí 0.939\n",
    "\n",
    "Classe 0 (n√©gatif) :\n",
    "\n",
    "Rappel ‚Üì 0.886 ‚Üí 0.758 (on loupe plus de n√©gatifs)\n",
    "\n",
    "Pr√©cision ‚Üë 0.727 ‚Üí 0.835 (ceux pr√©dits n√©gatifs sont plus ‚Äúpropres‚Äù)\n",
    "\n",
    "F1 ‚âà 0.798 ‚Üí 0.795 (quasi inchang√©)\n",
    "\n",
    "Global :\n",
    "\n",
    "Accuracy ‚Üë 0.893 ‚Üí 0.906\n",
    "\n",
    "F1 macro ‚Üë 0.863 ‚Üí 0.867 (l√©ger mieux)\n",
    "\n",
    "F1 pond√©r√© ‚Üë 0.897 ‚Üí 0.905\n",
    "\n",
    "üëâ En bref : t* = 0.27 augmente la r√©cup√©ration des avis positifs (moins de FN), au prix d‚Äôun peu plus de faux positifs. L‚Äôaccuracy et le F1 global montent l√©g√®rement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a7d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuil F1* = 0.30 | Seuil pr√©cision>=0.90: 0.18620296551666296\n"
     ]
    }
   ],
   "source": [
    "# On r√©utilise le meilleur mod√®le cv pour d√©terminer un seuil sur la validation seule\n",
    "proba_va = gs.best_estimator_.predict_proba(X_va_)[:,1]\n",
    "\n",
    "# 1) t* qui maximise F1\n",
    "prec, rec, thr = precision_recall_curve(y_va, proba_va)\n",
    "f1 = (2*prec*rec)/(prec+rec+1e-9)\n",
    "idx = int(np.nanargmax(f1))\n",
    "t_star = float(thr[idx]) if idx < len(thr) else 0.5\n",
    "\n",
    "# 2) (option) seuil m√©tier: ex. pr√©cision >= 0.9\n",
    "t_prec90 = None\n",
    "for p, r, t in zip(prec, rec, np.r_[thr, 1.0]):\n",
    "    if p >= 0.90:\n",
    "        t_prec90 = float(t); break\n",
    "\n",
    "print(f\"Seuil F1* = {t_star:.2f}\", \"| Seuil pr√©cision>=0.90:\", t_prec90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c6e52",
   "metadata": {},
   "source": [
    "## üöÄ 6) S√©rialiser le mod√®le final + m√©tadonn√©es (incluant le seuil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2906b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap      = df_metrics.loc[df_metrics['setting']==\"Test @0.5\", \"AP\"].values[0]\n",
    "roc     = df_metrics.loc[df_metrics['setting']==\"Test @0.5\", \"ROC_AUC\"].values[0]\n",
    "f1_05   = df_metrics.loc[df_metrics['setting']==\"Test @0.5\", \"F1\"].values[0]\n",
    "f1_t    = df_metrics.loc[df_metrics['setting']==\"Test @t*\",  \"F1\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99135c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âcrit -> clf_logreg_chi2_final_20250922_030316.joblib\n"
     ]
    }
   ],
   "source": [
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "art = {\n",
    "    \"model\": best,\n",
    "    \"selector_chi2\": sel,      # peut √™tre None si tu n'as pas appliqu√© chi¬≤\n",
    "    \"selector_l1\": sfm,        # idem\n",
    "    \"svd\": svd_pipe,           # idem\n",
    "    \"threshold\": t_star,\n",
    "    \"metrics_test\": {\"AP\": float(ap), \"ROC_AUC\": float(roc), \"F1@0.5\": float(f1_05), \"F1@t*\": float(f1_t)},\n",
    "    \"params\": {\"C\": float(gs.best_params_[\"C\"]), \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"class_weight\": \"balanced\"},\n",
    "    \"created\": stamp\n",
    "}\n",
    "out_path = OUT_DIR / f\"clf_logreg_chi2_final_{stamp}.joblib\"\n",
    "dump(art, out_path, compress=3)\n",
    "\n",
    "meta = {\n",
    "    \"artifact\": out_path.name,\n",
    "    \"X_shape\": list(X_all.shape),\n",
    "    \"train/val/test\": {\"train\": int(X_tr_.shape[0]), \"val\": int(X_va_.shape[0]), \"test\": int(X_te_.shape[0])},\n",
    "    \"threshold\": float(t_star),\n",
    "    \"notes\": \"TF-IDF -> chi2(k) -> LogReg(lbfgs).\"\n",
    "}\n",
    "(Path(OUT_DIR)/f\"clf_logreg_chi2_final_{stamp}.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "print(\"√âcrit ->\", out_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd54e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet attendu: C:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\notebooks\\data\\interim\\amazon_electronics_normalized.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Params ===\n",
    "from pathlib import Path\n",
    "import time, json, numpy as np, pandas as pd\n",
    "\n",
    "RNG = 42\n",
    "N   = 200_000                   # <- on entraine sur 200k lignes\n",
    "DATA_PARQUET = Path(\"data/interim/amazon_electronics_normalized.parquet\")  # ton fichier 200k\n",
    "MODELS_DIR   = Path(\"models\"); MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEXT_COL = \"review_body\"        # adapte si besoin\n",
    "# Essaie ces colonnes comme cible binaire (1 = positif)\n",
    "CAND_Y = [\"label_binary\",\"is_positive\",\"target\",\"y\",\"sentiment_binary\"]\n",
    "\n",
    "print(\"Parquet attendu:\", DATA_PARQUET.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682d4e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape parquet: (1314720, 11)\n",
      "Cible construite √† partir de star_rating >= 4\n",
      "n_texts: 200000 | pos_ratio: 0.793\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Charger 200k & cible binaire ===\n",
    "assert DATA_PARQUET.exists(), \"Parquet introuvable. V√©rifie DATA_PARQUET.\"\n",
    "df = pd.read_parquet(DATA_PARQUET)\n",
    "print(\"Shape parquet:\", df.shape)\n",
    "\n",
    "# on garde les N premi√®res (ou moins si dataset plus petit)\n",
    "dfN = df.iloc[:N].copy()\n",
    "assert TEXT_COL in dfN.columns, f\"Colonne texte '{TEXT_COL}' absente.\"\n",
    "\n",
    "# cible binaire: on prend la premi√®re colonne qui existe parmi CAND_Y\n",
    "y = None\n",
    "for c in CAND_Y:\n",
    "    if c in dfN.columns:\n",
    "        y = dfN[c].astype(int).to_numpy()\n",
    "        print(\"Cible binaire trouv√©e dans la colonne:\", c)\n",
    "        break\n",
    "\n",
    "# Si pas trouv√©, REMPLACE par ta r√®gle m√©tier (ex: √©toiles >=4)\n",
    "if y is None:\n",
    "    # EXEMPLE: si tu as 'star_rating' dans le parquet\n",
    "    if \"star_rating\" in dfN.columns:\n",
    "        y = (dfN[\"star_rating\"].astype(int) >= 4).astype(int).to_numpy()\n",
    "        print(\"Cible construite √† partir de star_rating >= 4\")\n",
    "    else:\n",
    "        raise AssertionError(\n",
    "            \"Aucune cible binaire trouv√©e. Ajoute une colonne (ex: label_binary) \"\n",
    "            \"ou adapte la r√®gle avec star_rating.\"\n",
    "        )\n",
    "\n",
    "texts = dfN[TEXT_COL].astype(str).tolist()\n",
    "print(f\"n_texts: {len(texts)} | pos_ratio: {y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82b029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF fit en 81.1s | word=(200000, 200000) char=(200000, 100000)\n",
      "‚úî TF-IDF sauvegard√© -> features_tfidf_runtime_full_20250922_043241.joblib\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: TF-IDF(200k) ===\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time, joblib\n",
    "\n",
    "vec_word = TfidfVectorizer(\n",
    "    lowercase=True, strip_accents=\"unicode\",\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2, max_df=0.95,\n",
    "    max_features=200_000,\n",
    "    analyzer=\"word\"\n",
    ")\n",
    "vec_char = TfidfVectorizer(\n",
    "    lowercase=True, strip_accents=\"unicode\",\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2, max_df=0.95,\n",
    "    max_features=100_000,\n",
    "    analyzer=\"char\"\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "Xw = vec_word.fit_transform(texts)\n",
    "Xc = vec_char.fit_transform(texts)\n",
    "print(f\"TF-IDF fit en {time.time()-t0:.1f}s | word={Xw.shape} char={Xc.shape}\")\n",
    "\n",
    "# on ne stocke QUE les vectorizers (l√©ger)\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "tfidf_art = {\n",
    "    \"vectorizer_word\": vec_word,\n",
    "    \"vectorizer_char\": vec_char,\n",
    "    \"created\": stamp,\n",
    "    \"params\": {\n",
    "        \"word\": {\"ngram\": (1,2), \"min_df\":2, \"max_df\":0.95, \"max_features\": 200_000},\n",
    "        \"char\": {\"ngram\": (3,5), \"min_df\":2, \"max_df\":0.95, \"max_features\": 100_000},\n",
    "    }\n",
    "}\n",
    "tfidf_path = MODELS_DIR / f\"features_tfidf_runtime_full_{stamp}.joblib\"\n",
    "joblib.dump(tfidf_art, tfidf_path, compress=3)\n",
    "print(\"‚úî TF-IDF sauvegard√© ->\", tfidf_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09896dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sizes -> train:160000 val:20000 test:20000\n",
      "shapes: (160000, 300000) (20000, 300000) (20000, 300000)\n",
      "after chi¬≤: (160000, 50000)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: split + vectorize + chi2 ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# splits\n",
    "X_tr_txt, X_tmp_txt, y_tr, y_tmp = train_test_split(\n",
    "    texts, y, test_size=0.20, stratify=y, random_state=RNG\n",
    ")\n",
    "X_va_txt, X_te_txt, y_va, y_te = train_test_split(\n",
    "    X_tmp_txt, y_tmp, test_size=0.50, stratify=y_tmp, random_state=RNG\n",
    ")\n",
    "print(f\"split sizes -> train:{len(X_tr_txt)} val:{len(X_va_txt)} test:{len(X_te_txt)}\")\n",
    "\n",
    "# vectorize avec les vectorizers tout juste fit√©s\n",
    "X_tr_w = vec_word.transform(X_tr_txt); X_tr_c = vec_char.transform(X_tr_txt)\n",
    "X_va_w = vec_word.transform(X_va_txt); X_va_c = vec_char.transform(X_va_txt)\n",
    "X_te_w = vec_word.transform(X_te_txt); X_te_c = vec_char.transform(X_te_txt)\n",
    "\n",
    "X_tr = sparse.hstack([X_tr_w, X_tr_c]).tocsr()\n",
    "X_va = sparse.hstack([X_va_w, X_va_c]).tocsr()\n",
    "X_te = sparse.hstack([X_te_w, X_te_c]).tocsr()\n",
    "print(\"shapes:\", X_tr.shape, X_va.shape, X_te.shape)\n",
    "\n",
    "# chi¬≤: on FITTE sur TRAIN (important pour que n_in corresponde)\n",
    "K = 50_000  # k out\n",
    "sel = SelectKBest(score_func=chi2, k=K)\n",
    "X_tr_ = sel.fit_transform(X_tr, y_tr)\n",
    "X_va_ = sel.transform(X_va)\n",
    "X_te_ = sel.transform(X_te)\n",
    "print(\"after chi¬≤:\", X_tr_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c8051d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best: {'C': 0.5} AP(CV)= 0.985878656376807\n",
      "t* (val) = 0.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "setting",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "thr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ROC_AUC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BalancedAcc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e138e981-86bc-4eb6-96d4-c08a2c97619c",
       "rows": [
        [
         "0",
         "Test@0.5",
         "0.5",
         "0.982144587673053",
         "0.938004043723465",
         "0.8547",
         "0.8653131868611035",
         "0.9653835104854926",
         "0.8472204714483802",
         "0.902450486740517"
        ],
        [
         "1",
         "Test@t*",
         "0.2528497053493731",
         "0.982144587673053",
         "0.938004043723465",
         "0.8929",
         "0.8114925490826016",
         "0.9176506390748631",
         "0.9502710197907475",
         "0.9336759970274957"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting</th>\n",
       "      <th>thr</th>\n",
       "      <th>AP</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>BalancedAcc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test@0.5</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.982145</td>\n",
       "      <td>0.938004</td>\n",
       "      <td>0.8547</td>\n",
       "      <td>0.865313</td>\n",
       "      <td>0.965384</td>\n",
       "      <td>0.847220</td>\n",
       "      <td>0.902450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test@t*</td>\n",
       "      <td>0.25285</td>\n",
       "      <td>0.982145</td>\n",
       "      <td>0.938004</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.811493</td>\n",
       "      <td>0.917651</td>\n",
       "      <td>0.950271</td>\n",
       "      <td>0.933676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    setting      thr        AP   ROC_AUC  Accuracy  BalancedAcc  Precision  \\\n",
       "0  Test@0.5  0.50000  0.982145  0.938004    0.8547     0.865313   0.965384   \n",
       "1   Test@t*  0.25285  0.982145  0.938004    0.8929     0.811493   0.917651   \n",
       "\n",
       "     Recall        F1  \n",
       "0  0.847220  0.902450  \n",
       "1  0.950271  0.933676  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix ‚Äî Test @0.5 (thr=0.50)\n",
      "[[ 3652   482]\n",
      " [ 2424 13442]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.601     0.883     0.715      4134\n",
      "           1      0.965     0.847     0.902     15866\n",
      "\n",
      "    accuracy                          0.855     20000\n",
      "   macro avg      0.783     0.865     0.809     20000\n",
      "weighted avg      0.890     0.855     0.864     20000\n",
      "\n",
      "\n",
      "Confusion matrix ‚Äî Test @t* (thr=0.25)\n",
      "[[ 2781  1353]\n",
      " [  789 15077]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.779     0.673     0.722      4134\n",
      "           1      0.918     0.950     0.934     15866\n",
      "\n",
      "    accuracy                          0.893     20000\n",
      "   macro avg      0.848     0.811     0.828     20000\n",
      "weighted avg      0.889     0.893     0.890     20000\n",
      "\n",
      "‚úî Bundle sentiment sauvegard√© -> sentiment_bundle_runtime_20250922_043451.joblib\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: train LR + m√©triques + save bundle ===\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (precision_recall_curve, confusion_matrix,\n",
    "                             average_precision_score, roc_auc_score, f1_score,\n",
    "                             accuracy_score, balanced_accuracy_score, classification_report)\n",
    "import numpy as np, pandas as pd, joblib, time\n",
    "\n",
    "# gridsearch\n",
    "param_grid = {\"C\": [0.25, 0.5, 1.0, 2.0]}\n",
    "base = LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", class_weight=\"balanced\",\n",
    "                          max_iter=5000, tol=1e-3)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RNG)\n",
    "gs = GridSearchCV(base, param_grid=param_grid, scoring=\"average_precision\",\n",
    "                  cv=cv, n_jobs=1, pre_dispatch=1, refit=True, verbose=1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_tr_, y_tr)\n",
    "print(\"Best:\", gs.best_params_, \"AP(CV)=\", gs.best_score_)\n",
    "\n",
    "# refit sur train+val\n",
    "X_trva_ = sparse.vstack([X_tr_, X_va_])\n",
    "y_trva  = np.hstack([y_tr, y_va])\n",
    "best = LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", class_weight=\"balanced\",\n",
    "                          C=gs.best_params_[\"C\"], max_iter=5000, tol=1e-3)\n",
    "best.fit(X_trva_, y_trva)\n",
    "\n",
    "# proba val pour t*\n",
    "proba_va = gs.best_estimator_.predict_proba(X_va_)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_va, proba_va)\n",
    "f1_curve = (2*prec*rec)/(prec+rec+1e-9)\n",
    "i = int(np.nanargmax(f1_curve))\n",
    "t_star = float(thr[i]) if i < len(thr) else 0.5\n",
    "print(f\"t* (val) = {t_star:.2f}\")\n",
    "\n",
    "# proba test + m√©triques\n",
    "proba_te = best.predict_proba(X_te_)[:,1]\n",
    "\n",
    "def metrics_at(y_true, p, thr, label):\n",
    "    y_pred = (p >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        \"setting\": label, \"thr\": thr,\n",
    "        \"AP\": average_precision_score(y_true, p),\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, p),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"BalancedAcc\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": (tp/(tp+fp)) if (tp+fp)>0 else 0.0,\n",
    "        \"Recall\": (tp/(tp+fn)) if (tp+fn)>0 else 0.0,\n",
    "        \"F1\": f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "dfm = pd.DataFrame([\n",
    "    metrics_at(y_te, proba_te, 0.5, \"Test@0.5\"),\n",
    "    metrics_at(y_te, proba_te, t_star, \"Test@t*\")\n",
    "])\n",
    "display(dfm)\n",
    "\n",
    "def print_cm(y_true, p, thr, title):\n",
    "    y_pred = (p >= thr).astype(int)\n",
    "    print(f\"\\n{title} (thr={thr:.2f})\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "print_cm(y_te, proba_te, 0.5,  \"Confusion matrix ‚Äî Test @0.5\")\n",
    "print_cm(y_te, proba_te, t_star, f\"Confusion matrix ‚Äî Test @t*\")\n",
    "\n",
    "# save bundle complet\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "bundle = {\n",
    "    \"model\": best,\n",
    "    \"selector_chi2\": sel,\n",
    "    \"vectorizer_word\": vec_word,\n",
    "    \"vectorizer_char\": vec_char,\n",
    "    \"threshold\": t_star,\n",
    "    \"meta\": {\n",
    "        \"n_rows\": int(len(texts)),\n",
    "        \"split\": {\"train\": int(len(y_tr)), \"val\": int(len(y_va)), \"test\": int(len(y_te))},\n",
    "        \"tfidf_runtime\": tfidf_path.name,\n",
    "        \"created\": stamp\n",
    "    }\n",
    "}\n",
    "out = MODELS_DIR / f\"sentiment_bundle_runtime_{stamp}.joblib\"\n",
    "joblib.dump(bundle, out, compress=3)\n",
    "print(\"‚úî Bundle sentiment sauvegard√© ->\", out.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c0f7d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (120000, 180007) | y: (120000,) | pos_ratio: 0.7614583333333333\n",
      "bundle sentiment: clf_logreg_chi2_final_20250919_114948.joblib\n",
      "cl√©s bundle: ['model', 'selector_chi2', 'selector_l1', 'svd', 'threshold', 'metrics_test', 'params', 'created']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/processed\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "\n",
    "# Charger les derniers X/y\n",
    "X_paths = sorted(DATA_DIR.glob(\"X_tfidf*.npz\"))\n",
    "y_paths = sorted(DATA_DIR.glob(\"y_*binary*.joblib\"))\n",
    "assert X_paths and y_paths, \"G√©n√®re X/y en C5.2.1.\"\n",
    "X_all = sparse.load_npz(X_paths[-1]).tocsr().astype(np.float32)\n",
    "y_all = load(y_paths[-1]).astype(np.int8)\n",
    "print(\"X:\", X_all.shape, \"| y:\", y_all.shape, \"| pos_ratio:\", y_all.mean())\n",
    "\n",
    "# Charger le dernier bundle sentiment si dispo\n",
    "bundle_paths = sorted(MODELS_DIR.glob(\"clf_logreg_chi2_final_*.joblib\"))\n",
    "bundle = load(bundle_paths[-1]) if bundle_paths else None\n",
    "print(\"bundle sentiment:\", bundle_paths[-1].name if bundle else \"None\")\n",
    "\n",
    "if bundle:\n",
    "    print(\"cl√©s bundle:\", list(bundle.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42391561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet attendu: C:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\notebooks\\data\\interim\\amazon_electronics_normalized.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Params & paths ===\n",
    "from pathlib import Path\n",
    "import time, json, numpy as np, pandas as pd\n",
    "\n",
    "RNG = 42\n",
    "N   = 200_000  # <- on entra√Æne sur 200k lignes\n",
    "\n",
    "DATA_PARQUET = Path(\"data/interim/amazon_electronics_normalized.parquet\")  # ton fichier normalis√©\n",
    "MODELS_DIR   = Path(\"models\"); MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEXT_COL = \"review_body\"  # adapte si besoin\n",
    "\n",
    "# Cible binaire (1 = positif) : on teste ces noms de colonnes, sinon fallback sur star_rating>=4\n",
    "CAND_Y = [\"label_binary\",\"is_positive\",\"target\",\"y\",\"sentiment_binary\"]\n",
    "\n",
    "print(\"Parquet attendu:\", DATA_PARQUET.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faec99d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape parquet: (1314720, 11)\n",
      "Cible construite √† partir de star_rating >= 4\n",
      "n_texts: 200000 | pos_ratio: 0.793\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Charger 200k & cible binaire ===\n",
    "assert DATA_PARQUET.exists(), \"Parquet introuvable. V√©rifie DATA_PARQUET.\"\n",
    "df = pd.read_parquet(DATA_PARQUET)\n",
    "print(\"Shape parquet:\", df.shape)\n",
    "\n",
    "# on garde les N premi√®res (ou moins si dataset plus petit)\n",
    "dfN = df.iloc[:N].copy()\n",
    "assert TEXT_COL in dfN.columns, f\"Colonne texte '{TEXT_COL}' absente.\"\n",
    "\n",
    "# cible binaire: on prend la premi√®re colonne qui existe parmi CAND_Y\n",
    "y = None\n",
    "for c in CAND_Y:\n",
    "    if c in dfN.columns:\n",
    "        y = dfN[c].astype(int).to_numpy()\n",
    "        print(\"Cible binaire trouv√©e dans la colonne:\", c)\n",
    "        break\n",
    "\n",
    "# Si pas trouv√©, fallback (ex: 1 si star_rating >= 4)\n",
    "if y is None:\n",
    "    if \"star_rating\" in dfN.columns:\n",
    "        y = (dfN[\"star_rating\"].astype(int) >= 4).astype(int).to_numpy()\n",
    "        print(\"Cible construite √† partir de star_rating >= 4\")\n",
    "    else:\n",
    "        raise AssertionError(\n",
    "            \"Aucune cible binaire trouv√©e. Ajoute une colonne (ex: label_binary) \"\n",
    "            \"ou adapte la r√®gle avec star_rating.\"\n",
    "        )\n",
    "\n",
    "texts = dfN[TEXT_COL].astype(str).tolist()\n",
    "print(f\"n_texts: {len(texts)} | pos_ratio: {y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae33a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab sizes -> word: 200000 | char: 100000 | elapsed: 86.7s\n",
      "‚úî TF-IDF runtime sauvegard√© -> features_tfidf_runtime_full_20250922_045623.joblib\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: TF-IDF word+char (300k) ‚Äî FIT 1x & SAVE ===\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np, time\n",
    "\n",
    "# Vectorizer WORD: 1-2grams, 200k features\n",
    "vec_word = TfidfVectorizer(\n",
    "    lowercase=True, strip_accents=\"unicode\",\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2, max_df=0.90,\n",
    "    analyzer=\"word\",\n",
    "    max_features=200_000,\n",
    "    dtype=np.float32,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Vectorizer CHAR: 3-5grams, 100k features\n",
    "vec_char = TfidfVectorizer(\n",
    "    lowercase=True, strip_accents=\"unicode\",\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2,              # max_df pas n√©cessaire sur char-ngrams\n",
    "    analyzer=\"char\",\n",
    "    max_features=100_000,\n",
    "    dtype=np.float32,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "vec_word.fit(texts)  # fit sur les 200k textes (ou moins si df < N)\n",
    "vec_char.fit(texts)\n",
    "print(f\"Vocab sizes -> word: {len(vec_word.vocabulary_)} | char: {len(vec_char.vocabulary_)} | elapsed: {time.time()-t0:.1f}s\")\n",
    "\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "tfidf_path = MODELS_DIR / f\"features_tfidf_runtime_full_{stamp}.joblib\"\n",
    "dump({\"vectorizer_word\": vec_word, \"vectorizer_char\": vec_char, \"created\": stamp}, tfidf_path, compress=3)\n",
    "print(\"‚úî TF-IDF runtime sauvegard√© ->\", tfidf_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df1651dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sizes -> train:160000 val:20000 test:20000\n",
      "TF-IDF used: features_tfidf_runtime_full_20250922_045623.joblib\n",
      "shapes (pre-chi2): (160000, 300000) (20000, 300000) (20000, 300000)\n",
      "after chi¬≤: (160000, 50000)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: split + LOAD TF-IDF + vectorize + chi2 ===\n",
    "from joblib import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy import sparse\n",
    "\n",
    "# splits\n",
    "X_tr_txt, X_tmp_txt, y_tr, y_tmp = train_test_split(\n",
    "    texts, y, test_size=0.20, stratify=y, random_state=RNG\n",
    ")\n",
    "X_va_txt, X_te_txt, y_va, y_te = train_test_split(\n",
    "    X_tmp_txt, y_tmp, test_size=0.50, stratify=y_tmp, random_state=RNG\n",
    ")\n",
    "print(f\"split sizes -> train:{len(X_tr_txt)} val:{len(X_va_txt)} test:{len(X_te_txt)}\")\n",
    "\n",
    "# LOAD dernier TF-IDF sauvegard√© √† la cellule 3\n",
    "TFIDF_FILE = sorted(MODELS_DIR.glob(\"features_tfidf_runtime_full_*.joblib\"))[-1]\n",
    "tfidf_obj = load(TFIDF_FILE)\n",
    "vec_word, vec_char = tfidf_obj[\"vectorizer_word\"], tfidf_obj[\"vectorizer_char\"]\n",
    "print(\"TF-IDF used:\", TFIDF_FILE.name)\n",
    "\n",
    "# vectorize\n",
    "X_tr_w = vec_word.transform(X_tr_txt); X_tr_c = vec_char.transform(X_tr_txt)\n",
    "X_va_w = vec_word.transform(X_va_txt); X_va_c = vec_char.transform(X_va_txt)\n",
    "X_te_w = vec_word.transform(X_te_txt); X_te_c = vec_char.transform(X_te_txt)\n",
    "\n",
    "X_tr = sparse.hstack([X_tr_w, X_tr_c], format=\"csr\")\n",
    "X_va = sparse.hstack([X_va_w, X_va_c], format=\"csr\")\n",
    "X_te = sparse.hstack([X_te_w, X_te_c], format=\"csr\")\n",
    "print(\"shapes (pre-chi2):\", X_tr.shape, X_va.shape, X_te.shape)  # attendu ~ (.., 300000)\n",
    "\n",
    "# chi¬≤: FIT sur TRAIN -> r√©duit √† K\n",
    "K = 50_000\n",
    "sel = SelectKBest(score_func=chi2, k=K)\n",
    "X_tr_ = sel.fit_transform(X_tr, y_tr)\n",
    "X_va_ = sel.transform(X_va)\n",
    "X_te_ = sel.transform(X_te)\n",
    "print(\"after chi¬≤:\", X_tr_.shape)  # attendu -> (.., 50000)\n",
    "\n",
    "# (optionnel) sanity dims pour √©viter toute r√©gression\n",
    "assert getattr(sel, \"n_features_in_\", None) == (200_000 + 100_000), \"chi¬≤ n_features_in_ doit valoir 300000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ded8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best: {'C': 1.0} AP(CV)= 0.9865423632195015 | fit: 29.1s\n",
      "t* (val) = 0.220\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "setting",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "thr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ROC_AUC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BalancedAcc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9efd86fe-c868-49fb-a9cd-32a156d7c000",
       "rows": [
        [
         "0",
         "Test@0.5",
         "0.5",
         "0.9842050351116107",
         "0.9444719765701026",
         "0.86715",
         "0.8759326186760905",
         "0.9680391184182553",
         "0.8609605445606958",
         "0.9113653801247623"
        ],
        [
         "1",
         "Test@t*",
         "0.2200693730666215",
         "0.9842050351116107",
         "0.9444719765701026",
         "0.89715",
         "0.8117565068259445",
         "0.9167119319210574",
         "0.9573301399218455",
         "0.9365808540157238"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting</th>\n",
       "      <th>thr</th>\n",
       "      <th>AP</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>BalancedAcc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test@0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.984205</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>0.86715</td>\n",
       "      <td>0.875933</td>\n",
       "      <td>0.968039</td>\n",
       "      <td>0.860961</td>\n",
       "      <td>0.911365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test@t*</td>\n",
       "      <td>0.220069</td>\n",
       "      <td>0.984205</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>0.89715</td>\n",
       "      <td>0.811757</td>\n",
       "      <td>0.916712</td>\n",
       "      <td>0.957330</td>\n",
       "      <td>0.936581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    setting       thr        AP   ROC_AUC  Accuracy  BalancedAcc  Precision  \\\n",
       "0  Test@0.5  0.500000  0.984205  0.944472   0.86715     0.875933   0.968039   \n",
       "1   Test@t*  0.220069  0.984205  0.944472   0.89715     0.811757   0.916712   \n",
       "\n",
       "     Recall        F1  \n",
       "0  0.860961  0.911365  \n",
       "1  0.957330  0.936581  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix ‚Äî Test @0.5 (thr=0.50)\n",
      "[[ 3683   451]\n",
      " [ 2206 13660]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.625     0.891     0.735      4134\n",
      "           1      0.968     0.861     0.911     15866\n",
      "\n",
      "    accuracy                          0.867     20000\n",
      "   macro avg      0.797     0.876     0.823     20000\n",
      "weighted avg      0.897     0.867     0.875     20000\n",
      "\n",
      "\n",
      "Confusion matrix ‚Äî Test @t* (thr=0.22)\n",
      "[[ 2754  1380]\n",
      " [  677 15189]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.803     0.666     0.728      4134\n",
      "           1      0.917     0.957     0.937     15866\n",
      "\n",
      "    accuracy                          0.897     20000\n",
      "   macro avg      0.860     0.812     0.832     20000\n",
      "weighted avg      0.893     0.897     0.893     20000\n",
      "\n",
      "‚úî Bundle sentiment sauvegard√© -> sentiment_bundle_runtime_20250922_045812.joblib\n",
      "‚úî Copie pour l'app -> sentiment_bundle_runtime.joblib\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: train LR + m√©triques + save bundle ===\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (precision_recall_curve, confusion_matrix,\n",
    "                             average_precision_score, roc_auc_score, f1_score,\n",
    "                             accuracy_score, balanced_accuracy_score, classification_report)\n",
    "import numpy as np, pandas as pd, joblib, time\n",
    "from scipy import sparse\n",
    "\n",
    "# GridSearch sur C\n",
    "param_grid = {\"C\": [0.25, 0.5, 1.0, 2.0]}\n",
    "base = LogisticRegression(\n",
    "    solver=\"lbfgs\", penalty=\"l2\", class_weight=\"balanced\",\n",
    "    max_iter=5000, tol=1e-3\n",
    ")\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RNG)\n",
    "gs = GridSearchCV(base, param_grid=param_grid, scoring=\"average_precision\",\n",
    "                  cv=cv, n_jobs=1, pre_dispatch=1, refit=True, verbose=1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_tr_, y_tr)\n",
    "print(\"Best:\", gs.best_params_, \"AP(CV)=\", gs.best_score_, \"| fit:\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "# Refit final sur train+val\n",
    "X_trva_ = sparse.vstack([X_tr_, X_va_])\n",
    "y_trva  = np.hstack([y_tr, y_va])\n",
    "best = LogisticRegression(\n",
    "    solver=\"lbfgs\", penalty=\"l2\", class_weight=\"balanced\",\n",
    "    C=gs.best_params_[\"C\"], max_iter=5000, tol=1e-3\n",
    ").fit(X_trva_, y_trva)\n",
    "\n",
    "# Seuil t* sur la validation (max F1)\n",
    "proba_va = gs.best_estimator_.predict_proba(X_va_)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_va, proba_va)\n",
    "f1_curve = (2*prec*rec)/(prec+rec+1e-9)\n",
    "i = int(np.nanargmax(f1_curve))\n",
    "t_star = float(thr[i]) if i < len(thr) else 0.5\n",
    "print(f\"t* (val) = {t_star:.3f}\")\n",
    "\n",
    "# √âvaluation sur test\n",
    "proba_te = best.predict_proba(X_te_)[:,1]\n",
    "\n",
    "def metrics_at(y_true, p, thr, label):\n",
    "    y_pred = (p >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        \"setting\": label, \"thr\": thr,\n",
    "        \"AP\": average_precision_score(y_true, p),\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, p),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"BalancedAcc\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": (tp/(tp+fp)) if (tp+fp)>0 else 0.0,\n",
    "        \"Recall\": (tp/(tp+fn)) if (tp+fn)>0 else 0.0,\n",
    "        \"F1\": f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "dfm = pd.DataFrame([\n",
    "    metrics_at(y_te, proba_te, 0.5, \"Test@0.5\"),\n",
    "    metrics_at(y_te, proba_te, t_star, \"Test@t*\")\n",
    "])\n",
    "display(dfm)\n",
    "\n",
    "def print_cm(y_true, p, thr, title):\n",
    "    y_pred = (p >= thr).astype(int)\n",
    "    print(f\"\\n{title} (thr={thr:.2f})\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "print_cm(y_te, proba_te, 0.5,  \"Confusion matrix ‚Äî Test @0.5\")\n",
    "print_cm(y_te, proba_te, t_star, f\"Confusion matrix ‚Äî Test @t*\")\n",
    "\n",
    "# Sauvegarde du bundle complet (avec nom stable pour Streamlit)\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "bundle = {\n",
    "    \"model\": best,\n",
    "    \"selector_chi2\": sel,\n",
    "    \"vectorizer_word\": vec_word,\n",
    "    \"vectorizer_char\": vec_char,\n",
    "    \"threshold\": float(t_star),\n",
    "    \"meta\": {\n",
    "        \"n_rows\": int(len(texts)),\n",
    "        \"split\": {\"train\": int(len(y_tr)), \"val\": int(len(y_va)), \"test\": int(len(y_te))},\n",
    "        \"tfidf_runtime\": Path(TFIDF_FILE).name,\n",
    "        \"created\": stamp\n",
    "    }\n",
    "}\n",
    "out = MODELS_DIR / f\"sentiment_bundle_runtime_{stamp}.joblib\"\n",
    "joblib.dump(bundle, out, compress=3)\n",
    "joblib.dump(bundle, MODELS_DIR / \"sentiment_bundle_runtime.joblib\", compress=3)  # nom fixe pour l'app\n",
    "print(\"‚úî Bundle sentiment sauvegard√© ->\", out.name)\n",
    "print(\"‚úî Copie pour l'app -> sentiment_bundle_runtime.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cf7115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vocab: 200000 char vocab: 100000\n",
      "chi2 n_in: 300000 n_out: 50000\n",
      "LR coef dims: (1, 50000)\n",
      "X test shapes: (3, 300000) -> (3, 50000)\n",
      "proba: [0.7380225  0.80310091 0.01028482]\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6 (option): sanity dims ===\n",
    "from joblib import load\n",
    "b = load(MODELS_DIR / \"sentiment_bundle_runtime.joblib\")\n",
    "v_w = b[\"vectorizer_word\"]; v_c = b[\"vectorizer_char\"]; sel2 = b[\"selector_chi2\"]; lr2 = b[\"model\"]\n",
    "\n",
    "print(\"word vocab:\", len(v_w.vocabulary_), \"char vocab:\", len(v_c.vocabulary_))\n",
    "print(\"chi2 n_in:\", getattr(sel2, \"n_features_in_\", None), \"n_out:\", sel2.get_support().sum())\n",
    "print(\"LR coef dims:\", lr2.coef_.shape)\n",
    "\n",
    "# test rapide sur 3 textes\n",
    "Xw = v_w.transform(texts[:3]); Xc = v_c.transform(texts[:3])\n",
    "X  = sparse.hstack([Xw, Xc], format=\"csr\")\n",
    "X_ = sel2.transform(X)\n",
    "print(\"X test shapes:\", X.shape, \"->\", X_.shape)\n",
    "print(\"proba:\", lr2.predict_proba(X_)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972eb9b4",
   "metadata": {},
   "source": [
    "## B) √âMOTIONS (multi-label) ‚Äî teacher ‚Üí pseudo-labels ‚Üí student SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7cb5a",
   "metadata": {},
   "source": [
    "- Id√©e : on utilise un mod√®le pr√©-entra√Æn√© √©motions (teacher) pour pseudo-labelliser un √©chantillon de tes avis.\n",
    "- Puis on entra√Æne un student rapide : SBERT embeddings ‚Üí One-vs-Rest LogReg (multi-label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238a5e9",
   "metadata": {},
   "source": [
    "> Mod√®le teacher conseill√© : joeddav/distilbert-base-uncased-go-emotions-student (l√©ger, 28 √©motions GoEmotions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7cf8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1314720"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B1) Charger le texte brut (le parquet/CSV normalis√© C5.1.1)\n",
    "TEXT_PARQUET = DATA_DIR / \"C:/Users/antoi/OneDrive/Documents/Ynov/Projet fil rouge/Bloc 5/amazon-reviews-insights/notebooks/data/interim/amazon_electronics_normalized.parquet\"  # adapte si besoin\n",
    "assert TEXT_PARQUET.exists(), f\"{TEXT_PARQUET} introuvable (exporte le parquet en C5.1.1).\"\n",
    "df = pd.read_parquet(TEXT_PARQUET)\n",
    "assert \"review_body\" in df.columns, \"Colonne review_body absente.\"\n",
    "texts = df[\"review_body\"].astype(str).tolist()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0f6ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-labels √©motions charg√©s: emotions_goemotions.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "emo_admiration",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_amusement",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_anger",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_annoyance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_approval",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_caring",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_confusion",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_curiosity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_desire",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_disappointment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_disapproval",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_disgust",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_embarrassment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_excitement",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_fear",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_gratitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_grief",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_joy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_love",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_nervousness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_optimism",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_pride",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_realization",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_relief",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_remorse",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_sadness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_surprise",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emo_neutral",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "dc8a2c7d-28ae-459b-84c3-732abddd301b",
       "rows": [
        [
         "0",
         "0.024527613073587418",
         "0.02908422239124775",
         "0.008322902955114841",
         "0.013083244673907757",
         "0.17281343042850494",
         "0.09721365571022034",
         "0.010339966043829918",
         "0.037348248064517975",
         "0.024304185062646866",
         "0.003253181930631399",
         "0.006245272234082222",
         "0.0075559550896286964",
         "0.009790271520614624",
         "0.018943123519420624",
         "0.008293414488434792",
         "0.016194293275475502",
         "0.009441192261874676",
         "0.01922108232975006",
         "0.010833133943378925",
         "0.018690986558794975",
         "0.05995050072669983",
         "0.04619671031832695",
         "0.12851832807064056",
         "0.07823843508958817",
         "0.01286399271339178",
         "0.007100278045982122",
         "0.013990071602165699",
         "0.10764232277870178"
        ],
        [
         "1",
         "0.01149833109229803",
         "0.018305007368326187",
         "0.01378216315060854",
         "0.055265873670578",
         "0.011068196035921574",
         "0.03072471357882023",
         "0.19420412182807922",
         "0.26759523153305054",
         "0.013572191819548607",
         "0.03656651824712753",
         "0.03424399718642235",
         "0.02300157956779003",
         "0.052125219255685806",
         "0.014675913378596306",
         "0.008049053139984608",
         "0.006680406164377928",
         "0.011029974557459354",
         "0.009457968175411224",
         "0.005769980605691671",
         "0.010786241851747036",
         "0.012822157703340054",
         "0.015537413768470287",
         "0.040305983275175095",
         "0.01179115753620863",
         "0.025945957750082016",
         "0.02163257636129856",
         "0.023347007110714912",
         "0.02021504007279873"
        ],
        [
         "2",
         "0.01115580927580595",
         "0.011994877830147743",
         "0.05697629973292351",
         "0.1760520190000534",
         "0.044197533279657364",
         "0.040556859225034714",
         "0.05649006739258766",
         "0.029103556647896767",
         "0.0339924655854702",
         "0.06778506189584732",
         "0.07855517417192459",
         "0.02861705794930458",
         "0.038868289440870285",
         "0.0386662632226944",
         "0.02227160893380642",
         "0.008037159219384193",
         "0.020872686058282852",
         "0.006742892321199179",
         "0.0053866212256252766",
         "0.02759392373263836",
         "0.014821415767073631",
         "0.0210966095328331",
         "0.04743906483054161",
         "0.015240892767906189",
         "0.03917526453733444",
         "0.017034506425261497",
         "0.029049435630440712",
         "0.012226591818034649"
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emo_admiration</th>\n",
       "      <th>emo_amusement</th>\n",
       "      <th>emo_anger</th>\n",
       "      <th>emo_annoyance</th>\n",
       "      <th>emo_approval</th>\n",
       "      <th>emo_caring</th>\n",
       "      <th>emo_confusion</th>\n",
       "      <th>emo_curiosity</th>\n",
       "      <th>emo_desire</th>\n",
       "      <th>emo_disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>emo_love</th>\n",
       "      <th>emo_nervousness</th>\n",
       "      <th>emo_optimism</th>\n",
       "      <th>emo_pride</th>\n",
       "      <th>emo_realization</th>\n",
       "      <th>emo_relief</th>\n",
       "      <th>emo_remorse</th>\n",
       "      <th>emo_sadness</th>\n",
       "      <th>emo_surprise</th>\n",
       "      <th>emo_neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024528</td>\n",
       "      <td>0.029084</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.172813</td>\n",
       "      <td>0.097214</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.037348</td>\n",
       "      <td>0.024304</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>0.018691</td>\n",
       "      <td>0.059951</td>\n",
       "      <td>0.046197</td>\n",
       "      <td>0.128518</td>\n",
       "      <td>0.078238</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.013990</td>\n",
       "      <td>0.107642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011498</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.055266</td>\n",
       "      <td>0.011068</td>\n",
       "      <td>0.030725</td>\n",
       "      <td>0.194204</td>\n",
       "      <td>0.267595</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>0.036567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005770</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.015537</td>\n",
       "      <td>0.040306</td>\n",
       "      <td>0.011791</td>\n",
       "      <td>0.025946</td>\n",
       "      <td>0.021633</td>\n",
       "      <td>0.023347</td>\n",
       "      <td>0.020215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>0.056976</td>\n",
       "      <td>0.176052</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.040557</td>\n",
       "      <td>0.056490</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>0.033992</td>\n",
       "      <td>0.067785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.027594</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.047439</td>\n",
       "      <td>0.015241</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>0.017035</td>\n",
       "      <td>0.029049</td>\n",
       "      <td>0.012227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   emo_admiration  emo_amusement  emo_anger  emo_annoyance  emo_approval  \\\n",
       "0        0.024528       0.029084   0.008323       0.013083      0.172813   \n",
       "1        0.011498       0.018305   0.013782       0.055266      0.011068   \n",
       "2        0.011156       0.011995   0.056976       0.176052      0.044198   \n",
       "\n",
       "   emo_caring  emo_confusion  emo_curiosity  emo_desire  emo_disappointment  \\\n",
       "0    0.097214       0.010340       0.037348    0.024304            0.003253   \n",
       "1    0.030725       0.194204       0.267595    0.013572            0.036567   \n",
       "2    0.040557       0.056490       0.029104    0.033992            0.067785   \n",
       "\n",
       "   ...  emo_love  emo_nervousness  emo_optimism  emo_pride  emo_realization  \\\n",
       "0  ...  0.010833         0.018691      0.059951   0.046197         0.128518   \n",
       "1  ...  0.005770         0.010786      0.012822   0.015537         0.040306   \n",
       "2  ...  0.005387         0.027594      0.014821   0.021097         0.047439   \n",
       "\n",
       "   emo_relief  emo_remorse  emo_sadness  emo_surprise  emo_neutral  \n",
       "0    0.078238     0.012864     0.007100      0.013990     0.107642  \n",
       "1    0.011791     0.025946     0.021633      0.023347     0.020215  \n",
       "2    0.015241     0.039175     0.017035      0.029049     0.012227  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B2) Pseudo-labels avec un teacher √©motions (cache en Parquet)\n",
    "from transformers import pipeline\n",
    "\n",
    "EMO_PSEUDO = PSEUDO_DIR / \"emotions_goemotions.parquet\"\n",
    "LABELS_EMO = None\n",
    "\n",
    "if EMO_PSEUDO.exists():\n",
    "    emo_df = pd.read_parquet(EMO_PSEUDO)\n",
    "    LABELS_EMO = [c for c in emo_df.columns if c.startswith(\"emo_\")]\n",
    "    print(\"Pseudo-labels √©motions charg√©s:\", EMO_PSEUDO.name)\n",
    "else:\n",
    "    emo_pipe = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "        return_all_scores=True, device=DEVICE\n",
    "    )\n",
    "    # ‚ö†Ô∏è pour le temps : prends un sous-√©chantillon (ex. 20k) ajustable\n",
    "    N_EMO = min(20_000, len(texts))\n",
    "    subset = texts[:N_EMO]\n",
    "    rows = []\n",
    "    for scores in emo_pipe(subset, truncation=True, batch_size=32):\n",
    "        d = {f\"emo_{s['label']}\": s[\"score\"] for s in scores}\n",
    "        rows.append(d)\n",
    "    emo_df = pd.DataFrame(rows).fillna(0.0)\n",
    "    LABELS_EMO = emo_df.columns.tolist()\n",
    "    emo_df.to_parquet(EMO_PSEUDO)\n",
    "    print(\"√âcrit pseudo-labels ‚Üí\", EMO_PSEUDO.name)\n",
    "\n",
    "emo_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4ae9f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches:   0%|          | 0/63 [00:00<?, ?it/s]c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:04<00:00, 13.62it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 14.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((16000, 384), (16000, 28))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B3) Multi-hot (seuil), split, SBERT embeddings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Seuil d'activation par √©motion (simple): 0.30\n",
    "TH_EMO = 0.30\n",
    "Y = (emo_df.values >= TH_EMO).astype(int)\n",
    "\n",
    "# textes align√©s au pseudo labelling (si sous-√©chantillonnage)\n",
    "N = len(emo_df); texts_emo = texts[:N]\n",
    "\n",
    "X_tr_txt, X_te_txt, Y_tr, Y_te = train_test_split(texts_emo, Y, test_size=0.2, random_state=RNG)\n",
    "\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\" if USE_GPU else \"cpu\")\n",
    "X_tr_emb = sbert.encode(X_tr_txt, batch_size=256, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "X_te_emb = sbert.encode(X_te_txt, batch_size=256, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "X_tr_emb.shape, Y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8ab59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On garde 10 / 28 labels : ['emo_amusement', 'emo_annoyance', 'emo_approval', 'emo_confusion', 'emo_desire', 'emo_disappointment', 'emo_excitement', 'emo_gratitude', 'emo_relief', 'emo_surprise']\n",
      "√âMOTIONS (filtr√©es=10) ‚Äî AP_macro=0.349 | F1_micro=0.265\n"
     ]
    }
   ],
   "source": [
    "# === Filtrer les √©tiquettes trop rares ===\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "\n",
    "labels = np.array(LABELS_EMO)  # d√©j√† d√©fini plus haut\n",
    "cnt_tr = Y_tr.sum(axis=0).astype(int)\n",
    "cnt_te = Y_te.sum(axis=0).astype(int)\n",
    "\n",
    "MIN_TR, MIN_TE = 30, 5  # √† ajuster selon la taille de ton sous-√©chantillon\n",
    "keep = (cnt_tr >= MIN_TR) & (cnt_te >= MIN_TE)\n",
    "print(f\"On garde {keep.sum()} / {len(keep)} labels :\", labels[keep].tolist())\n",
    "\n",
    "Ytr_k = Y_tr[:, keep]\n",
    "Yte_k = Y_te[:, keep]\n",
    "\n",
    "# (conseill√©) standardiser les embeddings SBERT\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "Xtr_s = scaler.fit_transform(X_tr_emb)\n",
    "Xte_s = scaler.transform(X_te_emb)\n",
    "\n",
    "emo_clf = OneVsRestClassifier(\n",
    "    LogisticRegression(max_iter=3000, class_weight=\"balanced\", solver=\"lbfgs\", C=1.0)\n",
    ")\n",
    "emo_clf.fit(Xtr_s, Ytr_k)\n",
    "\n",
    "proba = emo_clf.predict_proba(Xte_s)\n",
    "pred  = (proba >= 0.5).astype(int)\n",
    "\n",
    "aps = [average_precision_score(Yte_k[:,j], proba[:,j]) for j in range(Yte_k.shape[1]) if Yte_k[:,j].sum()>0]\n",
    "ap_macro = float(np.mean(aps)) if aps else 0.0\n",
    "f1_micro = f1_score(Yte_k, pred, average=\"micro\")\n",
    "print(f\"√âMOTIONS (filtr√©es={Ytr_k.shape[1]}) ‚Äî AP_macro={ap_macro:.3f} | F1_micro={f1_micro:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7abb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On repart de tes objets d√©j√† cr√©√©s : X_tr_emb, X_te_emb, Ytr_k, Yte_k\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "RNG = 42\n",
    "Xtr, Xva, Ytr, Yva = train_test_split(X_tr_emb, Ytr_k, test_size=0.2, random_state=RNG)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtr_s = scaler.fit_transform(Xtr)   # fit sur TRAIN seulement\n",
    "Xva_s = scaler.transform(Xva)\n",
    "Xte_s = scaler.transform(X_te_emb)  # transform test avec le m√™me scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eefbb75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.25 | AP_macro(val)=0.299\n",
      "C=0.5 | AP_macro(val)=0.282\n",
      "C=1.0 | AP_macro(val)=0.272\n",
      "C=2.0 | AP_macro(val)=0.261\n",
      "Meilleur C: 0.25 | AP_macro(val): 0.29909514751249805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92889581, 0.57873407, 0.85098745, 0.00153819, 0.92437752])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "Cs = [0.25, 0.5, 1.0, 2.0]\n",
    "best = (None, -1, None)  # (clf, ap_val, proba_val)\n",
    "\n",
    "for C in Cs:\n",
    "    clf = OneVsRestClassifier(\n",
    "        LogisticRegression(max_iter=3000, class_weight=\"balanced\", solver=\"lbfgs\", C=C)\n",
    "    )\n",
    "    clf.fit(Xtr_s, Ytr)\n",
    "    proba_va = clf.predict_proba(Xva_s)\n",
    "    aps = [average_precision_score(Yva[:, j], proba_va[:, j])\n",
    "           for j in range(Yva.shape[1]) if Yva[:, j].sum() > 0]\n",
    "    ap_val = float(np.mean(aps)) if aps else 0.0\n",
    "    print(f\"C={C} | AP_macro(val)={ap_val:.3f}\")\n",
    "    if ap_val > best[1]:\n",
    "        best = (clf, ap_val, proba_va)\n",
    "\n",
    "emo_clf, ap_val, proba_va = best\n",
    "print(\"Meilleur C:\", emo_clf.estimator.C, \"| AP_macro(val):\", ap_val)\n",
    "\n",
    "# Seuil optimal par √©tiquette (max F1 sur la val)\n",
    "thr = np.full(Yva.shape[1], 0.5, dtype=float)\n",
    "for j in range(Yva.shape[1]):\n",
    "    if Yva[:, j].sum() == 0:\n",
    "        continue\n",
    "    p, r, t = precision_recall_curve(Yva[:, j], proba_va[:, j])\n",
    "    f1 = (2*p*r)/(p+r+1e-9)\n",
    "    i = int(np.nanargmax(f1))\n",
    "    thr[j] = float(t[i]) if i < len(t) else 0.5\n",
    "thr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a3101d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label_idx",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "thr_val",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c34704af-729f-42c2-80e6-17527089e0a8",
       "rows": [
        [
         "0",
         "0",
         "33",
         "14",
         "0.9288958056945655"
        ],
        [
         "1",
         "1",
         "56",
         "10",
         "0.5787340696288341"
        ],
        [
         "2",
         "2",
         "38",
         "6",
         "0.8509874455946026"
        ],
        [
         "3",
         "3",
         "49",
         "7",
         "0.0015381883327783936"
        ],
        [
         "4",
         "4",
         "31",
         "6",
         "0.9243775221611099"
        ],
        [
         "5",
         "5",
         "208",
         "53",
         "0.9894054672157842"
        ],
        [
         "6",
         "6",
         "22",
         "8",
         "0.5438569382061721"
        ],
        [
         "7",
         "7",
         "60",
         "10",
         "0.9913750482864097"
        ],
        [
         "8",
         "8",
         "29",
         "10",
         "0.8940842162795725"
        ],
        [
         "9",
         "9",
         "74",
         "21",
         "0.9569756611520835"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_idx</th>\n",
       "      <th>support_train</th>\n",
       "      <th>support_val</th>\n",
       "      <th>thr_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>0.928896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>0.578734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0.850987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>0.924378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>53</td>\n",
       "      <td>0.989405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0.543857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.991375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>0.894084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "      <td>21</td>\n",
       "      <td>0.956976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_idx  support_train  support_val   thr_val\n",
       "0          0             33           14  0.928896\n",
       "1          1             56           10  0.578734\n",
       "2          2             38            6  0.850987\n",
       "3          3             49            7  0.001538\n",
       "4          4             31            6  0.924378\n",
       "5          5            208           53  0.989405\n",
       "6          6             22            8  0.543857\n",
       "7          7             60           10  0.991375\n",
       "8          8             29           10  0.894084\n",
       "9          9             74           21  0.956976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "median",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "p90",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b4fe3965-682b-464c-a566-f961282e6c5a",
       "rows": [
        [
         "5",
         "5",
         "0.06672197004091457",
         "1.6802410844853302e-05",
         "0.2105343385830925",
         "0.1257507289204499",
         "0.9999999999690325",
         "7.977431498087752e-17"
        ],
        [
         "9",
         "9",
         "0.025200657834853965",
         "3.013403826082359e-07",
         "0.12466212626841144",
         "0.007621390830539694",
         "0.9999978796841564",
         "2.4493651852887806e-22"
        ],
        [
         "7",
         "7",
         "0.012177434424742728",
         "1.0519317123193148e-06",
         "0.08169562398253213",
         "0.0033782406822135875",
         "0.9999292545186184",
         "1.1700211795401694e-19"
        ],
        [
         "1",
         "1",
         "0.010301336786609343",
         "7.576484467732652e-08",
         "0.0804343110452479",
         "0.0005037510107076267",
         "0.9999344690510799",
         "1.0052056960824126e-19"
        ],
        [
         "3",
         "3",
         "0.009153183681380064",
         "1.5026329575179035e-07",
         "0.07663713713490358",
         "0.0004613662258322092",
         "0.9994218326150626",
         "1.8192588281018386e-16"
        ],
        [
         "8",
         "8",
         "0.006863509316215469",
         "1.1598187856535134e-06",
         "0.06617013495783755",
         "0.000748359180294665",
         "0.9997363382968903",
         "2.515744630892918e-14"
        ],
        [
         "0",
         "0",
         "0.005737980457448437",
         "2.742050523611399e-06",
         "0.0648724871155432",
         "0.0002968031347879278",
         "0.9999979092238418",
         "3.7012453779205896e-14"
        ],
        [
         "2",
         "2",
         "0.0055424793089680425",
         "1.132588088731279e-06",
         "0.06043190196632173",
         "0.0004503817744708607",
         "0.9999994913123039",
         "4.824623873394474e-13"
        ],
        [
         "6",
         "6",
         "0.004816950135167291",
         "7.31756801856773e-07",
         "0.048072076218276",
         "0.0005728635207847355",
         "0.9991700598606837",
         "1.5998161500898977e-14"
        ],
        [
         "4",
         "4",
         "0.004653498689426981",
         "1.468289185491441e-06",
         "0.04744169166160036",
         "0.0005260948154525324",
         "0.9880096107256011",
         "3.445707034945002e-16"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>p90</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.066722</td>\n",
       "      <td>1.680241e-05</td>\n",
       "      <td>0.210534</td>\n",
       "      <td>0.125751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.977431e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.025201</td>\n",
       "      <td>3.013404e-07</td>\n",
       "      <td>0.124662</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>2.449365e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>1.051932e-06</td>\n",
       "      <td>0.081696</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>1.170021e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.010301</td>\n",
       "      <td>7.576484e-08</td>\n",
       "      <td>0.080434</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>1.005206e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.009153</td>\n",
       "      <td>1.502633e-07</td>\n",
       "      <td>0.076637</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>1.819259e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>1.159819e-06</td>\n",
       "      <td>0.066170</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.999736</td>\n",
       "      <td>2.515745e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>2.742051e-06</td>\n",
       "      <td>0.064872</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>3.701245e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005542</td>\n",
       "      <td>1.132588e-06</td>\n",
       "      <td>0.060432</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>4.824624e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>7.317568e-07</td>\n",
       "      <td>0.048072</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.999170</td>\n",
       "      <td>1.599816e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>1.468289e-06</td>\n",
       "      <td>0.047442</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.988010</td>\n",
       "      <td>3.445707e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label      mean        median       std       p90       max           min\n",
       "5      5  0.066722  1.680241e-05  0.210534  0.125751  1.000000  7.977431e-17\n",
       "9      9  0.025201  3.013404e-07  0.124662  0.007621  0.999998  2.449365e-22\n",
       "7      7  0.012177  1.051932e-06  0.081696  0.003378  0.999929  1.170021e-19\n",
       "1      1  0.010301  7.576484e-08  0.080434  0.000504  0.999934  1.005206e-19\n",
       "3      3  0.009153  1.502633e-07  0.076637  0.000461  0.999422  1.819259e-16\n",
       "8      8  0.006864  1.159819e-06  0.066170  0.000748  0.999736  2.515745e-14\n",
       "0      0  0.005738  2.742051e-06  0.064872  0.000297  0.999998  3.701245e-14\n",
       "2      2  0.005542  1.132588e-06  0.060432  0.000450  0.999999  4.824624e-13\n",
       "6      6  0.004817  7.317568e-07  0.048072  0.000573  0.999170  1.599816e-14\n",
       "4      4  0.004653  1.468289e-06  0.047442  0.000526  0.988010  3.445707e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell A - diagnostics: support labels & proba stats\n",
    "\n",
    "# Ytr, Yva, Yte are assumed binary indicator matrices (n_samples, n_labels)\n",
    "# proba_va is the predicted probabilities you already computed for the best model on val\n",
    "\n",
    "# 1) support\n",
    "support = Ytr.sum(axis=0).astype(int)\n",
    "support_va = Yva.sum(axis=0).astype(int)\n",
    "support_df = pd.DataFrame({\n",
    "    \"label_idx\": np.arange(len(support)),\n",
    "    \"support_train\": support,\n",
    "    \"support_val\": support_va,\n",
    "    \"thr_val\": thr\n",
    "})\n",
    "display(support_df.head(20))\n",
    "\n",
    "# 2) proba distribution summary per label\n",
    "proba_stats = []\n",
    "for j in range(proba_va.shape[1]):\n",
    "    arr = proba_va[:, j]\n",
    "    proba_stats.append({\n",
    "        \"label\": j,\n",
    "        \"mean\": float(np.nanmean(arr)),\n",
    "        \"median\": float(np.nanmedian(arr)),\n",
    "        \"std\": float(np.nanstd(arr)),\n",
    "        \"p90\": float(np.nanpercentile(arr,90)),\n",
    "        \"max\": float(np.nanmax(arr)),\n",
    "        \"min\": float(np.nanmin(arr))\n",
    "    })\n",
    "proba_stats_df = pd.DataFrame(proba_stats).sort_values(\"mean\", ascending=False)\n",
    "display(proba_stats_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d984d5e",
   "metadata": {},
   "source": [
    "# Interpr√©tation des r√©sultats ‚Äî Support & statistiques des probabilit√©s (Cell A)\n",
    "\n",
    "## Rappel des colonnes\n",
    "- **label_idx** : identifiant num√©rique de l'√©tiquette (emotion).  \n",
    "- **support_train** : nombre d'exemples positifs pour cette √©tiquette dans l'ensemble d'entra√Ænement.  \n",
    "- **support_val** : nombre d'exemples positifs pour cette √©tiquette dans la validation.  \n",
    "- **thr_val** : seuil optimis√© (sur la validation) calcul√© pour maximiser le F1 de cette √©tiquette (peut √™tre instable si peu de donn√©es).  \n",
    "\n",
    "Le deuxi√®me tableau donne, pour chaque √©tiquette, des statistiques sur les probabilit√©s pr√©dites (`proba_va`) :\n",
    "- **mean / median / std** : mesures de tendance centrale et dispersion des probabilit√©s.  \n",
    "- **p90** : 90·µâ centile (valeur au-dessus de laquelle se situent les 10% plus fortes probabilit√©s).  \n",
    "- **min / max** : valeurs extr√™mes observ√©es.\n",
    "\n",
    "---\n",
    "\n",
    "## Ce que montrent ces r√©sultats (lecture synth√©tique)\n",
    "1. **D√©s√©quilibre des classes**  \n",
    "   - Certaines √©tiquettes ont beaucoup d'exemples (ex. `support_train` ‚âà 200) alors que d'autres sont rares (‚âà 20‚Äì30).  \n",
    "   - Les √©tiquettes rares ont des m√©triques instables et des courbes PR souvent bruyantes.\n",
    "\n",
    "2. **Seuils tr√®s variables**  \n",
    "   - Des seuils **tr√®s √©lev√©s** (‚âà 0.9) signifient : le mod√®le n'attribue une probabilit√© √©lev√©e qu'√† tr√®s peu d'exemples ‚Äî il est tr√®s conservateur pour cette √©tiquette.  \n",
    "   - Un seuil **tr√®s bas** (‚âà 0.0015) indique que la F1 maximale s'obtient en acceptant presque toutes les pr√©dictions non-nulles ‚Äî souvent signe de distribution des probabilit√©s tr√®s proches de z√©ro pour la majorit√© des √©chantillons.\n",
    "\n",
    "3. **Distribution des probabilit√©s**  \n",
    "   - `mean` et `p90` faibles (proches de 0) pour beaucoup d'√©tiquettes ‚Üí la majorit√© des pr√©dictions sont proches de 0.  \n",
    "   - `max` proche de 1 pour plusieurs labels ‚Üí il existe au moins quelques pr√©dictions tr√®s confiantes (bon signe : quelques vrais positifs tr√®s bien d√©tect√©s).\n",
    "\n",
    "---\n",
    "\n",
    "## Interpr√©tation op√©rationnelle (ce que cela signifie concr√®tement)\n",
    "- Le mod√®le **rep√®re correctement quelques exemples** avec forte confiance, mais **ne g√©n√©ralise pas** bien sur la majorit√© des cas pour certaines √©tiquettes (probablement √† cause du peu d'exemples et/ou de features non discriminantes).  \n",
    "- Les **seuils calcul√©s sur un seul split validation peuvent √™tre instables** (surtout pour √©tiquettes rares). Donc ces `thr_val` ne sont pas fiables en production sans validation suppl√©mentaire.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommandations imm√©diates (actions √† court terme)\n",
    "1. **Ne pas d√©ployer des seuils quasi-z√©ro** : imposer un plancher (`min_threshold`, ex. 0.05) en production pour limiter les faux positifs absurdes.  \n",
    "2. **Calibrer les probabilit√©s** (CalibratedClassifierCV, `sigmoid` ou `isotonic`) pour obtenir des scores plus interpr√©tables avant de fixer des seuils.  \n",
    "3. **Estimer des seuils robustes par CV** : calculer les seuils sur plusieurs folds (p.ex. K-fold) et prendre la moyenne ou la m√©diane des seuils au lieu d‚Äôun unique split.  \n",
    "4. **Regrouper ou r√©-annoter les labels trop rares** (par ex. fusionner √©tiquettes proches s√©mantiquement) ou collecter plus d‚Äôexemples.  \n",
    "5. **Tester SBERT embeddings + un classifieur (LR/MLP)** ‚Äî souvent plus discriminant que TF-IDF pour √©motions fines.  \n",
    "6. **Utiliser des m√©thodes qui exploitent la corr√©lation entre labels** (ClassifierChain, mod√®les NN multi-label) si plusieurs √©motions coexistent fr√©quemment.\n",
    "\n",
    "---\n",
    "\n",
    "## Points courts √† dire √† l‚Äôoral (script)\n",
    "- ¬´ Les tableaux montrent que la distribution des labels est tr√®s in√©gale : quelques √©motions sont fr√©quentes, d‚Äôautres sont rares. ¬ª  \n",
    "- ¬´ Pour beaucoup d‚Äô√©tiquettes, la majorit√© des probabilit√©s pr√©dites est tr√®s proche de z√©ro, mais il existe quelques pr√©dictions tr√®s confiantes ‚Äî notre mod√®le saisit certains cas typiques mais manque de g√©n√©ralisation. ¬ª  \n",
    "- ¬´ Les seuils calcul√©s actuellement sont sensibles √† notre split de validation : pour plus de robustesse, nous allons calibrer les probabilit√©s et calculer des seuils par K-fold, puis appliquer un plancher minimal en production. ¬ª  \n",
    "- ¬´ En parall√®le, nous testerons SBERT + logistic/MLP et des approches multi-label pour tenter d‚Äôam√©liorer la discrimination sur les √©motions subtiles. ¬ª\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion (1 phrase)\n",
    "> ¬´ En l‚Äô√©tat, le mod√®le fournit une base correcte (quelques vrais positifs bien d√©tect√©s) mais n√©cessite calibration, seuils robustes et probablement de meilleures repr√©sentations (embeddings) ou plus de donn√©es pour √™tre fiable en production. ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a835fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AP_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8140b9c7-dff9-4626-8508-2143513aeec2",
       "rows": [
        [
         "0",
         "0",
         "0.6778586771853565",
         "14"
        ],
        [
         "1",
         "8",
         "0.5995333992892254",
         "10"
        ],
        [
         "2",
         "2",
         "0.46276709401709404",
         "6"
        ],
        [
         "3",
         "4",
         "0.32936260847219756",
         "6"
        ],
        [
         "4",
         "5",
         "0.24393671520050822",
         "53"
        ],
        [
         "5",
         "1",
         "0.20491793151537685",
         "10"
        ],
        [
         "6",
         "9",
         "0.20231075394255554",
         "21"
        ],
        [
         "7",
         "7",
         "0.1980094314384433",
         "10"
        ],
        [
         "8",
         "6",
         "0.06447357259418046",
         "8"
        ],
        [
         "9",
         "3",
         "0.007781291470042586",
         "7"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>AP_val</th>\n",
       "      <th>support_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.677859</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.599533</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.462767</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.329363</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.243937</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.204918</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>0.202311</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.198009</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>0.064474</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0.007781</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label    AP_val  support_val\n",
       "0      0  0.677859           14\n",
       "1      8  0.599533           10\n",
       "2      2  0.462767            6\n",
       "3      4  0.329363            6\n",
       "4      5  0.243937           53\n",
       "5      1  0.204918           10\n",
       "6      9  0.202311           21\n",
       "7      7  0.198009           10\n",
       "8      6  0.064474            8\n",
       "9      3  0.007781            7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell B - PR curves + AP per label (val)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "n_labels = Yva.shape[1]\n",
    "ap_per_label = []\n",
    "plt.figure(figsize=(10, 7))\n",
    "for j in range(n_labels):\n",
    "    if Yva[:, j].sum() == 0:\n",
    "        ap_per_label.append(np.nan)\n",
    "        continue\n",
    "    p, r, t = precision_recall_curve(Yva[:, j], proba_va[:, j])\n",
    "    ap = average_precision_score(Yva[:, j], proba_va[:, j])\n",
    "    ap_per_label.append(ap)\n",
    "    plt.plot(r, p, label=f\"L{j} AP={ap:.3f}\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall curves (validation)\")\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc='upper left', fontsize='small', ncol=1)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Table AP per label + support\n",
    "import pandas as pd\n",
    "ap_df = pd.DataFrame({\n",
    "    \"label\": np.arange(n_labels),\n",
    "    \"AP_val\": ap_per_label,\n",
    "    \"support_val\": Yva.sum(axis=0)\n",
    "})\n",
    "display(ap_df.sort_values(\"AP_val\", ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c6484",
   "metadata": {},
   "source": [
    "# Interpr√©tation des courbes PR et de l'Average Precision (AP)\n",
    "\n",
    "## Ce que montre cette visualisation\n",
    "- Chaque courbe Precision‚ÄìRecall (PR) correspond √† une √©tiquette (emotion label).  \n",
    "- L'**Average Precision (AP)** pour une √©tiquette r√©sume la qualit√© du classement (plus l'AP est √©lev√©e, mieux le mod√®le s√©pare les positifs des n√©gatifs pour cette √©tiquette).\n",
    "- Le tableau qui suit liste l‚ÄôAP par √©tiquette ainsi que leur support (nombre d‚Äôexemples positifs dans la validation).\n",
    "\n",
    "## Points cl√©s √† retenir (ce que je dirai √† l'oral)\n",
    "1. **Variabilit√© importante entre les √©tiquettes**  \n",
    "   Certaines √©tiquettes ont un AP √©lev√© (le mod√®le rep√®re bien les exemples), d'autres ont un AP tr√®s bas ‚Äî typiquement les √©tiquettes rares ou dont les features ne discriminent pas bien.\n",
    "\n",
    "2. **Effet du support**  \n",
    "   Les √©tiquettes avec peu d'exemples (`support_val` faible) ont souvent des courbes PR instables et un AP bas. C‚Äôest un signe que nous manquons de donn√©es pour ces classes.\n",
    "\n",
    "3. **Interpr√©ter le trade-off pr√©cision/recall**  \n",
    "   - Si la courbe est tr√®s plate pr√®s du rappel √©lev√© ‚Üí obtenir un bon rappel augmente fortement les faux positifs.  \n",
    "   - Pour la mise en production, on choisira des seuils conservateurs (plancher) pour √©viter un trop grand nombre de faux positifs.\n",
    "\n",
    "4. **Cons√©quences op√©rationnelles et d√©cisions**  \n",
    "   - Si l‚Äôobjectif est **de classer correctement** (d√©cision binaire), on privil√©giera F1 / pr√©cision selon le co√ªt du faux positif vs faux n√©gatif.  \n",
    "   - Si l‚Äôobjectif est **de hi√©rarchiser** (par ex. prioriser la mod√©ration humaine), on privil√©giera l'AP (meilleur pour ranking).\n",
    "\n",
    "5. **Actions correctrices possibles**  \n",
    "   - Calibrer les probabilit√©s (CalibratedClassifierCV) pour obtenir des scores interpr√©tables.  \n",
    "   - Moyennage des seuils via K-fold CV (au lieu d‚Äôun seul split) pour des seuils moins sensibles.  \n",
    "   - Explorer SBERT (embeddings) + logistic/MLP si TF-IDF plafonne.  \n",
    "   - Regrouper ou r√©annoter les √©tiquettes trop rares si n√©cessaire.\n",
    "\n",
    "## Phrase de conclusion courte (oral)\n",
    "> ¬´ Ces courbes montrent que notre mod√®le rep√®re correctement certaines √©motions mais √©prouve des difficult√©s sur d'autres ‚Äî surtout celles peu repr√©sent√©es. La strat√©gie imm√©diate est : 1) calibrer les probabilit√©s, 2) calculer des seuils robustes via CV, et 3) tester SBERT embeddings pour am√©liorer la discrimination sur les √©motions subtiles. ¬ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd722a",
   "metadata": {},
   "source": [
    "Labels avec AP √©lev√© (ex : L0 ‚âà 0.678, L8 ‚âà 0.599, L2 ‚âà 0.463) : le mod√®le sait bien classer ces √©motions ‚Äî c‚Äôest un bon point de d√©part.\n",
    "\n",
    "Labels avec AP faible (ex : L9 ‚âà 0.0077, L6 ‚âà 0.064) : mod√®le ne s√©pare pas positifs/n√©gatifs pour ces labels ‚Äî probablement peu d‚Äôexemples ou features non discriminantes.\n",
    "\n",
    "Les PR-curves : les segments verticaux tr√®s √† gauche signifient qu‚Äôil existe quelques pr√©dictions tr√®s confiantes (precision ‚âà1) mais tr√®s peu d‚Äôexemples (rappel bas). Les courbes lisses et descendantes indiquent un continuum de trade-offs.\n",
    "\n",
    "support_val montre la raret√© : les labels rares (ex. support 6,7,8) ont souvent AP plus faible et PR plus bruyante ‚Äî il faut plus de donn√©es ou regrouper/annoter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2839fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 ‚Äî train 10240 / val 2560\n",
      "Fold 2/5 ‚Äî train 10240 / val 2560\n",
      "Fold 3/5 ‚Äî train 10240 / val 2560\n",
      "Fold 4/5 ‚Äî train 10240 / val 2560\n",
      "Fold 5/5 ‚Äî train 10240 / val 2560\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "thr_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_median",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_mean_floor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "support_train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1972fc79-a21f-4cb3-8ea5-ecf6c1ad070d",
       "rows": [
        [
         "0",
         "9",
         "0.9568872971003811",
         "0.9776775417882",
         "0.046891538953250746",
         "0.9568872971003811",
         "74",
         "21"
        ],
        [
         "1",
         "5",
         "0.9391986367047668",
         "0.9278766258339627",
         "0.037895479365375566",
         "0.9391986367047668",
         "208",
         "53"
        ],
        [
         "2",
         "2",
         "0.9091967990231339",
         "0.9955403848252735",
         "0.13117839623316874",
         "0.9091967990231339",
         "38",
         "6"
        ],
        [
         "3",
         "8",
         "0.8965868724686967",
         "0.9866786547281231",
         "0.12102013649474522",
         "0.8965868724686967",
         "29",
         "10"
        ],
        [
         "4",
         "1",
         "0.8105685382342926",
         "0.8981967862955283",
         "0.25624135598472475",
         "0.8105685382342926",
         "56",
         "10"
        ],
        [
         "5",
         "3",
         "0.7875767517623615",
         "0.9429945340349163",
         "0.25101433005197643",
         "0.7875767517623615",
         "49",
         "7"
        ],
        [
         "6",
         "0",
         "0.7723517140822644",
         "0.9724919377530648",
         "0.3501198668766464",
         "0.7723517140822644",
         "33",
         "14"
        ],
        [
         "7",
         "7",
         "0.701113773029225",
         "0.8585290168205986",
         "0.310860636281838",
         "0.701113773029225",
         "60",
         "10"
        ],
        [
         "8",
         "4",
         "0.6891100290111518",
         "0.6340340445241218",
         "0.2226221420155321",
         "0.6891100290111518",
         "31",
         "6"
        ],
        [
         "9",
         "6",
         "0.5494094309008456",
         "0.6007908089113173",
         "0.37689741316700875",
         "0.5494094309008456",
         "22",
         "8"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>thr_mean</th>\n",
       "      <th>thr_median</th>\n",
       "      <th>thr_std</th>\n",
       "      <th>thr_mean_floor</th>\n",
       "      <th>support_train</th>\n",
       "      <th>support_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.956887</td>\n",
       "      <td>0.977678</td>\n",
       "      <td>0.046892</td>\n",
       "      <td>0.956887</td>\n",
       "      <td>74</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.939199</td>\n",
       "      <td>0.927877</td>\n",
       "      <td>0.037895</td>\n",
       "      <td>0.939199</td>\n",
       "      <td>208</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.909197</td>\n",
       "      <td>0.995540</td>\n",
       "      <td>0.131178</td>\n",
       "      <td>0.909197</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.896587</td>\n",
       "      <td>0.986679</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>0.896587</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.810569</td>\n",
       "      <td>0.898197</td>\n",
       "      <td>0.256241</td>\n",
       "      <td>0.810569</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.787577</td>\n",
       "      <td>0.942995</td>\n",
       "      <td>0.251014</td>\n",
       "      <td>0.787577</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.772352</td>\n",
       "      <td>0.972492</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>0.772352</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.701114</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.310861</td>\n",
       "      <td>0.701114</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.634034</td>\n",
       "      <td>0.222622</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>0.549409</td>\n",
       "      <td>0.600791</td>\n",
       "      <td>0.376897</td>\n",
       "      <td>0.549409</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  thr_mean  thr_median   thr_std  thr_mean_floor  support_train  \\\n",
       "0      9  0.956887    0.977678  0.046892        0.956887             74   \n",
       "1      5  0.939199    0.927877  0.037895        0.939199            208   \n",
       "2      2  0.909197    0.995540  0.131178        0.909197             38   \n",
       "3      8  0.896587    0.986679  0.121020        0.896587             29   \n",
       "4      1  0.810569    0.898197  0.256241        0.810569             56   \n",
       "5      3  0.787577    0.942995  0.251014        0.787577             49   \n",
       "6      0  0.772352    0.972492  0.350120        0.772352             33   \n",
       "7      7  0.701114    0.858529  0.310861        0.701114             60   \n",
       "8      4  0.689110    0.634034  0.222622        0.689110             31   \n",
       "9      6  0.549409    0.600791  0.376897        0.549409             22   \n",
       "\n",
       "   support_val  \n",
       "0           21  \n",
       "1           53  \n",
       "2            6  \n",
       "3           10  \n",
       "4           10  \n",
       "5            7  \n",
       "6           14  \n",
       "7           10  \n",
       "8            6  \n",
       "9            8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "bb33e787-a46a-449e-8d1b-04635cf68e7c",
       "rows": [
        [
         "0",
         "0",
         "14",
         "0.6153846153846154",
         "0.5714285714285714",
         "0.5925925925925926"
        ],
        [
         "1",
         "8",
         "10",
         "0.625",
         "0.5",
         "0.5555555555555556"
        ],
        [
         "2",
         "4",
         "6",
         "0.4",
         "0.3333333333333333",
         "0.36363636363636365"
        ],
        [
         "3",
         "9",
         "21",
         "0.3333333333333333",
         "0.2857142857142857",
         "0.3076923076923077"
        ],
        [
         "4",
         "2",
         "6",
         "0.2857142857142857",
         "0.3333333333333333",
         "0.3076923076923077"
        ],
        [
         "5",
         "5",
         "53",
         "0.24705882352941178",
         "0.39622641509433965",
         "0.30434782608695654"
        ],
        [
         "6",
         "1",
         "10",
         "0.17647058823529413",
         "0.3",
         "0.2222222222222222"
        ],
        [
         "7",
         "7",
         "10",
         "0.10526315789473684",
         "0.2",
         "0.13793103448275862"
        ],
        [
         "8",
         "6",
         "8",
         "0.1111111111111111",
         "0.125",
         "0.11764705882352941"
        ],
        [
         "9",
         "3",
         "7",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>support_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  support_val  precision    recall        f1\n",
       "0      0           14   0.615385  0.571429  0.592593\n",
       "1      8           10   0.625000  0.500000  0.555556\n",
       "2      4            6   0.400000  0.333333  0.363636\n",
       "3      9           21   0.333333  0.285714  0.307692\n",
       "4      2            6   0.285714  0.333333  0.307692\n",
       "5      5           53   0.247059  0.396226  0.304348\n",
       "6      1           10   0.176471  0.300000  0.222222\n",
       "7      7           10   0.105263  0.200000  0.137931\n",
       "8      6            8   0.111111  0.125000  0.117647\n",
       "9      3            7   0.000000  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/emo_thr_mean_floor.joblib , artifacts/thresholds_cv_summary.csv, artifacts/metrics_with_cvthr.csv\n"
     ]
    }
   ],
   "source": [
    "# === K-FOLD THRESHOLD FINDER ===\n",
    "# Attention : re-entraine le mod√®le K fois -> moderate compute but deterministic for robustness.\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# param√®tres\n",
    "K = 5\n",
    "RANDOM = 42\n",
    "MIN_THR = 0.05   # floor √† tester (change si besoin)\n",
    "base_C = emo_clf.estimator.C if hasattr(emo_clf, \"estimator\") else 1.0\n",
    "\n",
    "# stratify key : nombre de labels positifs par √©chantillon (pour multi-label)\n",
    "strat_key = np.sum(Ytr, axis=1)\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=RANDOM)\n",
    "\n",
    "thr_folds = []\n",
    "fold = 0\n",
    "for train_idx, val_idx in skf.split(Xtr_s, strat_key):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}/{K} ‚Äî train {len(train_idx)} / val {len(val_idx)}\")\n",
    "    X_train_cv, X_val_cv = Xtr_s[train_idx], Xtr_s[val_idx]\n",
    "    Y_train_cv, Y_val_cv = Ytr[train_idx], Ytr[val_idx]\n",
    "\n",
    "    # fit same family as emo_clf\n",
    "    clf_cv = OneVsRestClassifier(LogisticRegression(max_iter=3000, class_weight=\"balanced\",\n",
    "                                                    solver=\"lbfgs\", C=base_C))\n",
    "    clf_cv.fit(X_train_cv, Y_train_cv)\n",
    "\n",
    "    proba_valcv = clf_cv.predict_proba(X_val_cv)\n",
    "    thr_cv = np.full(Y_train_cv.shape[1], 0.5, dtype=float)\n",
    "\n",
    "    for j in range(Y_train_cv.shape[1]):\n",
    "        if Y_val_cv[:, j].sum() == 0:\n",
    "            thr_cv[j] = np.nan  # pas d'exemples dans ce fold pour cette √©tiquette\n",
    "            continue\n",
    "        p, r, t = precision_recall_curve(Y_val_cv[:, j], proba_valcv[:, j])\n",
    "        f1 = (2*p*r) / (p + r + 1e-12)\n",
    "        i = int(np.nanargmax(f1))\n",
    "        # t has length len(p)-1 typically; safe guard\n",
    "        if i < len(t):\n",
    "            thr_cv[j] = float(t[i])\n",
    "        else:\n",
    "            thr_cv[j] = float(t[-1]) if len(t)>0 else 0.5\n",
    "\n",
    "    thr_folds.append(thr_cv)\n",
    "\n",
    "# stack and compute statistics\n",
    "thr_matrix = np.vstack(thr_folds)  # shape (K, n_labels)\n",
    "thr_mean = np.nanmean(thr_matrix, axis=0)\n",
    "thr_median = np.nanmedian(thr_matrix, axis=0)\n",
    "thr_std = np.nanstd(thr_matrix, axis=0)\n",
    "\n",
    "# apply floor to avoid near-zero thresholds\n",
    "thr_mean_floor = np.maximum(thr_mean, MIN_THR)\n",
    "\n",
    "# Show summary dataframe (first columns)\n",
    "summary_df = pd.DataFrame({\n",
    "    \"label\": np.arange(len(thr_mean)),\n",
    "    \"thr_mean\": thr_mean,\n",
    "    \"thr_median\": thr_median,\n",
    "    \"thr_std\": thr_std,\n",
    "    \"thr_mean_floor\": thr_mean_floor,\n",
    "    \"support_train\": np.array(Ytr.sum(axis=0), dtype=int),\n",
    "    \"support_val\": np.array(Yva.sum(axis=0), dtype=int)\n",
    "})\n",
    "pd.options.display.max_rows = 999\n",
    "display(summary_df.sort_values(\"thr_mean\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Optional: quickly evaluate using emo_clf on validation with thr_mean_floor\n",
    "proba_va_current = proba_va  # ton proba_va d√©j√† calcul√©e pour le best model\n",
    "yhat_va_cvthr = (proba_va_current >= thr_mean_floor.reshape(1, -1)).astype(int)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "per_label = []\n",
    "for j in range(Yva.shape[1]):\n",
    "    sup = int(Yva[:, j].sum())\n",
    "    if sup == 0:\n",
    "        per_label.append({\"label\": j, \"support_val\": sup, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan})\n",
    "        continue\n",
    "    p = precision_score(Yva[:, j], yhat_va_cvthr[:, j], zero_division=0)\n",
    "    r = recall_score(Yva[:, j], yhat_va_cvthr[:, j], zero_division=0)\n",
    "    f1 = f1_score(Yva[:, j], yhat_va_cvthr[:, j], zero_division=0)\n",
    "    per_label.append({\"label\": j, \"support_val\": sup, \"precision\": p, \"recall\": r, \"f1\": f1})\n",
    "\n",
    "per_label_df = pd.DataFrame(per_label).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "display(per_label_df)\n",
    "\n",
    "# save artifacts\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(thr_mean_floor, \"artifacts/emo_thr_mean_floor.joblib\")\n",
    "summary_df.to_csv(\"artifacts/thresholds_cv_summary.csv\", index=False)\n",
    "per_label_df.to_csv(\"artifacts/metrics_with_cvthr.csv\", index=False)\n",
    "\n",
    "print(\"Saved: artifacts/emo_thr_mean_floor.joblib , artifacts/thresholds_cv_summary.csv, artifacts/metrics_with_cvthr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044c487",
   "metadata": {},
   "source": [
    "# Pourquoi calculer des seuils par K-fold ?\n",
    "- Les seuils optimaux trouv√©s sur un seul split sont sensibles au hasard du split, surtout pour les classes rares.  \n",
    "- En moyennant les seuils obtenus sur plusieurs folds, on obtient une estimation plus robuste et moins sujette au sur-apprentissage sur la validation.  \n",
    "- On ajoute un plancher (MIN_THR) pour √©viter des seuils quasi-nuls qui entra√Æneraient des faux positifs massifs en production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d87040",
   "metadata": {},
   "source": [
    "Lecture rapide des r√©sultats\n",
    "\n",
    "Les seuils moyens (thr_mean) sont majoritairement tr√®s √©lev√©s (0.55 ‚Üí 0.95).\n",
    "‚Üí Cela signifie que le mod√®le n‚Äôattribue des probabilit√©s √©lev√©es qu‚Äô√† tr√®s peu d‚Äôexemples ; il est donc tr√®s confident seulement sur certains cas.\n",
    "\n",
    "Le support est tr√®s in√©gal : une √©tiquette a 208 exemples (bonne), plusieurs en ont entre 20‚Äì70, et d‚Äôautres seulement 6‚Äì10.\n",
    "‚Üí Les √©tiquettes rares expliquent en grande partie les faibles performances et la variance √©lev√©e.\n",
    "\n",
    "Le tableau m√©triques (precision/recall/f1 avec les seuils CV+floor) montre :\n",
    "\n",
    "quelques labels avec F1 correctes (‚âà0.6, 0.55),\n",
    "\n",
    "la majorit√© des labels ont F1 faibles (<0.35), certains proches de 0.\n",
    "‚Üí Globalement la performance de d√©cision binaire reste faible pour plusieurs √©motions.\n",
    "\n",
    "Conclusion : le mod√®le a un pouvoir de ranking variable (certaines AP √©lev√©es vues avant), mais pour la d√©cision binaire (seuils) on obtient des F1 modestes. La prochaine √©tape logique est la calibration des probabilit√©s (sigmoid) pour rendre les scores plus interpr√©tables et potentiellement am√©liorer la s√©lection de seuils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6e85536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.7.2\n",
      "Utilisation de C = 0.25\n",
      "D√©but du fit du calibrateur OneVsRest (cela r√©-entraine et calibre chaque sous-classifieur)...\n",
      "Mean AP after calibration: 0.3191\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AP_cal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d86f3a79-9c5f-40db-bdbb-7912c91f5457",
       "rows": [
        [
         "0",
         "0",
         "0.678078992898422",
         "14"
        ],
        [
         "1",
         "8",
         "0.6204513904423864",
         "10"
        ],
        [
         "2",
         "2",
         "0.5195286195286195",
         "6"
        ],
        [
         "3",
         "4",
         "0.3134836666390064",
         "6"
        ],
        [
         "4",
         "5",
         "0.2733414308542173",
         "53"
        ],
        [
         "5",
         "1",
         "0.2656811074604281",
         "10"
        ],
        [
         "6",
         "7",
         "0.23228685957574496",
         "10"
        ],
        [
         "7",
         "9",
         "0.21748040378473527",
         "21"
        ],
        [
         "8",
         "6",
         "0.05705019178238641",
         "8"
        ],
        [
         "9",
         "3",
         "0.013751625119606755",
         "7"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>AP_cal</th>\n",
       "      <th>support_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.620451</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.519529</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.313484</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.273341</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.265681</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.232287</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.217480</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>0.057050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label    AP_cal  support_val\n",
       "0      0  0.678079           14\n",
       "1      8  0.620451           10\n",
       "2      2  0.519529            6\n",
       "3      4  0.313484            6\n",
       "4      5  0.273341           53\n",
       "5      1  0.265681           10\n",
       "6      7  0.232287           10\n",
       "7      9  0.217480           21\n",
       "8      6  0.057050            8\n",
       "9      3  0.013752            7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated OneVsRest saved -> artifacts/emo_calibrated_ovr_sigmoid.joblib\n",
      "Calibration termin√©e en 5.7s\n"
     ]
    }
   ],
   "source": [
    "# Calibration robuste (adapt√©e aux diff√©rences de versions de scikit-learn)\n",
    "# ATTENTION : recalibrer refit le mod√®le et peut √™tre long selon nb_labels / taille dataset.\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sklearn, time, joblib, numpy as np, os\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "t0 = time.time()\n",
    "\n",
    "# r√©cup√©rer C depuis emo_clf si possible\n",
    "base_C = 1.0\n",
    "if 'emo_clf' in globals():\n",
    "    try:\n",
    "        base_C = emo_clf.estimator.C\n",
    "    except Exception:\n",
    "        try:\n",
    "            # si OneVsRestClassifier wrapping, r√©cup√©rer le premier estimator\n",
    "            base_C = emo_clf.estimators_[0].C\n",
    "        except Exception:\n",
    "            base_C = 1.0\n",
    "print(\"Utilisation de C =\", base_C)\n",
    "\n",
    "# cr√©e l'estimateur de base\n",
    "lr_base = LogisticRegression(max_iter=3000, class_weight=\"balanced\", solver=\"lbfgs\", C=base_C)\n",
    "\n",
    "# instantiate CalibratedClassifierCV with a safe API wrapper (essayons plusieurs signatures)\n",
    "calibrator = None\n",
    "err = None\n",
    "for try_case in (\n",
    "    {\"kw\": {\"estimator\": lr_base, \"method\": \"sigmoid\", \"cv\": 3}},  # sklearn >= ~0.24 uses `estimator=`\n",
    "    {\"kw\": {\"base_estimator\": lr_base, \"method\": \"sigmoid\", \"cv\": 3}},  # older sklearn used base_estimator\n",
    "    {\"pos\": (lr_base, ), \"kw\": {\"method\": \"sigmoid\", \"cv\": 3}},  # estimator as positional arg\n",
    "):\n",
    "    try:\n",
    "        if \"pos\" in try_case:\n",
    "            calibrator = CalibratedClassifierCV(*try_case[\"pos\"], **try_case[\"kw\"])\n",
    "        else:\n",
    "            calibrator = CalibratedClassifierCV(**try_case[\"kw\"])\n",
    "        # quick sanity instantiate to ensure no immediate TypeError\n",
    "        break\n",
    "    except TypeError as e:\n",
    "        err = e\n",
    "        calibrator = None\n",
    "        continue\n",
    "\n",
    "if calibrator is None:\n",
    "    raise RuntimeError(f\"Impossible d'instancier CalibratedClassifierCV avec la version sklearn={sklearn.__version__}. Derni√®re erreur: {err}\")\n",
    "\n",
    "# Wrap the calibrated estimator in OneVsRestClassifier\n",
    "calib_wrapper = OneVsRestClassifier(calibrator)\n",
    "\n",
    "print(\"D√©but du fit du calibrateur OneVsRest (cela r√©-entraine et calibre chaque sous-classifieur)...\")\n",
    "calib_wrapper.fit(Xtr_s, Ytr)\n",
    "\n",
    "# probabilities sur la validation apr√®s calibration\n",
    "proba_va_cal = calib_wrapper.predict_proba(Xva_s)\n",
    "\n",
    "# calculer l'AP par label et la moyenne\n",
    "aps_cal = []\n",
    "for j in range(Yva.shape[1]):\n",
    "    if Yva[:, j].sum() == 0:\n",
    "        aps_cal.append(np.nan)\n",
    "    else:\n",
    "        aps_cal.append(average_precision_score(Yva[:, j], proba_va_cal[:, j]))\n",
    "mean_ap_cal = float(np.nanmean(aps_cal))\n",
    "print(f\"Mean AP after calibration: {mean_ap_cal:.4f}\")\n",
    "\n",
    "# mettre les r√©sultats dans un DataFrame pour affichage (optionnel)\n",
    "import pandas as pd\n",
    "ap_df_cal = pd.DataFrame({\n",
    "    \"label\": np.arange(len(aps_cal)),\n",
    "    \"AP_cal\": aps_cal,\n",
    "    \"support_val\": Yva.sum(axis=0)\n",
    "}).sort_values(\"AP_cal\", ascending=False).reset_index(drop=True)\n",
    "display(ap_df_cal)\n",
    "\n",
    "# sauvegarde du calibrateur (optionnel)\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(calib_wrapper, \"artifacts/emo_calibrated_ovr_sigmoid.joblib\")\n",
    "print(\"Calibrated OneVsRest saved -> artifacts/emo_calibrated_ovr_sigmoid.joblib\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Calibration termin√©e en {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "# expose la variable proba_va_cal pour r√©utilisation dans cellules suivantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b71b8a",
   "metadata": {},
   "source": [
    "# R√©sum√© interpr√©tatif ‚Äî effet de la calibration et objectifs du K-fold\n",
    "\n",
    "- Apr√®s calibration, la **Mean AP** est pass√©e d‚Äôenviron **0.299 ‚Üí 0.319** : c‚Äôest un gain modeste mais concret.  \n",
    "  Cela montre que les probabilit√©s sont d√©sormais mieux ordonn√©es pour le *ranking* (utile si on veut prioriser / trier).\n",
    "- Malgr√© ce gain, certaines √©tiquettes restent faibles (ex. label 9) : la cause est principalement le **faible support** (peu d'exemples) et/ou un signal discriminant faible dans les features actuelles.\n",
    "- Les seuils optimaux trouv√©s sur un seul split √©taient tr√®s variables. L‚Äôobjectif du **K-fold thresholds** est de :\n",
    "  - **estimer des seuils plus robustes** en moyennant les seuils trouv√©s sur plusieurs folds (r√©duction de variance due au split),\n",
    "  - **√©viter** qu‚Äôun seuil tr√®s petit ou tr√®s grand (qui serait sur-sp√©cifique au split) soit appliqu√© en production.\n",
    "- On appliquera ces seuils moyens (avec un plancher `MIN_THR`) sur les probabilit√©s **calibr√©es** (si disponible) ‚Äî combinaison qui tend √† donner des d√©cisions binaires plus stables.\n",
    "- √Ä l‚Äôissue du K-fold, on comparera les m√©triques binaires (precision/recall/f1) calcul√©es avec ces seuils √† l‚Äô√©tat pr√©c√©dent et on d√©cidera : garder ce pipeline LR (avec thresholds CV + calibration) ou passer √† SBERT / mod√®le plus puissant.\n",
    "\n",
    "Phrase courte √† dire √† l‚Äôoral :  \n",
    "¬´ La calibration a am√©lior√© le pouvoir de classement du mod√®le. Pour rendre les d√©cisions binaires robustes en production, nous allons maintenant calculer des seuils en moyennant ceux obtenus sur plusieurs folds ‚Äî puis appliquer ces seuils sur les probabilit√©s calibr√©es et comparer les m√©triques. ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee05fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.7.2\n",
      "Using base C = 0.25\n",
      "Will evaluate final metrics using proba_va_cal (probabilities from calibrated model on full train).\n",
      "\n",
      "--- Fold 1/5 --- train 10240 / val 2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 2/5 --- train 10240 / val 2560\n",
      "\n",
      "--- Fold 3/5 --- train 10240 / val 2560\n",
      "\n",
      "--- Fold 4/5 --- train 10240 / val 2560\n",
      "\n",
      "--- Fold 5/5 --- train 10240 / val 2560\n",
      "\n",
      "Thresholds CV summary (sorted by thr_mean desc):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "thr_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_median",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_mean_floor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "support_train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cb09443f-9a57-407d-9649-8686e7b9b155",
       "rows": [
        [
         "0",
         "0",
         "0.37045774144962473",
         "0.35464236468522486",
         "0.21457751353735474",
         "0.37045774144962473",
         "33",
         "14"
        ],
        [
         "1",
         "2",
         "0.3408294831043728",
         "0.43654946501336583",
         "0.17643482938144517",
         "0.3408294831043728",
         "38",
         "6"
        ],
        [
         "2",
         "8",
         "0.17013795029785264",
         "0.15814998066538935",
         "0.12604396512380678",
         "0.17013795029785264",
         "29",
         "10"
        ],
        [
         "3",
         "5",
         "0.1290191277713864",
         "0.12536388690178754",
         "0.030115102792014282",
         "0.1290191277713864",
         "208",
         "53"
        ],
        [
         "4",
         "1",
         "0.11761993842455987",
         "0.10606842640808639",
         "0.061799975413882234",
         "0.11761993842455987",
         "56",
         "10"
        ],
        [
         "5",
         "4",
         "0.11467695685996974",
         "0.08997171995540611",
         "0.051361148956854195",
         "0.11467695685996974",
         "31",
         "6"
        ],
        [
         "6",
         "3",
         "0.10197930354560943",
         "0.11723564382210627",
         "0.030135616813590112",
         "0.10197930354560943",
         "49",
         "7"
        ],
        [
         "7",
         "7",
         "0.07765969354491994",
         "0.07113738708850768",
         "0.031002925651382175",
         "0.07765969354491994",
         "60",
         "10"
        ],
        [
         "8",
         "9",
         "0.055760031364378126",
         "0.047870284253347245",
         "0.01690685706500768",
         "0.055760031364378126",
         "74",
         "21"
        ],
        [
         "9",
         "6",
         "0.022803418133585933",
         "0.01895788015978795",
         "0.015519287292942002",
         "0.05",
         "22",
         "8"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>thr_mean</th>\n",
       "      <th>thr_median</th>\n",
       "      <th>thr_std</th>\n",
       "      <th>thr_mean_floor</th>\n",
       "      <th>support_train</th>\n",
       "      <th>support_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.370458</td>\n",
       "      <td>0.354642</td>\n",
       "      <td>0.214578</td>\n",
       "      <td>0.370458</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.340829</td>\n",
       "      <td>0.436549</td>\n",
       "      <td>0.176435</td>\n",
       "      <td>0.340829</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.170138</td>\n",
       "      <td>0.158150</td>\n",
       "      <td>0.126044</td>\n",
       "      <td>0.170138</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.129019</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>0.030115</td>\n",
       "      <td>0.129019</td>\n",
       "      <td>208</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.117620</td>\n",
       "      <td>0.106068</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.117620</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.114677</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.051361</td>\n",
       "      <td>0.114677</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0.101979</td>\n",
       "      <td>0.117236</td>\n",
       "      <td>0.030136</td>\n",
       "      <td>0.101979</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.077660</td>\n",
       "      <td>0.071137</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.077660</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.055760</td>\n",
       "      <td>0.047870</td>\n",
       "      <td>0.016907</td>\n",
       "      <td>0.055760</td>\n",
       "      <td>74</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>0.022803</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  thr_mean  thr_median   thr_std  thr_mean_floor  support_train  \\\n",
       "0      0  0.370458    0.354642  0.214578        0.370458             33   \n",
       "1      2  0.340829    0.436549  0.176435        0.340829             38   \n",
       "2      8  0.170138    0.158150  0.126044        0.170138             29   \n",
       "3      5  0.129019    0.125364  0.030115        0.129019            208   \n",
       "4      1  0.117620    0.106068  0.061800        0.117620             56   \n",
       "5      4  0.114677    0.089972  0.051361        0.114677             31   \n",
       "6      3  0.101979    0.117236  0.030136        0.101979             49   \n",
       "7      7  0.077660    0.071137  0.031003        0.077660             60   \n",
       "8      9  0.055760    0.047870  0.016907        0.055760             74   \n",
       "9      6  0.022803    0.018958  0.015519        0.050000             22   \n",
       "\n",
       "   support_val  \n",
       "0           14  \n",
       "1            6  \n",
       "2           10  \n",
       "3           53  \n",
       "4           10  \n",
       "5            6  \n",
       "6            7  \n",
       "7           10  \n",
       "8           21  \n",
       "9            8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics on validation using CV-mean thresholds (with floor):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e8d6ba25-c579-4917-b43c-5bea8f706671",
       "rows": [
        [
         "0",
         "0",
         "14",
         "0.8571428571428571",
         "0.42857142857142855",
         "0.5714285714285714"
        ],
        [
         "1",
         "5",
         "53",
         "0.36363636363636365",
         "0.37735849056603776",
         "0.37037037037037035"
        ],
        [
         "2",
         "2",
         "6",
         "0.4",
         "0.3333333333333333",
         "0.36363636363636365"
        ],
        [
         "3",
         "8",
         "10",
         "0.6666666666666666",
         "0.2",
         "0.3076923076923077"
        ],
        [
         "4",
         "1",
         "10",
         "0.3",
         "0.3",
         "0.3"
        ],
        [
         "5",
         "4",
         "6",
         "0.5",
         "0.16666666666666666",
         "0.25"
        ],
        [
         "6",
         "7",
         "10",
         "0.2857142857142857",
         "0.2",
         "0.23529411764705882"
        ],
        [
         "7",
         "9",
         "21",
         "0.2727272727272727",
         "0.14285714285714285",
         "0.1875"
        ],
        [
         "8",
         "3",
         "7",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "6",
         "8",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>support_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  support_val  precision    recall        f1\n",
       "0      0           14   0.857143  0.428571  0.571429\n",
       "1      5           53   0.363636  0.377358  0.370370\n",
       "2      2            6   0.400000  0.333333  0.363636\n",
       "3      8           10   0.666667  0.200000  0.307692\n",
       "4      1           10   0.300000  0.300000  0.300000\n",
       "5      4            6   0.500000  0.166667  0.250000\n",
       "6      7           10   0.285714  0.200000  0.235294\n",
       "7      9           21   0.272727  0.142857  0.187500\n",
       "8      3            7   0.000000  0.000000  0.000000\n",
       "9      6            8   0.000000  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro-F1 (avg per label) = 0.2586\n",
      "Micro precision/recall/f1 = 0.3645 / 0.2690 / 0.3095\n",
      "\n",
      "Saved artifacts to artifacts/: emo_thr_mean_floor_from_calib_folds.joblib, thresholds_cv_summary_from_calib_folds.csv, metrics_with_cvthr_from_calib_folds.csv\n",
      "\n",
      "K-fold thresholds completed in 16.3s\n"
     ]
    }
   ],
   "source": [
    "# === K-FOLD THRESHOLD FINDER (robuste, avec calibration par fold) ===\n",
    "# IMPORTANT: co√ªteux si dataset / labels nombreux, mais robuste.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib, os, time, sklearn\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "\n",
    "# param√®tres\n",
    "K = 5\n",
    "RANDOM = 42\n",
    "MIN_THR = 0.05   # floor par d√©faut ; ajuste si besoin\n",
    "inner_cv_for_calibrator = 3  # CV interne pour CalibratedClassifierCV\n",
    "base_C = 0.25\n",
    "if 'emo_clf' in globals():\n",
    "    try:\n",
    "        base_C = emo_clf.estimator.C\n",
    "    except Exception:\n",
    "        try:\n",
    "            base_C = emo_clf.estimators_[0].C\n",
    "        except Exception:\n",
    "            base_C = base_C\n",
    "print(\"Using base C =\", base_C)\n",
    "\n",
    "# fallback si pas de proba calibr√©e globale disponible\n",
    "if 'proba_va_cal' in globals():\n",
    "    global_proba_to_eval = proba_va_cal\n",
    "    print(\"Will evaluate final metrics using proba_va_cal (probabilities from calibrated model on full train).\")\n",
    "elif 'proba_va' in globals():\n",
    "    global_proba_to_eval = proba_va\n",
    "    print(\"proba_va_cal not found ‚Äî will evaluate using proba_va (non-calibrated).\")\n",
    "else:\n",
    "    raise RuntimeError(\"proba_va_cal et proba_va absents. G√©n√®re d'abord les probabilit√©s sur validation.\")\n",
    "\n",
    "# stratify key: number of positive labels per sample (multi-label)\n",
    "strat_key = np.sum(Ytr, axis=1)\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=RANDOM)\n",
    "\n",
    "thr_folds = []\n",
    "t0 = time.time()\n",
    "fold = 0\n",
    "\n",
    "for train_idx, val_idx in skf.split(Xtr_s, strat_key):\n",
    "    fold += 1\n",
    "    print(f\"\\n--- Fold {fold}/{K} --- train {len(train_idx)} / val {len(val_idx)}\")\n",
    "    X_train_cv, X_val_cv = Xtr_s[train_idx], Xtr_s[val_idx]\n",
    "    Y_train_cv, Y_val_cv = Ytr[train_idx], Ytr[val_idx]\n",
    "\n",
    "    # on cr√©e un LR de base\n",
    "    lr = LogisticRegression(max_iter=3000, class_weight=\"balanced\", solver=\"lbfgs\", C=base_C)\n",
    "\n",
    "    # instantiation safe de CalibratedClassifierCV (gestion versions sklearn)\n",
    "    calibrator = None\n",
    "    last_err = None\n",
    "    for try_case in (\n",
    "        {\"kw\": {\"estimator\": lr, \"method\": \"sigmoid\", \"cv\": inner_cv_for_calibrator}},\n",
    "        {\"kw\": {\"base_estimator\": lr, \"method\": \"sigmoid\", \"cv\": inner_cv_for_calibrator}},\n",
    "        {\"pos\": (lr,), \"kw\": {\"method\": \"sigmoid\", \"cv\": inner_cv_for_calibrator}},\n",
    "    ):\n",
    "        try:\n",
    "            if \"pos\" in try_case:\n",
    "                calibrator = CalibratedClassifierCV(*try_case[\"pos\"], **try_case[\"kw\"])\n",
    "            else:\n",
    "                calibrator = CalibratedClassifierCV(**try_case[\"kw\"])\n",
    "            break\n",
    "        except TypeError as e:\n",
    "            last_err = e\n",
    "            calibrator = None\n",
    "            continue\n",
    "    if calibrator is None:\n",
    "        raise RuntimeError(f\"Impossible d'instancier CalibratedClassifierCV avec sklearn={sklearn.__version__}. Erreur: {last_err}\")\n",
    "\n",
    "    # wrap in OneVsRest and fit on train_cv (cela entra√Æne + calibre)\n",
    "    clf_fold = OneVsRestClassifier(calibrator)\n",
    "    clf_fold.fit(X_train_cv, Y_train_cv)\n",
    "\n",
    "    # probabilit√©s sur la val du fold\n",
    "    proba_valcv = clf_fold.predict_proba(X_val_cv)\n",
    "\n",
    "    # compute best-threshold per label on this fold\n",
    "    n_labels = Y_train_cv.shape[1]\n",
    "    thr_cv = np.full(n_labels, np.nan, dtype=float)\n",
    "    for j in range(n_labels):\n",
    "        if Y_val_cv[:, j].sum() == 0:\n",
    "            thr_cv[j] = np.nan\n",
    "            continue\n",
    "        p, r, t = precision_recall_curve(Y_val_cv[:, j], proba_valcv[:, j])\n",
    "        f1 = (2 * p * r) / (p + r + 1e-12)\n",
    "        i = int(np.nanargmax(f1))\n",
    "        # t length can be len(p)-1, but use safe guard\n",
    "        if len(t) > 0:\n",
    "            # if i is last index beyond t, clamp\n",
    "            idx = min(i, len(t)-1)\n",
    "            thr_cv[j] = float(t[idx])\n",
    "        else:\n",
    "            thr_cv[j] = 0.5\n",
    "    thr_folds.append(thr_cv)\n",
    "    # optional: free memory\n",
    "    del clf_fold\n",
    "\n",
    "# aggregate thresholds\n",
    "thr_matrix = np.vstack(thr_folds)  # shape (K, n_labels)\n",
    "thr_mean = np.nanmean(thr_matrix, axis=0)\n",
    "thr_median = np.nanmedian(thr_matrix, axis=0)\n",
    "thr_std = np.nanstd(thr_matrix, axis=0)\n",
    "thr_mean_floor = np.maximum(thr_mean, MIN_THR)\n",
    "\n",
    "# summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    \"label\": np.arange(len(thr_mean)),\n",
    "    \"thr_mean\": thr_mean,\n",
    "    \"thr_median\": thr_median,\n",
    "    \"thr_std\": thr_std,\n",
    "    \"thr_mean_floor\": thr_mean_floor,\n",
    "    \"support_train\": np.array(Ytr.sum(axis=0), dtype=int),\n",
    "    \"support_val\": np.array(Yva.sum(axis=0), dtype=int)\n",
    "})\n",
    "pd.options.display.max_rows = 999\n",
    "print(\"\\nThresholds CV summary (sorted by thr_mean desc):\")\n",
    "display(summary_df.sort_values(\"thr_mean\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Evaluate on the global validation probabilities (calibrated if available)\n",
    "proba_eval = global_proba_to_eval\n",
    "yhat_va_thr = (proba_eval >= thr_mean_floor.reshape(1, -1)).astype(int)\n",
    "\n",
    "per_label = []\n",
    "for j in range(Yva.shape[1]):\n",
    "    sup = int(Yva[:, j].sum())\n",
    "    if sup == 0:\n",
    "        per_label.append({\"label\": j, \"support_val\": sup, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan})\n",
    "        continue\n",
    "    p = precision_score(Yva[:, j], yhat_va_thr[:, j], zero_division=0)\n",
    "    r = recall_score(Yva[:, j], yhat_va_thr[:, j], zero_division=0)\n",
    "    f1 = f1_score(Yva[:, j], yhat_va_thr[:, j], zero_division=0)\n",
    "    per_label.append({\"label\": j, \"support_val\": sup, \"precision\": p, \"recall\": r, \"f1\": f1})\n",
    "\n",
    "per_label_df = pd.DataFrame(per_label).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nMetrics on validation using CV-mean thresholds (with floor):\")\n",
    "display(per_label_df)\n",
    "\n",
    "# global metrics\n",
    "macro_f1 = per_label_df['f1'].dropna().mean()\n",
    "micro_precision = precision_score(Yva.flatten(), yhat_va_thr.flatten(), zero_division=0)\n",
    "micro_recall = recall_score(Yva.flatten(), yhat_va_thr.flatten(), zero_division=0)\n",
    "micro_f1 = f1_score(Yva.flatten(), yhat_va_thr.flatten(), zero_division=0)\n",
    "\n",
    "print(f\"\\nMacro-F1 (avg per label) = {macro_f1:.4f}\")\n",
    "print(f\"Micro precision/recall/f1 = {micro_precision:.4f} / {micro_recall:.4f} / {micro_f1:.4f}\")\n",
    "\n",
    "# save artifacts\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(thr_mean_floor, \"artifacts/emo_thr_mean_floor_from_calib_folds.joblib\")\n",
    "summary_df.to_csv(\"artifacts/thresholds_cv_summary_from_calib_folds.csv\", index=False)\n",
    "per_label_df.to_csv(\"artifacts/metrics_with_cvthr_from_calib_folds.csv\", index=False)\n",
    "print(\"\\nSaved artifacts to artifacts/: emo_thr_mean_floor_from_calib_folds.joblib, thresholds_cv_summary_from_calib_folds.csv, metrics_with_cvthr_from_calib_folds.csv\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"\\nK-fold thresholds completed in {:.1f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea26c7",
   "metadata": {},
   "source": [
    "# R√©sum√© et interpr√©tation ‚Äî Seuils K-fold & m√©triques\n",
    "\n",
    "## R√©sum√© rapide des r√©sultats\n",
    "- Nous avons calcul√© des **seuils optimaux par √©tiquette** sur K-folds (K=5) puis pris la **moyenne** (avec un plancher `MIN_THR = 0.05` appliqu√©).  \n",
    "- Les seuils moyens (`thr_mean`) sont d√©sormais dans une fourchette raisonnable (ex. ~0.37, 0.34, 0.17, 0.13, ...), et le plancher a √©vit√© des seuils quasi-nuls pour les labels tr√®s rares.  \n",
    "- √âvaluation binaire (appliqu√©e sur les probabilit√©s calibr√©es si disponible) avec ces seuils moyens donne :\n",
    "  - **Macro-F1 (moyenne des F1 par label)** ‚âà **0.2586**  \n",
    "  - **Micro precision / recall / f1** ‚âà **0.3645 / 0.2690 / 0.3095**\n",
    "\n",
    "## Ce que signifient ces chiffres (interpr√©tation)\n",
    "- **Seuils plus robustes** : en moyennant sur plusieurs folds on r√©duit la variance li√©e au split unique ; les seuils extr√™mes trouv√©s auparavant (‚âà0.001 ou ‚âà0.99) ont √©t√© liss√©s.  \n",
    "- **Pr√©cision souvent bonne, rappel faible** pour plusieurs labels : cela signifie que le mod√®le est *conservateur* ‚Äî quand il pr√©dit la classe il a souvent raison (pr√©cision √©lev√©e sur certains labels), mais il manque beaucoup de positifs (rappel bas). Exemple concret : pour l‚Äô√©tiquette 0 on a une pr√©cision √©lev√©e (~0.86) mais rappel ~0.43 ‚Üí F1 ‚âà 0.57.  \n",
    "- **Performance globale encore modeste** : macro-F1 ‚âà 0.26 montre que la performance moyenne par √©tiquette reste faible ‚Äî surtout sur labels rares. Micro-F1 ‚âà 0.31 refl√®te la difficult√© globale sur toutes les √©tiquettes combin√©es.\n",
    "\n",
    "## Causes probables\n",
    "- **D√©s√©quilibre fort des √©tiquettes** (certaines √©tiquettes ont tr√®s peu d‚Äôexemples).  \n",
    "- **Repr√©sentation textuelle actuelle** (TF-IDF) limite la capture des indices s√©mantiques pour √©motions subtiles.  \n",
    "- **Mod√®le lin√©aire** (LR) peut plafonner sur distinctions plus fines / non lin√©aires.\n",
    "\n",
    "## Recommandations et plan d‚Äôaction (imm√©diat)\n",
    "1. **Comparer avant / apr√®s** : garder les m√©triques pr√©c√©dentes et celles obtenues maintenant pour prouver l‚Äôam√©lioration (ou non) induite par calibration + thresholds CV.  \n",
    "2. **Tester plusieurs planchers (`MIN_THR`)** : essayer 0.01 / 0.05 / 0.10 et observer l‚Äôimpact sur recall vs precision ; choisir selon le co√ªt des faux positifs.  \n",
    "3. **Si besoin de meilleure discrimination rapidement** : tester **SBERT embeddings + LR/MLP** (souvent gain rapide sans fine-tuning co√ªteux).  \n",
    "4. **Pour labels tr√®s rares** : regrouper √©tiquettes proches s√©mantiquement, ou traiter-les comme ¬´ suggestions √† r√©viser ¬ª (human-in-loop) plut√¥t que les automatiser.  \n",
    "5. **Si on veut aller plus loin** : essayer ClassifierChain ou LightGBM sur embeddings + features, puis recalibration + seuils CV.  \n",
    "6. **Conserver et versionner les artefacts** : `artifacts/thresholds_cv_summary_from_calib_folds.csv` et `emo_thr_mean_floor_from_calib_folds.joblib` sont sauvegard√©s ‚Äî utiles pour l‚Äôannexe et la reproductibilit√©.\n",
    "\n",
    "## Message court pour l‚Äôoral\n",
    "> ¬´ Nous avons stabilis√© les seuils en moyennant ceux obtenus par K-fold sur des probabilit√©s calibr√©es. Les seuils sont maintenant plus fiables (moins sensibles au split). La d√©cision binaire est encore perfectible (macro-F1 ‚âà 0.26), surtout sur les classes rares. Les prochaines actions sont : tester SBERT pour obtenir de meilleures repr√©sentations, explorer plusieurs planchers de seuils et √©ventuellement regrouper/annoter davantage les √©tiquettes rares. ¬ª\n",
    "\n",
    "## Conclusion\n",
    "- Le K-fold a **am√©lior√© la robustesse** des seuils et √©vit√© des cas extr√™mes.  \n",
    "- Reste : am√©liorer la **repr√©sentation** (embeddings) et/ou **collecter/agr√©ger** des exemples pour les labels rares afin d‚Äôaugmenter le rappel et la F1 moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc8c2dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using proba_va_cal (calibrated probabilities) for evaluation.\n",
      "\n",
      "=== MIN_THR = 0.01 ===\n",
      "\n",
      "Macro-F1 (avg per label) = 0.0536\n",
      "Micro precision/recall/f1 = 1.0000 / 0.0207 / 0.0405\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "152184d9-e6c1-447f-bf52-2cb857b76fd3",
       "rows": [
        [
         "0",
         "2",
         "6",
         "1.0",
         "0.16666666666666666",
         "0.2857142857142857"
        ],
        [
         "1",
         "0",
         "14",
         "1.0",
         "0.14285714285714285",
         "0.25"
        ],
        [
         "2",
         "1",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "3",
         "7",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "4",
         "6",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "5",
         "53",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "6",
         "8",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "7",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "8",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "9",
         "21",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>support_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  support_val  precision    recall        f1\n",
       "0      2            6        1.0  0.166667  0.285714\n",
       "1      0           14        1.0  0.142857  0.250000\n",
       "2      1           10        0.0  0.000000  0.000000\n",
       "3      3            7        0.0  0.000000  0.000000\n",
       "4      4            6        0.0  0.000000  0.000000\n",
       "5      5           53        0.0  0.000000  0.000000\n",
       "6      6            8        0.0  0.000000  0.000000\n",
       "7      7           10        0.0  0.000000  0.000000\n",
       "8      8           10        0.0  0.000000  0.000000\n",
       "9      9           21        0.0  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MIN_THR = 0.05 ===\n",
      "\n",
      "Macro-F1 (avg per label) = 0.0536\n",
      "Micro precision/recall/f1 = 1.0000 / 0.0207 / 0.0405\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e687871b-f2b8-4945-8ba6-77567ca03bfa",
       "rows": [
        [
         "0",
         "2",
         "6",
         "1.0",
         "0.16666666666666666",
         "0.2857142857142857"
        ],
        [
         "1",
         "0",
         "14",
         "1.0",
         "0.14285714285714285",
         "0.25"
        ],
        [
         "2",
         "1",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "3",
         "7",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "4",
         "6",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "5",
         "53",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "6",
         "8",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "7",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "8",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "9",
         "21",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>support_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  support_val  precision    recall        f1\n",
       "0      2            6        1.0  0.166667  0.285714\n",
       "1      0           14        1.0  0.142857  0.250000\n",
       "2      1           10        0.0  0.000000  0.000000\n",
       "3      3            7        0.0  0.000000  0.000000\n",
       "4      4            6        0.0  0.000000  0.000000\n",
       "5      5           53        0.0  0.000000  0.000000\n",
       "6      6            8        0.0  0.000000  0.000000\n",
       "7      7           10        0.0  0.000000  0.000000\n",
       "8      8           10        0.0  0.000000  0.000000\n",
       "9      9           21        0.0  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MIN_THR = 0.1 ===\n",
      "\n",
      "Macro-F1 (avg per label) = 0.0536\n",
      "Micro precision/recall/f1 = 1.0000 / 0.0207 / 0.0405\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7592f9ec-7f4a-4d86-aac9-0b7a80ff769e",
       "rows": [
        [
         "0",
         "2",
         "6",
         "1.0",
         "0.16666666666666666",
         "0.2857142857142857"
        ],
        [
         "1",
         "0",
         "14",
         "1.0",
         "0.14285714285714285",
         "0.25"
        ],
        [
         "2",
         "1",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "3",
         "7",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "4",
         "6",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "5",
         "53",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "6",
         "8",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "7",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "8",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "9",
         "21",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>support_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  support_val  precision    recall        f1\n",
       "0      2            6        1.0  0.166667  0.285714\n",
       "1      0           14        1.0  0.142857  0.250000\n",
       "2      1           10        0.0  0.000000  0.000000\n",
       "3      3            7        0.0  0.000000  0.000000\n",
       "4      4            6        0.0  0.000000  0.000000\n",
       "5      5           53        0.0  0.000000  0.000000\n",
       "6      6            8        0.0  0.000000  0.000000\n",
       "7      7           10        0.0  0.000000  0.000000\n",
       "8      8           10        0.0  0.000000  0.000000\n",
       "9      9           21        0.0  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== R√©sum√© comparatif ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "min_thr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macro_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "micro_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "micro_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "micro_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_threshold_used",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "599a1722-aa45-46a5-8923-6ef6c7d50b62",
       "rows": [
        [
         "0",
         "0.01",
         "0.05357142857142857",
         "1.0",
         "0.020689655172413793",
         "0.04054054054054054",
         "0.8011999842317119"
        ],
        [
         "1",
         "0.05",
         "0.05357142857142857",
         "1.0",
         "0.020689655172413793",
         "0.04054054054054054",
         "0.8011999842317119"
        ],
        [
         "2",
         "0.1",
         "0.05357142857142857",
         "1.0",
         "0.020689655172413793",
         "0.04054054054054054",
         "0.8011999842317119"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_thr</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>avg_threshold_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02069</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.8012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02069</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.8012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02069</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.8012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_thr  macro_f1  micro_precision  micro_recall  micro_f1  \\\n",
       "0     0.01  0.053571              1.0       0.02069  0.040541   \n",
       "1     0.05  0.053571              1.0       0.02069  0.040541   \n",
       "2     0.10  0.053571              1.0       0.02069  0.040541   \n",
       "\n",
       "   avg_threshold_used  \n",
       "0              0.8012  \n",
       "1              0.8012  \n",
       "2              0.8012  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved artifacts: artifacts/min_thr_comparison.csv, artifacts/per_label_results_by_min_thr.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cell: tester automatiquement plusieurs MIN_THR et afficher l'impact sur macro/micro F1\n",
    "# Elle teste MIN_THR = [0.01, 0.05, 0.10] (modifiable), affiche les m√©triques globales et les tableaux per-label.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Configurable ---\n",
    "min_thr_list = [0.01, 0.05, 0.10]     # valeurs test√©es\n",
    "show_per_label_tables = True          # affiche le per_label_df pour chaque min_thr\n",
    "plot_results = True                   # affiche un petit graphique comparatif (matplotlib)\n",
    "# --------------------\n",
    "\n",
    "# 1) retrouver la base des thresholds issus du K-fold\n",
    "if 'thr_mean' in globals():\n",
    "    base_thr = np.array(thr_mean, dtype=float)\n",
    "elif 'summary_df' in globals() and 'thr_mean' in summary_df.columns:\n",
    "    base_thr = np.array(summary_df['thr_mean'].values, dtype=float)\n",
    "elif 'thr_mean_floor' in globals():\n",
    "    # fallback to floored thresholds (we'll remove the floor effect)\n",
    "    base_thr = np.array(thr_mean_floor, dtype=float)\n",
    "elif 'thr' in globals():\n",
    "    base_thr = np.array(thr, dtype=float)\n",
    "else:\n",
    "    raise RuntimeError(\"Impossible de trouver 'thr_mean'/'summary_df'/'thr_mean_floor'/'thr' dans l'environnement. Ex√©cute d'abord la cellule K-fold ou d√©finis 'thr'.\")\n",
    "\n",
    "# replace NaN by default (0.5) to avoid NaN propagation when applying floor\n",
    "base_thr = np.where(np.isnan(base_thr), 0.5, base_thr)\n",
    "\n",
    "# 2) choisir quelles probabilit√©s utiliser (pr√©f√©rer calibr√©es si dispo)\n",
    "if 'proba_va_cal' in globals():\n",
    "    proba_eval = proba_va_cal\n",
    "    print(\"Using proba_va_cal (calibrated probabilities) for evaluation.\")\n",
    "elif 'proba_va' in globals():\n",
    "    proba_eval = proba_va\n",
    "    print(\"proba_va_cal not found ‚Äî using proba_va (non-calibrated probabilities).\")\n",
    "else:\n",
    "    raise RuntimeError(\"Aucune variable 'proba_va_cal' ou 'proba_va' trouv√©e. G√©n√®re d'abord les probabilit√©s sur validation.\")\n",
    "\n",
    "results = []\n",
    "per_label_results = {}  # store per-label dfs keyed by min_thr\n",
    "\n",
    "for min_thr in min_thr_list:\n",
    "    thr_used = np.maximum(base_thr, min_thr)           # apply floor\n",
    "    yhat_va = (proba_eval >= thr_used.reshape(1, -1)).astype(int)\n",
    "\n",
    "    # compute per-label metrics\n",
    "    per_label = []\n",
    "    for j in range(Yva.shape[1]):\n",
    "        sup = int(Yva[:, j].sum())\n",
    "        if sup == 0:\n",
    "            per_label.append({\"label\": j, \"support_val\": sup, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan})\n",
    "            continue\n",
    "        p = precision_score(Yva[:, j], yhat_va[:, j], zero_division=0)\n",
    "        r = recall_score(Yva[:, j], yhat_va[:, j], zero_division=0)\n",
    "        f1 = f1_score(Yva[:, j], yhat_va[:, j], zero_division=0)\n",
    "        per_label.append({\"label\": j, \"support_val\": sup, \"precision\": p, \"recall\": r, \"f1\": f1})\n",
    "\n",
    "    per_label_df = pd.DataFrame(per_label).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "    per_label_results[min_thr] = per_label_df\n",
    "\n",
    "    # global metrics\n",
    "    macro_f1 = per_label_df['f1'].dropna().mean()\n",
    "    micro_precision = precision_score(Yva.flatten(), yhat_va.flatten(), zero_division=0)\n",
    "    micro_recall = recall_score(Yva.flatten(), yhat_va.flatten(), zero_division=0)\n",
    "    micro_f1 = f1_score(Yva.flatten(), yhat_va.flatten(), zero_division=0)\n",
    "\n",
    "    results.append({\n",
    "        \"min_thr\": min_thr,\n",
    "        \"macro_f1\": float(macro_f1),\n",
    "        \"micro_precision\": float(micro_precision),\n",
    "        \"micro_recall\": float(micro_recall),\n",
    "        \"micro_f1\": float(micro_f1),\n",
    "        \"avg_threshold_used\": float(np.nanmean(thr_used))\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== MIN_THR =\", min_thr, \"===\\n\")\n",
    "    print(\"Macro-F1 (avg per label) = {:.4f}\".format(macro_f1))\n",
    "    print(\"Micro precision/recall/f1 = {:.4f} / {:.4f} / {:.4f}\".format(micro_precision, micro_recall, micro_f1))\n",
    "    if show_per_label_tables:\n",
    "        display(per_label_df)\n",
    "\n",
    "# summary table\n",
    "summary_results = pd.DataFrame(results).sort_values(\"min_thr\").reset_index(drop=True)\n",
    "print(\"\\n=== R√©sum√© comparatif ===\")\n",
    "display(summary_results)\n",
    "\n",
    "# optional small plot\n",
    "if plot_results:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        ax[0].plot(summary_results['min_thr'], summary_results['macro_f1'], marker='o', label='macro_f1')\n",
    "        ax[0].plot(summary_results['min_thr'], summary_results['micro_f1'], marker='o', label='micro_f1')\n",
    "        ax[0].set_xlabel(\"MIN_THR\")\n",
    "        ax[0].set_title(\"Macro & Micro F1 vs MIN_THR\")\n",
    "        ax[0].legend()\n",
    "        ax[0].grid(True)\n",
    "\n",
    "        ax[1].bar(summary_results['min_thr'].astype(str), summary_results['micro_precision'])\n",
    "        ax[1].set_title(\"Micro precision vs MIN_THR\")\n",
    "        ax[1].set_xlabel(\"MIN_THR\")\n",
    "        ax[1].grid(axis='y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Plot failed:\", e)\n",
    "\n",
    "# Save summary artifacts for traceability\n",
    "import os, joblib\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "summary_results.to_csv(\"artifacts/min_thr_comparison.csv\", index=False)\n",
    "joblib.dump(per_label_results, \"artifacts/per_label_results_by_min_thr.joblib\")\n",
    "print(\"\\nSaved artifacts: artifacts/min_thr_comparison.csv, artifacts/per_label_results_by_min_thr.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17b9fc",
   "metadata": {},
   "source": [
    "# D√©finition\n",
    "Le **threshold** (seuil) transforme la **probabilit√©** fournie par un mod√®le en une **d√©cision binaire**.  \n",
    "Si `proba >= threshold` ‚Üí on pr√©dit la classe (oui).  \n",
    "Si `proba < threshold`  ‚Üí on ne la pr√©dit pas (non)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef3a81",
   "metadata": {},
   "source": [
    "# Analogie simple\n",
    "Imagine des candidatures not√©es 0‚Äì100 :\n",
    "- Seuil = 90 ‚Üí on n‚Äôengage que les tr√®s bons (peu d‚Äôembauches, tr√®s peu d‚Äôerreurs) ‚Üí **pr√©cision √©lev√©e**.  \n",
    "- Seuil = 50 ‚Üí on engage plus de monde (plus d‚Äôembauches, plus d‚Äôerreurs) ‚Üí **rappel √©lev√©**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ab078",
   "metadata": {},
   "source": [
    "# Effet sur les m√©triques\n",
    "- **Augmenter le seuil** ‚Üí ‚Üë pr√©cision, ‚Üì rappel.  \n",
    "- **Baisser le seuil** ‚Üí ‚Üì pr√©cision, ‚Üë rappel.  \n",
    "- **AP (Average Precision)** mesure la qualit√© du classement sans fixer de seuil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb61df",
   "metadata": {},
   "source": [
    "# Choisir un seuil ‚Äî guide pratique\n",
    "1. Si l'objectif est **de classer/prioriser** ‚Üí utiliser AP / courbe PR (pas forc√©ment de seuil unique).  \n",
    "2. Si l'objectif est **d√©cision binaire** ‚Üí choisir un seuil sur la validation (ex. maximise F1 ou garantit pr√©cision minimale).  \n",
    "3. Pour du multi-label ‚Üí calculer un **seuil par √©tiquette**.  \n",
    "4. Pour la robustesse ‚Üí estimer seuils par **K-fold** (prendre moyenne ou m√©diane).  \n",
    "5. En production ‚Üí appliquer un **plancher MIN_THR** (ex. 0.01 / 0.05 / 0.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f4e4e",
   "metadata": {},
   "source": [
    "# M√©thodes courantes pour obtenir un seuil\n",
    "- `precision_recall_curve` ‚Üí choisir le seuil qui maximise F1 (ou autre crit√®re).  \n",
    "- Moyennage des seuils obtenus sur plusieurs folds (K-fold).  \n",
    "- Calibration (`CalibratedClassifierCV`) avant de fixer le seuil pour rendre les probabilit√©s plus fiables.  \n",
    "- Tester plusieurs `MIN_THR` et comparer macro/micro F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c4889",
   "metadata": {},
   "source": [
    "# Checklist courte pour l‚Äôoral\n",
    "- ¬´ Le threshold transforme une probabilit√© en d√©cision oui/non. ¬ª  \n",
    "- ¬´ C‚Äôest un compromis entre pr√©cision et rappel. ¬ª  \n",
    "- ¬´ On choisit le seuil sur la validation, on le rend robuste par K-fold et calibration. ¬ª  \n",
    "- ¬´ En production, on impose souvent un plancher pour √©viter des seuils quasi-nuls. ¬ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d2ab6",
   "metadata": {},
   "source": [
    "# Conclusion (1 phrase)\n",
    "Le threshold d√©cide quand une probabilit√© devient une action ‚Äî le r√©gler, c‚Äôest choisir l‚Äô√©quilibre qui convient entre rater des vrais positifs et limiter les faux positifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0205f2",
   "metadata": {},
   "source": [
    "R√©utiliser sentence-transformers pour am√©liorer la labellisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60c1fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts: True ['artifacts\\\\absa_auto_ensemble_conservative.csv', 'artifacts\\\\absa_auto_ensemble_conservative_disagreements.csv', 'artifacts\\\\absa_auto_ensemble_disagreements.csv', 'artifacts\\\\absa_auto_ensemble_labels.csv', 'artifacts\\\\absa_auto_ensemble_ml_disagreements.csv', 'artifacts\\\\absa_auto_ensemble_ml_labels.csv', 'artifacts\\\\absa_auto_mapping.joblib', 'artifacts\\\\absa_candidates_embeddings.joblib', 'artifacts\\\\absa_candidates_embs_reduced_labels.joblib', 'artifacts\\\\absa_candidates_umap_embs_labels.joblib', 'artifacts\\\\absa_manual_qc_sample.csv', 'artifacts\\\\absa_reviews_with_auto_aspects.csv', 'artifacts\\\\emo_calibrated_ovr_sigmoid.joblib', 'artifacts\\\\emo_thr_mean_floor.joblib', 'artifacts\\\\emo_thr_mean_floor_from_calib_folds.joblib', 'artifacts\\\\metrics_with_cvthr.csv', 'artifacts\\\\metrics_with_cvthr_from_calib_folds.csv', 'artifacts\\\\min_thr_comparison.csv', 'artifacts\\\\per_label_results_by_min_thr.joblib', 'artifacts\\\\thresholds_cv_summary.csv', 'artifacts\\\\thresholds_cv_summary_from_calib_folds.csv']\n",
      "Found candidates: {'emo_clf': True, 'clf_emo': False, 'emo_pipeline_final': False, 'Xte_s': True, 'Yte': False}\n",
      "emo_clf type: OneVsRestClassifier\n",
      "thr length: 10\n"
     ]
    }
   ],
   "source": [
    "# === Diagnostic Emotions: cherche emo_clf, Xte_s, Yte, thr, etc. ===\n",
    "import os, glob, pandas as pd\n",
    "print(\"artifacts:\", os.path.exists(\"artifacts\"), sorted(glob.glob(\"artifacts/*\"))[:50])\n",
    "vars_try = ['emo_clf','clf_emo','emo_pipeline_final','Xte_s','Yte','Yte']\n",
    "found = {v: (v in globals() or os.path.exists(f\"artifacts/{v}.joblib\")) for v in vars_try}\n",
    "print(\"Found candidates:\", found)\n",
    "if 'emo_clf' in globals():\n",
    "    print(\"emo_clf type:\", type(globals()['emo_clf']).__name__)\n",
    "if 'thr' in globals():\n",
    "    print(\"thr length:\", len(thr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16e29f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold thresholds: K = 5 | n_labels = 10\n",
      "base_C used for LR: 0.25\n",
      "\n",
      "Fold 1/5 ‚Äî train 10240 / val 2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/5 ‚Äî train 10240 / val 2560\n",
      "\n",
      "Fold 3/5 ‚Äî train 10240 / val 2560\n",
      "\n",
      "Fold 4/5 ‚Äî train 10240 / val 2560\n",
      "\n",
      "Fold 5/5 ‚Äî train 10240 / val 2560\n",
      "\n",
      "Summary thresholds (top):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "thr_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_median",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_mean_floor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "support_train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ce3feb80-deb0-4fdd-b30f-9b7ba335e749",
       "rows": [
        [
         "0",
         "9",
         "0.9568872971003811",
         "0.9776775417882",
         "0.046891538953250746",
         "0.9568872971003811",
         "74",
         "21"
        ],
        [
         "1",
         "5",
         "0.9391986367047668",
         "0.9278766258339627",
         "0.037895479365375566",
         "0.9391986367047668",
         "208",
         "53"
        ],
        [
         "2",
         "2",
         "0.9091967990231339",
         "0.9955403848252735",
         "0.13117839623316874",
         "0.9091967990231339",
         "38",
         "6"
        ],
        [
         "3",
         "8",
         "0.8965868724686967",
         "0.9866786547281231",
         "0.12102013649474522",
         "0.8965868724686967",
         "29",
         "10"
        ],
        [
         "4",
         "1",
         "0.8105685382342926",
         "0.8981967862955283",
         "0.25624135598472475",
         "0.8105685382342926",
         "56",
         "10"
        ],
        [
         "5",
         "3",
         "0.7875767517623615",
         "0.9429945340349163",
         "0.25101433005197643",
         "0.7875767517623615",
         "49",
         "7"
        ],
        [
         "6",
         "0",
         "0.7723517140822644",
         "0.9724919377530648",
         "0.3501198668766464",
         "0.7723517140822644",
         "33",
         "14"
        ],
        [
         "7",
         "7",
         "0.701113773029225",
         "0.8585290168205986",
         "0.310860636281838",
         "0.701113773029225",
         "60",
         "10"
        ],
        [
         "8",
         "4",
         "0.6891100290111518",
         "0.6340340445241218",
         "0.2226221420155321",
         "0.6891100290111518",
         "31",
         "6"
        ],
        [
         "9",
         "6",
         "0.5494094309008456",
         "0.6007908089113173",
         "0.37689741316700875",
         "0.5494094309008456",
         "22",
         "8"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>thr_mean</th>\n",
       "      <th>thr_median</th>\n",
       "      <th>thr_std</th>\n",
       "      <th>thr_mean_floor</th>\n",
       "      <th>support_train</th>\n",
       "      <th>support_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.956887</td>\n",
       "      <td>0.977678</td>\n",
       "      <td>0.046892</td>\n",
       "      <td>0.956887</td>\n",
       "      <td>74</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.939199</td>\n",
       "      <td>0.927877</td>\n",
       "      <td>0.037895</td>\n",
       "      <td>0.939199</td>\n",
       "      <td>208</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.909197</td>\n",
       "      <td>0.995540</td>\n",
       "      <td>0.131178</td>\n",
       "      <td>0.909197</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.896587</td>\n",
       "      <td>0.986679</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>0.896587</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.810569</td>\n",
       "      <td>0.898197</td>\n",
       "      <td>0.256241</td>\n",
       "      <td>0.810569</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.787577</td>\n",
       "      <td>0.942995</td>\n",
       "      <td>0.251014</td>\n",
       "      <td>0.787577</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.772352</td>\n",
       "      <td>0.972492</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>0.772352</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.701114</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.310861</td>\n",
       "      <td>0.701114</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.634034</td>\n",
       "      <td>0.222622</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>0.549409</td>\n",
       "      <td>0.600791</td>\n",
       "      <td>0.376897</td>\n",
       "      <td>0.549409</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  thr_mean  thr_median   thr_std  thr_mean_floor  support_train  \\\n",
       "0      9  0.956887    0.977678  0.046892        0.956887             74   \n",
       "1      5  0.939199    0.927877  0.037895        0.939199            208   \n",
       "2      2  0.909197    0.995540  0.131178        0.909197             38   \n",
       "3      8  0.896587    0.986679  0.121020        0.896587             29   \n",
       "4      1  0.810569    0.898197  0.256241        0.810569             56   \n",
       "5      3  0.787577    0.942995  0.251014        0.787577             49   \n",
       "6      0  0.772352    0.972492  0.350120        0.772352             33   \n",
       "7      7  0.701114    0.858529  0.310861        0.701114             60   \n",
       "8      4  0.689110    0.634034  0.222622        0.689110             31   \n",
       "9      6  0.549409    0.600791  0.376897        0.549409             22   \n",
       "\n",
       "   support_val  \n",
       "0           21  \n",
       "1           53  \n",
       "2            6  \n",
       "3           10  \n",
       "4           10  \n",
       "5            7  \n",
       "6           14  \n",
       "7           10  \n",
       "8            6  \n",
       "9            8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick eval on validation with CV thresholds (thr_mean_floor):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support_val",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "da3d36f9-571a-4230-83fd-ce1d1c8ab385",
       "rows": [
        [
         "0",
         "0",
         "14",
         "0.6153846153846154",
         "0.5714285714285714",
         "0.5925925925925926"
        ],
        [
         "1",
         "8",
         "10",
         "0.625",
         "0.5",
         "0.5555555555555556"
        ],
        [
         "2",
         "4",
         "6",
         "0.4",
         "0.3333333333333333",
         "0.36363636363636365"
        ],
        [
         "3",
         "9",
         "21",
         "0.3333333333333333",
         "0.2857142857142857",
         "0.3076923076923077"
        ],
        [
         "4",
         "2",
         "6",
         "0.2857142857142857",
         "0.3333333333333333",
         "0.3076923076923077"
        ],
        [
         "5",
         "5",
         "53",
         "0.24705882352941178",
         "0.39622641509433965",
         "0.30434782608695654"
        ],
        [
         "6",
         "1",
         "10",
         "0.17647058823529413",
         "0.3",
         "0.2222222222222222"
        ],
        [
         "7",
         "7",
         "10",
         "0.10526315789473684",
         "0.2",
         "0.13793103448275862"
        ],
        [
         "8",
         "6",
         "8",
         "0.1111111111111111",
         "0.125",
         "0.11764705882352941"
        ],
        [
         "9",
         "3",
         "7",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>support_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  support_val  precision    recall        f1\n",
       "0      0           14   0.615385  0.571429  0.592593\n",
       "1      8           10   0.625000  0.500000  0.555556\n",
       "2      4            6   0.400000  0.333333  0.363636\n",
       "3      9           21   0.333333  0.285714  0.307692\n",
       "4      2            6   0.285714  0.333333  0.307692\n",
       "5      5           53   0.247059  0.396226  0.304348\n",
       "6      1           10   0.176471  0.300000  0.222222\n",
       "7      7           10   0.105263  0.200000  0.137931\n",
       "8      6            8   0.111111  0.125000  0.117647\n",
       "9      3            7   0.000000  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: artifacts/emo_thr_mean_floor.joblib , artifacts/thresholds_cv_summary.csv , artifacts/metrics_with_cvthr.csv\n"
     ]
    }
   ],
   "source": [
    "# === K-FOLD THRESHOLD FINDER (adapt√© & robuste) ===\n",
    "import os, joblib, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Param√®tres (modifie si tu veux)\n",
    "K = 5\n",
    "RANDOM = 42\n",
    "MIN_THR = 0.05   # floor minimal pour thresholds\n",
    "VERBOSE = True\n",
    "\n",
    "# --- Validation des inputs (variables attendues dans l'environnement) ---\n",
    "emo_clf_var = globals().get('emo_clf', globals().get('clf_emo', None))\n",
    "Xtr = globals().get('Xtr_s', globals().get('Xtr', None))\n",
    "Ytr = globals().get('Ytr', globals().get('ytr', None))\n",
    "Xva = globals().get('Xva_s', globals().get('Xva', None))\n",
    "Yva = globals().get('Yva', globals().get('yva', None))\n",
    "proba_va = globals().get('proba_va', None)  # peut exister si d√©j√† calcul√©e\n",
    "\n",
    "if emo_clf_var is None:\n",
    "    raise RuntimeError(\"emo_clf introuvable. Charge/nomme ton classifieur en 'emo_clf' ou 'clf_emo' avant d'ex√©cuter.\")\n",
    "\n",
    "if Xtr is None or Ytr is None:\n",
    "    raise RuntimeError(\"Xtr_s / Ytr introuvables. Assure-toi d'avoir Xtr_s (ou Xtr) et Ytr dans l'environnement.\")\n",
    "\n",
    "n_labels = int(Ytr.shape[1])\n",
    "if VERBOSE:\n",
    "    print(\"K-fold thresholds: K =\", K, \"| n_labels =\", n_labels)\n",
    "\n",
    "# detect base_C from current emo_clf if possible (pour r√©-entrainer la m√™me famille)\n",
    "try:\n",
    "    base_C = getattr(emo_clf_var, \"estimator\", None)\n",
    "    if base_C is not None and hasattr(emo_clf_var.estimator, \"C\"):\n",
    "        base_C = emo_clf_var.estimator.C\n",
    "    else:\n",
    "        base_C = 1.0\n",
    "except Exception:\n",
    "    base_C = 1.0\n",
    "if VERBOSE: print(\"base_C used for LR:\", base_C)\n",
    "\n",
    "# Stratify key (nombre d'√©tiquettes positives par √©chantillon) pour stratified folds\n",
    "strat_key = np.sum(Ytr, axis=1)\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=RANDOM)\n",
    "\n",
    "thr_folds = []\n",
    "fold_no = 0\n",
    "for train_idx, val_idx in skf.split(Xtr, strat_key):\n",
    "    fold_no += 1\n",
    "    print(f\"\\nFold {fold_no}/{K} ‚Äî train {len(train_idx)} / val {len(val_idx)}\")\n",
    "    X_train_cv, X_val_cv = Xtr[train_idx], Xtr[val_idx]\n",
    "    Y_train_cv, Y_val_cv = Ytr[train_idx], Ytr[val_idx]\n",
    "\n",
    "    # R√©entra√Æner la m√™me famille (OneVsRest + LogisticRegression)\n",
    "    clf_cv = OneVsRestClassifier(\n",
    "        LogisticRegression(max_iter=3000, class_weight=\"balanced\", solver=\"lbfgs\", C=base_C)\n",
    "    )\n",
    "    clf_cv.fit(X_train_cv, Y_train_cv)\n",
    "\n",
    "    # Obtenir proba sur val\n",
    "    try:\n",
    "        proba_valcv = clf_cv.predict_proba(X_val_cv)\n",
    "    except Exception:\n",
    "        # si predict_proba renvoie liste de arrays par sous-classifieur -> convertir\n",
    "        preds_list = clf_cv.predict_proba(X_val_cv)\n",
    "        if isinstance(preds_list, (list, tuple)):\n",
    "            proba_valcv = np.vstack([p[:,1] if p.ndim==2 else p for p in preds_list]).T\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # trouver thresholds par label (max F1 sur PR curve)\n",
    "    thr_cv = np.full(n_labels, np.nan, dtype=float)\n",
    "    for j in range(n_labels):\n",
    "        if Y_val_cv[:, j].sum() == 0:\n",
    "            # pas d'exemples positifs pour cette √©tiquette dans ce fold\n",
    "            thr_cv[j] = np.nan\n",
    "            continue\n",
    "        p, r, t = precision_recall_curve(Y_val_cv[:, j], proba_valcv[:, j])\n",
    "        f1 = (2 * p * r) / (p + r + 1e-12)\n",
    "        i_best = int(np.nanargmax(f1))\n",
    "        # safeguard: t length peut √™tre len(p)-1\n",
    "        if i_best < len(t):\n",
    "            thr_cv[j] = float(t[i_best])\n",
    "        else:\n",
    "            thr_cv[j] = float(t[-1]) if len(t) > 0 else 0.5\n",
    "\n",
    "    thr_folds.append(thr_cv)\n",
    "\n",
    "# stack et stats\n",
    "thr_matrix = np.vstack(thr_folds)  # (K, n_labels)\n",
    "thr_mean = np.nanmean(thr_matrix, axis=0)\n",
    "thr_median = np.nanmedian(thr_matrix, axis=0)\n",
    "thr_std = np.nanstd(thr_matrix, axis=0)\n",
    "\n",
    "# floor minimal\n",
    "thr_mean_floor = np.maximum(thr_mean, MIN_THR)\n",
    "\n",
    "# construire le r√©sum√©\n",
    "summary_df = pd.DataFrame({\n",
    "    \"label\": np.arange(n_labels),\n",
    "    \"thr_mean\": thr_mean,\n",
    "    \"thr_median\": thr_median,\n",
    "    \"thr_std\": thr_std,\n",
    "    \"thr_mean_floor\": thr_mean_floor,\n",
    "    \"support_train\": np.array(Ytr.sum(axis=0), dtype=int),\n",
    "    \"support_val\": np.array(Yva.sum(axis=0), dtype=int) if Yva is not None else np.nan\n",
    "})\n",
    "pd.options.display.max_rows = 999\n",
    "print(\"\\nSummary thresholds (top):\")\n",
    "display(summary_df.sort_values(\"thr_mean\", ascending=False).reset_index(drop=True).head(50))\n",
    "\n",
    "# --- Eval rapide sur validation en utilisant thr_mean_floor (si proba_va existe sinon calcule)\n",
    "if proba_va is None:\n",
    "    # tente de calculer proba_va depuis emo_clf (si Xva disponible)\n",
    "    if Xva is not None:\n",
    "        try:\n",
    "            proba_va_current = emo_clf_var.predict_proba(Xva)\n",
    "        except Exception:\n",
    "            preds_list = emo_clf_var.predict_proba(Xva)\n",
    "            if isinstance(preds_list, (list, tuple)):\n",
    "                proba_va_current = np.vstack([p[:,1] if p.ndim==2 else p for p in preds_list]).T\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        proba_va_current = None\n",
    "else:\n",
    "    proba_va_current = proba_va\n",
    "\n",
    "if proba_va_current is not None and Yva is not None:\n",
    "    print(\"\\nQuick eval on validation with CV thresholds (thr_mean_floor):\")\n",
    "    yhat_va_cvthr = (proba_va_current >= thr_mean_floor.reshape(1, -1)).astype(int)\n",
    "    per_label = []\n",
    "    for j in range(n_labels):\n",
    "        sup = int(Yva[:, j].sum())\n",
    "        if sup == 0:\n",
    "            per_label.append({\"label\": j, \"support_val\": sup, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan})\n",
    "            continue\n",
    "        p = precision_score(Yva[:, j], yhat_va_cvthr[:, j], zero_division=0)\n",
    "        r = recall_score(Yva[:, j], yhat_va_cvthr[:, j], zero_division=0)\n",
    "        f1 = f1_score(Yva[:, j], yhat_va_cvthr[:, j], zero_division=0)\n",
    "        per_label.append({\"label\": j, \"support_val\": sup, \"precision\": p, \"recall\": r, \"f1\": f1})\n",
    "    per_label_df = pd.DataFrame(per_label).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "    display(per_label_df.head(50))\n",
    "else:\n",
    "    print(\"Pas d'√©valuation rapide possible : proba_va et/ou Yva manquants.\")\n",
    "\n",
    "# --- sauvegardes\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(thr_mean_floor, \"artifacts/emo_thr_mean_floor.joblib\")\n",
    "summary_df.to_csv(\"artifacts/thresholds_cv_summary.csv\", index=False)\n",
    "if 'per_label_df' in locals():\n",
    "    per_label_df.to_csv(\"artifacts/metrics_with_cvthr.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved: artifacts/emo_thr_mean_floor.joblib , artifacts/thresholds_cv_summary.csv\",\n",
    "      (\", artifacts/metrics_with_cvthr.csv\" if 'per_label_df' in locals() else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c60631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_texts = 1314720\n",
      "Loading SBERT on device: cuda model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded batch 1/5136  samples 0-255  elapsed 0.6s\n",
      "Encoded batch 11/5136  samples 2560-2815  elapsed 4.7s\n",
      "Encoded batch 21/5136  samples 5120-5375  elapsed 8.6s\n",
      "Encoded batch 31/5136  samples 7680-7935  elapsed 12.7s\n",
      "Encoded batch 41/5136  samples 10240-10495  elapsed 16.7s\n",
      "Encoded batch 51/5136  samples 12800-13055  elapsed 20.7s\n",
      "Encoded batch 61/5136  samples 15360-15615  elapsed 24.7s\n",
      "Encoded batch 71/5136  samples 17920-18175  elapsed 28.7s\n",
      "Encoded batch 81/5136  samples 20480-20735  elapsed 32.8s\n",
      "Encoded batch 91/5136  samples 23040-23295  elapsed 36.9s\n",
      "Encoded batch 101/5136  samples 25600-25855  elapsed 40.9s\n",
      "Encoded batch 111/5136  samples 28160-28415  elapsed 45.0s\n",
      "Encoded batch 121/5136  samples 30720-30975  elapsed 49.0s\n",
      "Encoded batch 131/5136  samples 33280-33535  elapsed 53.1s\n",
      "Encoded batch 141/5136  samples 35840-36095  elapsed 57.2s\n",
      "Encoded batch 151/5136  samples 38400-38655  elapsed 61.3s\n",
      "Encoded batch 161/5136  samples 40960-41215  elapsed 65.4s\n",
      "Encoded batch 171/5136  samples 43520-43775  elapsed 69.3s\n",
      "Encoded batch 181/5136  samples 46080-46335  elapsed 73.6s\n",
      "Encoded batch 191/5136  samples 48640-48895  elapsed 78.5s\n",
      "Encoded batch 201/5136  samples 51200-51455  elapsed 83.4s\n",
      "Encoded batch 211/5136  samples 53760-54015  elapsed 88.3s\n",
      "Encoded batch 221/5136  samples 56320-56575  elapsed 93.2s\n",
      "Encoded batch 231/5136  samples 58880-59135  elapsed 97.9s\n",
      "Encoded batch 241/5136  samples 61440-61695  elapsed 102.8s\n",
      "Encoded batch 251/5136  samples 64000-64255  elapsed 107.7s\n",
      "Encoded batch 261/5136  samples 66560-66815  elapsed 112.6s\n",
      "Encoded batch 271/5136  samples 69120-69375  elapsed 117.5s\n",
      "Encoded batch 281/5136  samples 71680-71935  elapsed 122.4s\n",
      "Encoded batch 291/5136  samples 74240-74495  elapsed 127.3s\n",
      "Encoded batch 301/5136  samples 76800-77055  elapsed 132.2s\n",
      "Encoded batch 311/5136  samples 79360-79615  elapsed 136.6s\n",
      "Encoded batch 321/5136  samples 81920-82175  elapsed 141.5s\n",
      "Encoded batch 331/5136  samples 84480-84735  elapsed 146.5s\n",
      "Encoded batch 341/5136  samples 87040-87295  elapsed 151.3s\n",
      "Encoded batch 351/5136  samples 89600-89855  elapsed 156.2s\n",
      "Encoded batch 361/5136  samples 92160-92415  elapsed 161.0s\n",
      "Encoded batch 371/5136  samples 94720-94975  elapsed 165.9s\n",
      "Encoded batch 381/5136  samples 97280-97535  elapsed 170.8s\n",
      "Encoded batch 391/5136  samples 99840-100095  elapsed 175.7s\n",
      "Encoded batch 401/5136  samples 102400-102655  elapsed 180.6s\n",
      "Encoded batch 411/5136  samples 104960-105215  elapsed 185.5s\n",
      "Encoded batch 421/5136  samples 107520-107775  elapsed 190.4s\n",
      "Encoded batch 431/5136  samples 110080-110335  elapsed 195.1s\n",
      "Encoded batch 441/5136  samples 112640-112895  elapsed 200.0s\n",
      "Encoded batch 451/5136  samples 115200-115455  elapsed 205.0s\n",
      "Encoded batch 461/5136  samples 117760-118015  elapsed 209.9s\n",
      "Encoded batch 471/5136  samples 120320-120575  elapsed 214.8s\n",
      "Encoded batch 481/5136  samples 122880-123135  elapsed 219.7s\n",
      "Encoded batch 491/5136  samples 125440-125695  elapsed 224.6s\n",
      "Encoded batch 501/5136  samples 128000-128255  elapsed 229.5s\n",
      "Encoded batch 511/5136  samples 130560-130815  elapsed 234.4s\n",
      "Encoded batch 521/5136  samples 133120-133375  elapsed 239.3s\n",
      "Encoded batch 531/5136  samples 135680-135935  elapsed 244.2s\n",
      "Encoded batch 541/5136  samples 138240-138495  elapsed 249.2s\n",
      "Encoded batch 551/5136  samples 140800-141055  elapsed 254.1s\n",
      "Encoded batch 561/5136  samples 143360-143615  elapsed 259.0s\n",
      "Encoded batch 571/5136  samples 145920-146175  elapsed 264.0s\n",
      "Encoded batch 581/5136  samples 148480-148735  elapsed 268.5s\n",
      "Encoded batch 591/5136  samples 151040-151295  elapsed 273.5s\n",
      "Encoded batch 601/5136  samples 153600-153855  elapsed 278.4s\n",
      "Encoded batch 611/5136  samples 156160-156415  elapsed 283.3s\n",
      "Encoded batch 621/5136  samples 158720-158975  elapsed 288.1s\n",
      "Encoded batch 631/5136  samples 161280-161535  elapsed 293.0s\n",
      "Encoded batch 641/5136  samples 163840-164095  elapsed 297.9s\n",
      "Encoded batch 651/5136  samples 166400-166655  elapsed 302.8s\n",
      "Encoded batch 661/5136  samples 168960-169215  elapsed 307.7s\n",
      "Encoded batch 671/5136  samples 171520-171775  elapsed 312.6s\n",
      "Encoded batch 681/5136  samples 174080-174335  elapsed 317.6s\n",
      "Encoded batch 691/5136  samples 176640-176895  elapsed 322.5s\n",
      "Encoded batch 701/5136  samples 179200-179455  elapsed 327.4s\n",
      "Encoded batch 711/5136  samples 181760-182015  elapsed 332.3s\n",
      "Encoded batch 721/5136  samples 184320-184575  elapsed 337.2s\n",
      "Encoded batch 731/5136  samples 186880-187135  elapsed 342.1s\n",
      "Encoded batch 741/5136  samples 189440-189695  elapsed 347.1s\n",
      "Encoded batch 751/5136  samples 192000-192255  elapsed 352.0s\n",
      "Encoded batch 761/5136  samples 194560-194815  elapsed 356.9s\n",
      "Encoded batch 771/5136  samples 197120-197375  elapsed 361.8s\n",
      "Encoded batch 781/5136  samples 199680-199935  elapsed 366.7s\n",
      "Encoded batch 791/5136  samples 202240-202495  elapsed 371.6s\n",
      "Encoded batch 801/5136  samples 204800-205055  elapsed 376.5s\n",
      "Encoded batch 811/5136  samples 207360-207615  elapsed 381.8s\n",
      "Encoded batch 821/5136  samples 209920-210175  elapsed 386.7s\n",
      "Encoded batch 831/5136  samples 212480-212735  elapsed 391.5s\n",
      "Encoded batch 841/5136  samples 215040-215295  elapsed 396.4s\n",
      "Encoded batch 851/5136  samples 217600-217855  elapsed 401.3s\n",
      "Encoded batch 861/5136  samples 220160-220415  elapsed 406.2s\n",
      "Encoded batch 871/5136  samples 222720-222975  elapsed 411.1s\n",
      "Encoded batch 881/5136  samples 225280-225535  elapsed 416.0s\n",
      "Encoded batch 891/5136  samples 227840-228095  elapsed 420.9s\n",
      "Encoded batch 901/5136  samples 230400-230655  elapsed 426.0s\n",
      "Encoded batch 911/5136  samples 232960-233215  elapsed 430.9s\n",
      "Encoded batch 921/5136  samples 235520-235775  elapsed 435.8s\n",
      "Encoded batch 931/5136  samples 238080-238335  elapsed 440.7s\n",
      "Encoded batch 941/5136  samples 240640-240895  elapsed 445.6s\n",
      "Encoded batch 951/5136  samples 243200-243455  elapsed 450.6s\n",
      "Encoded batch 961/5136  samples 245760-246015  elapsed 455.5s\n",
      "Encoded batch 971/5136  samples 248320-248575  elapsed 460.4s\n",
      "Encoded batch 981/5136  samples 250880-251135  elapsed 465.3s\n",
      "Encoded batch 991/5136  samples 253440-253695  elapsed 470.6s\n",
      "Encoded batch 1001/5136  samples 256000-256255  elapsed 475.6s\n",
      "Encoded batch 1011/5136  samples 258560-258815  elapsed 480.6s\n",
      "Encoded batch 1021/5136  samples 261120-261375  elapsed 485.5s\n",
      "Encoded batch 1031/5136  samples 263680-263935  elapsed 490.4s\n",
      "Encoded batch 1041/5136  samples 266240-266495  elapsed 495.3s\n",
      "Encoded batch 1051/5136  samples 268800-269055  elapsed 502.1s\n",
      "Encoded batch 1061/5136  samples 271360-271615  elapsed 507.9s\n",
      "Encoded batch 1071/5136  samples 273920-274175  elapsed 514.1s\n",
      "Encoded batch 1081/5136  samples 276480-276735  elapsed 520.2s\n",
      "Encoded batch 1091/5136  samples 279040-279295  elapsed 526.4s\n",
      "Encoded batch 1101/5136  samples 281600-281855  elapsed 532.6s\n",
      "Encoded batch 1111/5136  samples 284160-284415  elapsed 538.6s\n",
      "Encoded batch 1121/5136  samples 286720-286975  elapsed 544.8s\n",
      "Encoded batch 1131/5136  samples 289280-289535  elapsed 550.8s\n",
      "Encoded batch 1141/5136  samples 291840-292095  elapsed 556.8s\n",
      "Encoded batch 1151/5136  samples 294400-294655  elapsed 562.9s\n",
      "Encoded batch 1161/5136  samples 296960-297215  elapsed 569.0s\n",
      "Encoded batch 1171/5136  samples 299520-299775  elapsed 575.0s\n",
      "Encoded batch 1181/5136  samples 302080-302335  elapsed 581.0s\n",
      "Encoded batch 1191/5136  samples 304640-304895  elapsed 587.0s\n",
      "Encoded batch 1201/5136  samples 307200-307455  elapsed 593.1s\n",
      "Encoded batch 1211/5136  samples 309760-310015  elapsed 599.1s\n",
      "Encoded batch 1221/5136  samples 312320-312575  elapsed 605.2s\n",
      "Encoded batch 1231/5136  samples 314880-315135  elapsed 611.2s\n",
      "Encoded batch 1241/5136  samples 317440-317695  elapsed 617.3s\n",
      "Encoded batch 1251/5136  samples 320000-320255  elapsed 623.3s\n",
      "Encoded batch 1261/5136  samples 322560-322815  elapsed 629.4s\n",
      "Encoded batch 1271/5136  samples 325120-325375  elapsed 635.6s\n",
      "Encoded batch 1281/5136  samples 327680-327935  elapsed 641.7s\n",
      "Encoded batch 1291/5136  samples 330240-330495  elapsed 647.7s\n",
      "Encoded batch 1301/5136  samples 332800-333055  elapsed 653.7s\n",
      "Encoded batch 1311/5136  samples 335360-335615  elapsed 659.7s\n",
      "Encoded batch 1321/5136  samples 337920-338175  elapsed 665.7s\n",
      "Encoded batch 1331/5136  samples 340480-340735  elapsed 672.3s\n",
      "Encoded batch 1341/5136  samples 343040-343295  elapsed 678.4s\n",
      "Encoded batch 1351/5136  samples 345600-345855  elapsed 682.1s\n",
      "Encoded batch 1361/5136  samples 348160-348415  elapsed 685.6s\n",
      "Encoded batch 1371/5136  samples 350720-350975  elapsed 689.0s\n",
      "Encoded batch 1381/5136  samples 353280-353535  elapsed 692.5s\n",
      "Encoded batch 1391/5136  samples 355840-356095  elapsed 695.9s\n",
      "Encoded batch 1401/5136  samples 358400-358655  elapsed 699.3s\n",
      "Encoded batch 1411/5136  samples 360960-361215  elapsed 702.8s\n",
      "Encoded batch 1421/5136  samples 363520-363775  elapsed 706.2s\n",
      "Encoded batch 1431/5136  samples 366080-366335  elapsed 709.7s\n",
      "Encoded batch 1441/5136  samples 368640-368895  elapsed 713.1s\n",
      "Encoded batch 1451/5136  samples 371200-371455  elapsed 716.6s\n",
      "Encoded batch 1461/5136  samples 373760-374015  elapsed 720.1s\n",
      "Encoded batch 1471/5136  samples 376320-376575  elapsed 723.5s\n",
      "Encoded batch 1481/5136  samples 378880-379135  elapsed 727.0s\n",
      "Encoded batch 1491/5136  samples 381440-381695  elapsed 730.5s\n",
      "Encoded batch 1501/5136  samples 384000-384255  elapsed 733.9s\n",
      "Encoded batch 1511/5136  samples 386560-386815  elapsed 737.4s\n",
      "Encoded batch 1521/5136  samples 389120-389375  elapsed 740.9s\n",
      "Encoded batch 1531/5136  samples 391680-391935  elapsed 744.4s\n",
      "Encoded batch 1541/5136  samples 394240-394495  elapsed 747.7s\n",
      "Encoded batch 1551/5136  samples 396800-397055  elapsed 751.1s\n",
      "Encoded batch 1561/5136  samples 399360-399615  elapsed 754.4s\n",
      "Encoded batch 1571/5136  samples 401920-402175  elapsed 757.7s\n",
      "Encoded batch 1581/5136  samples 404480-404735  elapsed 761.0s\n",
      "Encoded batch 1591/5136  samples 407040-407295  elapsed 764.4s\n",
      "Encoded batch 1601/5136  samples 409600-409855  elapsed 767.7s\n",
      "Encoded batch 1611/5136  samples 412160-412415  elapsed 771.0s\n",
      "Encoded batch 1621/5136  samples 414720-414975  elapsed 774.3s\n",
      "Encoded batch 1631/5136  samples 417280-417535  elapsed 777.6s\n",
      "Encoded batch 1641/5136  samples 419840-420095  elapsed 781.0s\n",
      "Encoded batch 1651/5136  samples 422400-422655  elapsed 784.3s\n",
      "Encoded batch 1661/5136  samples 424960-425215  elapsed 787.7s\n",
      "Encoded batch 1671/5136  samples 427520-427775  elapsed 791.0s\n",
      "Encoded batch 1681/5136  samples 430080-430335  elapsed 794.4s\n",
      "Encoded batch 1691/5136  samples 432640-432895  elapsed 797.7s\n",
      "Encoded batch 1701/5136  samples 435200-435455  elapsed 801.0s\n",
      "Encoded batch 1711/5136  samples 437760-438015  elapsed 804.4s\n",
      "Encoded batch 1721/5136  samples 440320-440575  elapsed 807.8s\n",
      "Encoded batch 1731/5136  samples 442880-443135  elapsed 811.1s\n",
      "Encoded batch 1741/5136  samples 445440-445695  elapsed 814.7s\n",
      "Encoded batch 1751/5136  samples 448000-448255  elapsed 818.1s\n",
      "Encoded batch 1761/5136  samples 450560-450815  elapsed 821.6s\n",
      "Encoded batch 1771/5136  samples 453120-453375  elapsed 825.2s\n",
      "Encoded batch 1781/5136  samples 455680-455935  elapsed 828.7s\n",
      "Encoded batch 1791/5136  samples 458240-458495  elapsed 832.1s\n",
      "Encoded batch 1801/5136  samples 460800-461055  elapsed 835.6s\n",
      "Encoded batch 1811/5136  samples 463360-463615  elapsed 839.0s\n",
      "Encoded batch 1821/5136  samples 465920-466175  elapsed 842.3s\n",
      "Encoded batch 1831/5136  samples 468480-468735  elapsed 845.7s\n",
      "Encoded batch 1841/5136  samples 471040-471295  elapsed 849.1s\n",
      "Encoded batch 1851/5136  samples 473600-473855  elapsed 852.6s\n",
      "Encoded batch 1861/5136  samples 476160-476415  elapsed 856.0s\n",
      "Encoded batch 1871/5136  samples 478720-478975  elapsed 859.5s\n",
      "Encoded batch 1881/5136  samples 481280-481535  elapsed 862.9s\n",
      "Encoded batch 1891/5136  samples 483840-484095  elapsed 866.3s\n",
      "Encoded batch 1901/5136  samples 486400-486655  elapsed 869.9s\n",
      "Encoded batch 1911/5136  samples 488960-489215  elapsed 873.4s\n",
      "Encoded batch 1921/5136  samples 491520-491775  elapsed 876.9s\n",
      "Encoded batch 1931/5136  samples 494080-494335  elapsed 880.5s\n",
      "Encoded batch 1941/5136  samples 496640-496895  elapsed 883.9s\n",
      "Encoded batch 1951/5136  samples 499200-499455  elapsed 887.4s\n",
      "Encoded batch 1961/5136  samples 501760-502015  elapsed 890.9s\n",
      "Encoded batch 1971/5136  samples 504320-504575  elapsed 894.4s\n",
      "Encoded batch 1981/5136  samples 506880-507135  elapsed 897.9s\n",
      "Encoded batch 1991/5136  samples 509440-509695  elapsed 901.5s\n",
      "Encoded batch 2001/5136  samples 512000-512255  elapsed 905.0s\n",
      "Encoded batch 2011/5136  samples 514560-514815  elapsed 908.6s\n",
      "Encoded batch 2021/5136  samples 517120-517375  elapsed 912.2s\n",
      "Encoded batch 2031/5136  samples 519680-519935  elapsed 915.7s\n",
      "Encoded batch 2041/5136  samples 522240-522495  elapsed 919.3s\n",
      "Encoded batch 2051/5136  samples 524800-525055  elapsed 922.9s\n",
      "Encoded batch 2061/5136  samples 527360-527615  elapsed 926.4s\n",
      "Encoded batch 2071/5136  samples 529920-530175  elapsed 930.0s\n",
      "Encoded batch 2081/5136  samples 532480-532735  elapsed 933.6s\n",
      "Encoded batch 2091/5136  samples 535040-535295  elapsed 937.1s\n",
      "Encoded batch 2101/5136  samples 537600-537855  elapsed 940.7s\n",
      "Encoded batch 2111/5136  samples 540160-540415  elapsed 944.2s\n",
      "Encoded batch 2121/5136  samples 542720-542975  elapsed 947.7s\n",
      "Encoded batch 2131/5136  samples 545280-545535  elapsed 951.3s\n",
      "Encoded batch 2141/5136  samples 547840-548095  elapsed 954.9s\n",
      "Encoded batch 2151/5136  samples 550400-550655  elapsed 958.4s\n",
      "Encoded batch 2161/5136  samples 552960-553215  elapsed 962.0s\n",
      "Encoded batch 2171/5136  samples 555520-555775  elapsed 965.6s\n",
      "Encoded batch 2181/5136  samples 558080-558335  elapsed 969.1s\n",
      "Encoded batch 2191/5136  samples 560640-560895  elapsed 972.7s\n",
      "Encoded batch 2201/5136  samples 563200-563455  elapsed 976.4s\n",
      "Encoded batch 2211/5136  samples 565760-566015  elapsed 980.0s\n",
      "Encoded batch 2221/5136  samples 568320-568575  elapsed 983.6s\n",
      "Encoded batch 2231/5136  samples 570880-571135  elapsed 987.1s\n",
      "Encoded batch 2241/5136  samples 573440-573695  elapsed 990.7s\n",
      "Encoded batch 2251/5136  samples 576000-576255  elapsed 994.2s\n",
      "Encoded batch 2261/5136  samples 578560-578815  elapsed 997.8s\n",
      "Encoded batch 2271/5136  samples 581120-581375  elapsed 1001.4s\n",
      "Encoded batch 2281/5136  samples 583680-583935  elapsed 1004.9s\n",
      "Encoded batch 2291/5136  samples 586240-586495  elapsed 1008.5s\n",
      "Encoded batch 2301/5136  samples 588800-589055  elapsed 1012.0s\n",
      "Encoded batch 2311/5136  samples 591360-591615  elapsed 1015.6s\n",
      "Encoded batch 2321/5136  samples 593920-594175  elapsed 1019.2s\n",
      "Encoded batch 2331/5136  samples 596480-596735  elapsed 1022.8s\n",
      "Encoded batch 2341/5136  samples 599040-599295  elapsed 1026.4s\n",
      "Encoded batch 2351/5136  samples 601600-601855  elapsed 1029.9s\n",
      "Encoded batch 2361/5136  samples 604160-604415  elapsed 1033.5s\n",
      "Encoded batch 2371/5136  samples 606720-606975  elapsed 1037.0s\n",
      "Encoded batch 2381/5136  samples 609280-609535  elapsed 1040.6s\n",
      "Encoded batch 2391/5136  samples 611840-612095  elapsed 1044.2s\n",
      "Encoded batch 2401/5136  samples 614400-614655  elapsed 1047.8s\n",
      "Encoded batch 2411/5136  samples 616960-617215  elapsed 1051.4s\n",
      "Encoded batch 2421/5136  samples 619520-619775  elapsed 1055.0s\n",
      "Encoded batch 2431/5136  samples 622080-622335  elapsed 1058.5s\n",
      "Encoded batch 2441/5136  samples 624640-624895  elapsed 1062.1s\n",
      "Encoded batch 2451/5136  samples 627200-627455  elapsed 1065.6s\n",
      "Encoded batch 2461/5136  samples 629760-630015  elapsed 1069.1s\n",
      "Encoded batch 2471/5136  samples 632320-632575  elapsed 1072.5s\n",
      "Encoded batch 2481/5136  samples 634880-635135  elapsed 1076.0s\n",
      "Encoded batch 2491/5136  samples 637440-637695  elapsed 1079.6s\n",
      "Encoded batch 2501/5136  samples 640000-640255  elapsed 1083.1s\n",
      "Encoded batch 2511/5136  samples 642560-642815  elapsed 1086.6s\n",
      "Encoded batch 2521/5136  samples 645120-645375  elapsed 1090.1s\n",
      "Encoded batch 2531/5136  samples 647680-647935  elapsed 1093.5s\n",
      "Encoded batch 2541/5136  samples 650240-650495  elapsed 1097.0s\n",
      "Encoded batch 2551/5136  samples 652800-653055  elapsed 1100.5s\n",
      "Encoded batch 2561/5136  samples 655360-655615  elapsed 1104.0s\n",
      "Encoded batch 2571/5136  samples 657920-658175  elapsed 1107.6s\n",
      "Encoded batch 2581/5136  samples 660480-660735  elapsed 1111.0s\n",
      "Encoded batch 2591/5136  samples 663040-663295  elapsed 1114.5s\n",
      "Encoded batch 2601/5136  samples 665600-665855  elapsed 1118.0s\n",
      "Encoded batch 2611/5136  samples 668160-668415  elapsed 1121.5s\n",
      "Encoded batch 2621/5136  samples 670720-670975  elapsed 1125.0s\n",
      "Encoded batch 2631/5136  samples 673280-673535  elapsed 1128.5s\n",
      "Encoded batch 2641/5136  samples 675840-676095  elapsed 1132.0s\n",
      "Encoded batch 2651/5136  samples 678400-678655  elapsed 1135.5s\n",
      "Encoded batch 2661/5136  samples 680960-681215  elapsed 1139.2s\n",
      "Encoded batch 2671/5136  samples 683520-683775  elapsed 1142.6s\n",
      "Encoded batch 2681/5136  samples 686080-686335  elapsed 1146.2s\n",
      "Encoded batch 2691/5136  samples 688640-688895  elapsed 1149.6s\n",
      "Encoded batch 2701/5136  samples 691200-691455  elapsed 1153.1s\n",
      "Encoded batch 2711/5136  samples 693760-694015  elapsed 1156.6s\n",
      "Encoded batch 2721/5136  samples 696320-696575  elapsed 1160.1s\n",
      "Encoded batch 2731/5136  samples 698880-699135  elapsed 1163.6s\n",
      "Encoded batch 2741/5136  samples 701440-701695  elapsed 1167.2s\n",
      "Encoded batch 2751/5136  samples 704000-704255  elapsed 1170.7s\n",
      "Encoded batch 2761/5136  samples 706560-706815  elapsed 1174.2s\n",
      "Encoded batch 2771/5136  samples 709120-709375  elapsed 1177.7s\n",
      "Encoded batch 2781/5136  samples 711680-711935  elapsed 1181.2s\n",
      "Encoded batch 2791/5136  samples 714240-714495  elapsed 1184.6s\n",
      "Encoded batch 2801/5136  samples 716800-717055  elapsed 1188.1s\n",
      "Encoded batch 2811/5136  samples 719360-719615  elapsed 1191.6s\n",
      "Encoded batch 2821/5136  samples 721920-722175  elapsed 1195.1s\n",
      "Encoded batch 2831/5136  samples 724480-724735  elapsed 1198.5s\n",
      "Encoded batch 2841/5136  samples 727040-727295  elapsed 1202.0s\n",
      "Encoded batch 2851/5136  samples 729600-729855  elapsed 1205.5s\n",
      "Encoded batch 2861/5136  samples 732160-732415  elapsed 1209.0s\n",
      "Encoded batch 2871/5136  samples 734720-734975  elapsed 1212.5s\n",
      "Encoded batch 2881/5136  samples 737280-737535  elapsed 1216.0s\n",
      "Encoded batch 2891/5136  samples 739840-740095  elapsed 1219.4s\n",
      "Encoded batch 2901/5136  samples 742400-742655  elapsed 1222.9s\n",
      "Encoded batch 2911/5136  samples 744960-745215  elapsed 1226.4s\n",
      "Encoded batch 2921/5136  samples 747520-747775  elapsed 1229.9s\n",
      "Encoded batch 2931/5136  samples 750080-750335  elapsed 1233.4s\n",
      "Encoded batch 2941/5136  samples 752640-752895  elapsed 1236.9s\n",
      "Encoded batch 2951/5136  samples 755200-755455  elapsed 1240.4s\n",
      "Encoded batch 2961/5136  samples 757760-758015  elapsed 1243.8s\n",
      "Encoded batch 2971/5136  samples 760320-760575  elapsed 1247.3s\n",
      "Encoded batch 2981/5136  samples 762880-763135  elapsed 1250.7s\n",
      "Encoded batch 2991/5136  samples 765440-765695  elapsed 1254.2s\n",
      "Encoded batch 3001/5136  samples 768000-768255  elapsed 1257.7s\n",
      "Encoded batch 3011/5136  samples 770560-770815  elapsed 1261.1s\n",
      "Encoded batch 3021/5136  samples 773120-773375  elapsed 1264.6s\n",
      "Encoded batch 3031/5136  samples 775680-775935  elapsed 1268.0s\n",
      "Encoded batch 3041/5136  samples 778240-778495  elapsed 1271.5s\n",
      "Encoded batch 3051/5136  samples 780800-781055  elapsed 1275.0s\n",
      "Encoded batch 3061/5136  samples 783360-783615  elapsed 1278.5s\n",
      "Encoded batch 3071/5136  samples 785920-786175  elapsed 1281.9s\n",
      "Encoded batch 3081/5136  samples 788480-788735  elapsed 1285.4s\n",
      "Encoded batch 3091/5136  samples 791040-791295  elapsed 1288.8s\n",
      "Encoded batch 3101/5136  samples 793600-793855  elapsed 1292.3s\n",
      "Encoded batch 3111/5136  samples 796160-796415  elapsed 1295.8s\n",
      "Encoded batch 3121/5136  samples 798720-798975  elapsed 1299.4s\n",
      "Encoded batch 3131/5136  samples 801280-801535  elapsed 1302.8s\n",
      "Encoded batch 3141/5136  samples 803840-804095  elapsed 1306.3s\n",
      "Encoded batch 3151/5136  samples 806400-806655  elapsed 1309.8s\n",
      "Encoded batch 3161/5136  samples 808960-809215  elapsed 1313.2s\n",
      "Encoded batch 3171/5136  samples 811520-811775  elapsed 1316.7s\n",
      "Encoded batch 3181/5136  samples 814080-814335  elapsed 1320.2s\n",
      "Encoded batch 3191/5136  samples 816640-816895  elapsed 1323.6s\n",
      "Encoded batch 3201/5136  samples 819200-819455  elapsed 1327.1s\n",
      "Encoded batch 3211/5136  samples 821760-822015  elapsed 1330.6s\n",
      "Encoded batch 3221/5136  samples 824320-824575  elapsed 1334.0s\n",
      "Encoded batch 3231/5136  samples 826880-827135  elapsed 1337.5s\n",
      "Encoded batch 3241/5136  samples 829440-829695  elapsed 1340.9s\n",
      "Encoded batch 3251/5136  samples 832000-832255  elapsed 1344.4s\n",
      "Encoded batch 3261/5136  samples 834560-834815  elapsed 1347.8s\n",
      "Encoded batch 3271/5136  samples 837120-837375  elapsed 1351.4s\n",
      "Encoded batch 3281/5136  samples 839680-839935  elapsed 1354.8s\n",
      "Encoded batch 3291/5136  samples 842240-842495  elapsed 1358.3s\n",
      "Encoded batch 3301/5136  samples 844800-845055  elapsed 1361.8s\n",
      "Encoded batch 3311/5136  samples 847360-847615  elapsed 1365.2s\n",
      "Encoded batch 3321/5136  samples 849920-850175  elapsed 1368.7s\n",
      "Encoded batch 3331/5136  samples 852480-852735  elapsed 1372.2s\n",
      "Encoded batch 3341/5136  samples 855040-855295  elapsed 1375.7s\n",
      "Encoded batch 3351/5136  samples 857600-857855  elapsed 1379.1s\n",
      "Encoded batch 3361/5136  samples 860160-860415  elapsed 1382.6s\n",
      "Encoded batch 3371/5136  samples 862720-862975  elapsed 1386.1s\n",
      "Encoded batch 3381/5136  samples 865280-865535  elapsed 1389.6s\n",
      "Encoded batch 3391/5136  samples 867840-868095  elapsed 1393.0s\n",
      "Encoded batch 3401/5136  samples 870400-870655  elapsed 1396.6s\n",
      "Encoded batch 3411/5136  samples 872960-873215  elapsed 1400.0s\n",
      "Encoded batch 3421/5136  samples 875520-875775  elapsed 1403.5s\n",
      "Encoded batch 3431/5136  samples 878080-878335  elapsed 1407.0s\n",
      "Encoded batch 3441/5136  samples 880640-880895  elapsed 1410.5s\n",
      "Encoded batch 3451/5136  samples 883200-883455  elapsed 1414.0s\n",
      "Encoded batch 3461/5136  samples 885760-886015  elapsed 1417.5s\n",
      "Encoded batch 3471/5136  samples 888320-888575  elapsed 1421.0s\n",
      "Encoded batch 3481/5136  samples 890880-891135  elapsed 1424.4s\n",
      "Encoded batch 3491/5136  samples 893440-893695  elapsed 1427.9s\n",
      "Encoded batch 3501/5136  samples 896000-896255  elapsed 1431.3s\n",
      "Encoded batch 3511/5136  samples 898560-898815  elapsed 1434.8s\n",
      "Encoded batch 3521/5136  samples 901120-901375  elapsed 1438.3s\n",
      "Encoded batch 3531/5136  samples 903680-903935  elapsed 1441.8s\n",
      "Encoded batch 3541/5136  samples 906240-906495  elapsed 1445.2s\n",
      "Encoded batch 3551/5136  samples 908800-909055  elapsed 1448.7s\n",
      "Encoded batch 3561/5136  samples 911360-911615  elapsed 1452.2s\n",
      "Encoded batch 3571/5136  samples 913920-914175  elapsed 1455.7s\n",
      "Encoded batch 3581/5136  samples 916480-916735  elapsed 1459.3s\n",
      "Encoded batch 3591/5136  samples 919040-919295  elapsed 1462.8s\n",
      "Encoded batch 3601/5136  samples 921600-921855  elapsed 1466.3s\n",
      "Encoded batch 3611/5136  samples 924160-924415  elapsed 1469.8s\n",
      "Encoded batch 3621/5136  samples 926720-926975  elapsed 1473.2s\n",
      "Encoded batch 3631/5136  samples 929280-929535  elapsed 1476.7s\n",
      "Encoded batch 3641/5136  samples 931840-932095  elapsed 1480.1s\n",
      "Encoded batch 3651/5136  samples 934400-934655  elapsed 1483.6s\n",
      "Encoded batch 3661/5136  samples 936960-937215  elapsed 1487.1s\n",
      "Encoded batch 3671/5136  samples 939520-939775  elapsed 1490.5s\n",
      "Encoded batch 3681/5136  samples 942080-942335  elapsed 1494.0s\n",
      "Encoded batch 3691/5136  samples 944640-944895  elapsed 1497.4s\n",
      "Encoded batch 3701/5136  samples 947200-947455  elapsed 1500.9s\n",
      "Encoded batch 3711/5136  samples 949760-950015  elapsed 1504.3s\n",
      "Encoded batch 3721/5136  samples 952320-952575  elapsed 1507.8s\n",
      "Encoded batch 3731/5136  samples 954880-955135  elapsed 1511.3s\n",
      "Encoded batch 3741/5136  samples 957440-957695  elapsed 1515.0s\n",
      "Encoded batch 3751/5136  samples 960000-960255  elapsed 1518.4s\n",
      "Encoded batch 3761/5136  samples 962560-962815  elapsed 1521.9s\n",
      "Encoded batch 3771/5136  samples 965120-965375  elapsed 1525.4s\n",
      "Encoded batch 3781/5136  samples 967680-967935  elapsed 1528.9s\n",
      "Encoded batch 3791/5136  samples 970240-970495  elapsed 1532.4s\n",
      "Encoded batch 3801/5136  samples 972800-973055  elapsed 1535.8s\n",
      "Encoded batch 3811/5136  samples 975360-975615  elapsed 1539.3s\n",
      "Encoded batch 3821/5136  samples 977920-978175  elapsed 1542.8s\n",
      "Encoded batch 3831/5136  samples 980480-980735  elapsed 1546.2s\n",
      "Encoded batch 3841/5136  samples 983040-983295  elapsed 1549.6s\n",
      "Encoded batch 3851/5136  samples 985600-985855  elapsed 1553.0s\n",
      "Encoded batch 3861/5136  samples 988160-988415  elapsed 1556.4s\n",
      "Encoded batch 3871/5136  samples 990720-990975  elapsed 1559.9s\n",
      "Encoded batch 3881/5136  samples 993280-993535  elapsed 1563.3s\n",
      "Encoded batch 3891/5136  samples 995840-996095  elapsed 1566.8s\n",
      "Encoded batch 3901/5136  samples 998400-998655  elapsed 1570.2s\n",
      "Encoded batch 3911/5136  samples 1000960-1001215  elapsed 1573.6s\n",
      "Encoded batch 3921/5136  samples 1003520-1003775  elapsed 1577.1s\n",
      "Encoded batch 3931/5136  samples 1006080-1006335  elapsed 1580.6s\n",
      "Encoded batch 3941/5136  samples 1008640-1008895  elapsed 1583.9s\n",
      "Encoded batch 3951/5136  samples 1011200-1011455  elapsed 1587.4s\n",
      "Encoded batch 3961/5136  samples 1013760-1014015  elapsed 1590.8s\n",
      "Encoded batch 3971/5136  samples 1016320-1016575  elapsed 1594.3s\n",
      "Encoded batch 3981/5136  samples 1018880-1019135  elapsed 1597.7s\n",
      "Encoded batch 3991/5136  samples 1021440-1021695  elapsed 1601.2s\n",
      "Encoded batch 4001/5136  samples 1024000-1024255  elapsed 1604.6s\n",
      "Encoded batch 4011/5136  samples 1026560-1026815  elapsed 1608.1s\n",
      "Encoded batch 4021/5136  samples 1029120-1029375  elapsed 1611.5s\n",
      "Encoded batch 4031/5136  samples 1031680-1031935  elapsed 1614.9s\n",
      "Encoded batch 4041/5136  samples 1034240-1034495  elapsed 1618.3s\n",
      "Encoded batch 4051/5136  samples 1036800-1037055  elapsed 1621.9s\n",
      "Encoded batch 4061/5136  samples 1039360-1039615  elapsed 1625.4s\n",
      "Encoded batch 4071/5136  samples 1041920-1042175  elapsed 1628.9s\n",
      "Encoded batch 4081/5136  samples 1044480-1044735  elapsed 1632.3s\n",
      "Encoded batch 4091/5136  samples 1047040-1047295  elapsed 1635.7s\n",
      "Encoded batch 4101/5136  samples 1049600-1049855  elapsed 1639.2s\n",
      "Encoded batch 4111/5136  samples 1052160-1052415  elapsed 1642.7s\n",
      "Encoded batch 4121/5136  samples 1054720-1054975  elapsed 1646.2s\n",
      "Encoded batch 4131/5136  samples 1057280-1057535  elapsed 1649.7s\n",
      "Encoded batch 4141/5136  samples 1059840-1060095  elapsed 1653.1s\n",
      "Encoded batch 4151/5136  samples 1062400-1062655  elapsed 1656.6s\n",
      "Encoded batch 4161/5136  samples 1064960-1065215  elapsed 1660.0s\n",
      "Encoded batch 4171/5136  samples 1067520-1067775  elapsed 1663.5s\n",
      "Encoded batch 4181/5136  samples 1070080-1070335  elapsed 1667.0s\n",
      "Encoded batch 4191/5136  samples 1072640-1072895  elapsed 1670.4s\n",
      "Encoded batch 4201/5136  samples 1075200-1075455  elapsed 1673.9s\n",
      "Encoded batch 4211/5136  samples 1077760-1078015  elapsed 1677.3s\n",
      "Encoded batch 4221/5136  samples 1080320-1080575  elapsed 1680.8s\n",
      "Encoded batch 4231/5136  samples 1082880-1083135  elapsed 1684.2s\n",
      "Encoded batch 4241/5136  samples 1085440-1085695  elapsed 1687.7s\n",
      "Encoded batch 4251/5136  samples 1088000-1088255  elapsed 1691.2s\n",
      "Encoded batch 4261/5136  samples 1090560-1090815  elapsed 1694.6s\n",
      "Encoded batch 4271/5136  samples 1093120-1093375  elapsed 1698.1s\n",
      "Encoded batch 4281/5136  samples 1095680-1095935  elapsed 1701.6s\n",
      "Encoded batch 4291/5136  samples 1098240-1098495  elapsed 1705.1s\n",
      "Encoded batch 4301/5136  samples 1100800-1101055  elapsed 1708.5s\n",
      "Encoded batch 4311/5136  samples 1103360-1103615  elapsed 1712.0s\n",
      "Encoded batch 4321/5136  samples 1105920-1106175  elapsed 1715.5s\n",
      "Encoded batch 4331/5136  samples 1108480-1108735  elapsed 1718.9s\n",
      "Encoded batch 4341/5136  samples 1111040-1111295  elapsed 1722.4s\n",
      "Encoded batch 4351/5136  samples 1113600-1113855  elapsed 1725.8s\n",
      "Encoded batch 4361/5136  samples 1116160-1116415  elapsed 1729.3s\n",
      "Encoded batch 4371/5136  samples 1118720-1118975  elapsed 1732.8s\n",
      "Encoded batch 4381/5136  samples 1121280-1121535  elapsed 1736.2s\n",
      "Encoded batch 4391/5136  samples 1123840-1124095  elapsed 1739.6s\n",
      "Encoded batch 4401/5136  samples 1126400-1126655  elapsed 1743.1s\n",
      "Encoded batch 4411/5136  samples 1128960-1129215  elapsed 1746.6s\n",
      "Encoded batch 4421/5136  samples 1131520-1131775  elapsed 1750.1s\n",
      "Encoded batch 4431/5136  samples 1134080-1134335  elapsed 1753.5s\n",
      "Encoded batch 4441/5136  samples 1136640-1136895  elapsed 1756.9s\n",
      "Encoded batch 4451/5136  samples 1139200-1139455  elapsed 1760.4s\n",
      "Encoded batch 4461/5136  samples 1141760-1142015  elapsed 1763.9s\n",
      "Encoded batch 4471/5136  samples 1144320-1144575  elapsed 1767.3s\n",
      "Encoded batch 4481/5136  samples 1146880-1147135  elapsed 1770.8s\n",
      "Encoded batch 4491/5136  samples 1149440-1149695  elapsed 1774.4s\n",
      "Encoded batch 4501/5136  samples 1152000-1152255  elapsed 1777.8s\n",
      "Encoded batch 4511/5136  samples 1154560-1154815  elapsed 1781.5s\n",
      "Encoded batch 4521/5136  samples 1157120-1157375  elapsed 1785.0s\n",
      "Encoded batch 4531/5136  samples 1159680-1159935  elapsed 1788.4s\n",
      "Encoded batch 4541/5136  samples 1162240-1162495  elapsed 1791.9s\n",
      "Encoded batch 4551/5136  samples 1164800-1165055  elapsed 1795.3s\n",
      "Encoded batch 4561/5136  samples 1167360-1167615  elapsed 1798.7s\n",
      "Encoded batch 4571/5136  samples 1169920-1170175  elapsed 1802.2s\n",
      "Encoded batch 4581/5136  samples 1172480-1172735  elapsed 1805.7s\n",
      "Encoded batch 4591/5136  samples 1175040-1175295  elapsed 1809.2s\n",
      "Encoded batch 4601/5136  samples 1177600-1177855  elapsed 1812.6s\n",
      "Encoded batch 4611/5136  samples 1180160-1180415  elapsed 1816.1s\n",
      "Encoded batch 4621/5136  samples 1182720-1182975  elapsed 1819.5s\n",
      "Encoded batch 4631/5136  samples 1185280-1185535  elapsed 1823.3s\n",
      "Encoded batch 4641/5136  samples 1187840-1188095  elapsed 1826.8s\n",
      "Encoded batch 4651/5136  samples 1190400-1190655  elapsed 1830.3s\n",
      "Encoded batch 4661/5136  samples 1192960-1193215  elapsed 1833.7s\n",
      "Encoded batch 4671/5136  samples 1195520-1195775  elapsed 1837.2s\n",
      "Encoded batch 4681/5136  samples 1198080-1198335  elapsed 1840.6s\n",
      "Encoded batch 4691/5136  samples 1200640-1200895  elapsed 1844.1s\n",
      "Encoded batch 4701/5136  samples 1203200-1203455  elapsed 1847.6s\n",
      "Encoded batch 4711/5136  samples 1205760-1206015  elapsed 1851.0s\n",
      "Encoded batch 4721/5136  samples 1208320-1208575  elapsed 1854.5s\n",
      "Encoded batch 4731/5136  samples 1210880-1211135  elapsed 1858.0s\n",
      "Encoded batch 4741/5136  samples 1213440-1213695  elapsed 1861.5s\n",
      "Encoded batch 4751/5136  samples 1216000-1216255  elapsed 1864.9s\n",
      "Encoded batch 4761/5136  samples 1218560-1218815  elapsed 1868.4s\n",
      "Encoded batch 4771/5136  samples 1221120-1221375  elapsed 1871.8s\n",
      "Encoded batch 4781/5136  samples 1223680-1223935  elapsed 1875.3s\n",
      "Encoded batch 4791/5136  samples 1226240-1226495  elapsed 1878.8s\n",
      "Encoded batch 4801/5136  samples 1228800-1229055  elapsed 1882.3s\n",
      "Encoded batch 4811/5136  samples 1231360-1231615  elapsed 1885.7s\n",
      "Encoded batch 4821/5136  samples 1233920-1234175  elapsed 1889.2s\n",
      "Encoded batch 4831/5136  samples 1236480-1236735  elapsed 1892.8s\n",
      "Encoded batch 4841/5136  samples 1239040-1239295  elapsed 1896.3s\n",
      "Encoded batch 4851/5136  samples 1241600-1241855  elapsed 1899.8s\n",
      "Encoded batch 4861/5136  samples 1244160-1244415  elapsed 1903.2s\n",
      "Encoded batch 4871/5136  samples 1246720-1246975  elapsed 1906.8s\n",
      "Encoded batch 4881/5136  samples 1249280-1249535  elapsed 1910.3s\n",
      "Encoded batch 4891/5136  samples 1251840-1252095  elapsed 1913.7s\n",
      "Encoded batch 4901/5136  samples 1254400-1254655  elapsed 1917.2s\n",
      "Encoded batch 4911/5136  samples 1256960-1257215  elapsed 1920.7s\n",
      "Encoded batch 4921/5136  samples 1259520-1259775  elapsed 1924.1s\n",
      "Encoded batch 4931/5136  samples 1262080-1262335  elapsed 1927.6s\n",
      "Encoded batch 4941/5136  samples 1264640-1264895  elapsed 1931.0s\n",
      "Encoded batch 4951/5136  samples 1267200-1267455  elapsed 1934.5s\n",
      "Encoded batch 4961/5136  samples 1269760-1270015  elapsed 1938.0s\n",
      "Encoded batch 4971/5136  samples 1272320-1272575  elapsed 1941.6s\n",
      "Encoded batch 4981/5136  samples 1274880-1275135  elapsed 1945.1s\n",
      "Encoded batch 4991/5136  samples 1277440-1277695  elapsed 1948.5s\n",
      "Encoded batch 5001/5136  samples 1280000-1280255  elapsed 1952.0s\n",
      "Encoded batch 5011/5136  samples 1282560-1282815  elapsed 1955.4s\n",
      "Encoded batch 5021/5136  samples 1285120-1285375  elapsed 1958.9s\n",
      "Encoded batch 5031/5136  samples 1287680-1287935  elapsed 1962.4s\n",
      "Encoded batch 5041/5136  samples 1290240-1290495  elapsed 1965.9s\n",
      "Encoded batch 5051/5136  samples 1292800-1293055  elapsed 1969.4s\n",
      "Encoded batch 5061/5136  samples 1295360-1295615  elapsed 1972.8s\n",
      "Encoded batch 5071/5136  samples 1297920-1298175  elapsed 1976.3s\n",
      "Encoded batch 5081/5136  samples 1300480-1300735  elapsed 1979.7s\n",
      "Encoded batch 5091/5136  samples 1303040-1303295  elapsed 1983.2s\n",
      "Encoded batch 5101/5136  samples 1305600-1305855  elapsed 1986.7s\n",
      "Encoded batch 5111/5136  samples 1308160-1308415  elapsed 1990.1s\n",
      "Encoded batch 5121/5136  samples 1310720-1310975  elapsed 1993.6s\n",
      "Encoded batch 5131/5136  samples 1313280-1313535  elapsed 1997.1s\n",
      "Final embeddings shape: (1314720, 384) dtype: float32\n",
      "Saved embeddings to artifacts\\emotions_all_embeddings.npz\n",
      "Done. embeddings shape: (1314720, 384)\n"
     ]
    }
   ],
   "source": [
    "# Snippet : encode_full_sbert\n",
    "# Ex√©cute cette cellule pour encoder TOUT le corpus en embeddings SBERT.\n",
    "# Reprend si 'artifacts/emotions_all_embeddings.npz' existe.\n",
    "\n",
    "import os, math, time, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n",
    "OUT_PATH = ART / \"emotions_all_embeddings.npz\"\n",
    "TEXT_COL = \"review_body\"   # adapte si n√©cessaire\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"   # rapide / qualit√© correcte\n",
    "USE_GPU = True  # change si tu veux CPU -> False\n",
    "BATCH_SIZE = 256  # ajuste selon GPU/m√©moire\n",
    "PRINT_EVERY = 10   # logs √† chaque X batches\n",
    "\n",
    "# v√©rifications\n",
    "assert TEXT_COL in df.columns, f\"Colonne {TEXT_COL} introuvable dans df\"\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "n = len(texts)\n",
    "print(\"n_texts =\", n)\n",
    "\n",
    "# si d√©j√† encod√© -> chargement / info\n",
    "if OUT_PATH.exists():\n",
    "    print(\"Found existing embeddings file:\", OUT_PATH)\n",
    "    d = np.load(OUT_PATH, allow_pickle=True)\n",
    "    X_all = d[\"X\"]\n",
    "    print(\"Loaded embeddings shape:\", X_all.shape)\n",
    "else:\n",
    "    # import SBERT\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    device = \"cuda\" if (USE_GPU and __import__(\"torch\").cuda.is_available()) else \"cpu\"\n",
    "    print(\"Loading SBERT on device:\", device, \"model:\", MODEL_NAME)\n",
    "    sbert = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "    # encode in batches and accumulate as float32 (sauvegarde compress√©e √† la fin)\n",
    "    t0 = time.time()\n",
    "    emb_list = []\n",
    "    nbatches = math.ceil(n / BATCH_SIZE)\n",
    "    for i in range(nbatches):\n",
    "        start = i * BATCH_SIZE\n",
    "        stop = min(n, (i+1) * BATCH_SIZE)\n",
    "        batch = texts[start:stop]\n",
    "        emb = sbert.encode(batch, batch_size=BATCH_SIZE, show_progress_bar=False,\n",
    "                          convert_to_numpy=True, normalize_embeddings=True)\n",
    "        emb_list.append(emb.astype(\"float32\"))\n",
    "        if (i % PRINT_EVERY) == 0:\n",
    "            print(f\"Encoded batch {i+1}/{nbatches}  samples {start}-{stop-1}  elapsed {time.time()-t0:.1f}s\")\n",
    "\n",
    "    X_all = np.vstack(emb_list)\n",
    "    print(\"Final embeddings shape:\", X_all.shape, \"dtype:\", X_all.dtype)\n",
    "\n",
    "    # save compressed (npz) + metadata\n",
    "    np.savez_compressed(OUT_PATH, X=X_all)\n",
    "    print(\"Saved embeddings to\", OUT_PATH)\n",
    "\n",
    "# quick sanity\n",
    "print(\"Done. embeddings shape:\", X_all.shape)\n",
    "# expose X_all in globals for next cells\n",
    "globals()[\"X_all_emb\"] = X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22768355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\antoi\\.cache\\huggingface\\hub\\models--joeddav--distilbert-base-uncased-go-emotions-student. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\antoi\\OneDrive\\Documents\\Ynov\\Projet fil rouge\\Bloc 5\\amazon-reviews-insights\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0  /  1314720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1600  /  1314720\n",
      "Processed 3200  /  1314720\n",
      "Processed 4800  /  1314720\n",
      "Processed 6400  /  1314720\n",
      "Processed 8000  /  1314720\n",
      "Processed 9600  /  1314720\n",
      "Processed 11200  /  1314720\n",
      "Processed 12800  /  1314720\n",
      "Processed 14400  /  1314720\n",
      "Processed 16000  /  1314720\n",
      "Processed 17600  /  1314720\n",
      "Processed 19200  /  1314720\n",
      "Processed 20800  /  1314720\n",
      "Processed 22400  /  1314720\n",
      "Processed 24000  /  1314720\n",
      "Processed 25600  /  1314720\n",
      "Processed 27200  /  1314720\n",
      "Processed 28800  /  1314720\n",
      "Processed 30400  /  1314720\n",
      "Processed 32000  /  1314720\n",
      "Processed 33600  /  1314720\n",
      "Processed 35200  /  1314720\n",
      "Processed 36800  /  1314720\n",
      "Processed 38400  /  1314720\n",
      "Processed 40000  /  1314720\n",
      "Processed 41600  /  1314720\n",
      "Processed 43200  /  1314720\n",
      "Processed 44800  /  1314720\n",
      "Processed 46400  /  1314720\n",
      "Processed 48000  /  1314720\n",
      "Processed 49600  /  1314720\n",
      "Processed 51200  /  1314720\n",
      "Processed 52800  /  1314720\n",
      "Processed 54400  /  1314720\n",
      "Processed 56000  /  1314720\n",
      "Processed 57600  /  1314720\n",
      "Processed 59200  /  1314720\n",
      "Processed 60800  /  1314720\n",
      "Processed 62400  /  1314720\n",
      "Processed 64000  /  1314720\n",
      "Processed 65600  /  1314720\n",
      "Processed 67200  /  1314720\n",
      "Processed 68800  /  1314720\n",
      "Processed 70400  /  1314720\n",
      "Processed 72000  /  1314720\n",
      "Processed 73600  /  1314720\n",
      "Processed 75200  /  1314720\n",
      "Processed 76800  /  1314720\n",
      "Processed 78400  /  1314720\n",
      "Processed 80000  /  1314720\n",
      "Processed 81600  /  1314720\n",
      "Processed 83200  /  1314720\n",
      "Processed 84800  /  1314720\n",
      "Processed 86400  /  1314720\n",
      "Processed 88000  /  1314720\n",
      "Processed 89600  /  1314720\n",
      "Processed 91200  /  1314720\n",
      "Processed 92800  /  1314720\n",
      "Processed 94400  /  1314720\n",
      "Processed 96000  /  1314720\n",
      "Processed 97600  /  1314720\n",
      "Processed 99200  /  1314720\n",
      "Processed 100800  /  1314720\n",
      "Processed 102400  /  1314720\n",
      "Processed 104000  /  1314720\n",
      "Processed 105600  /  1314720\n",
      "Processed 107200  /  1314720\n",
      "Processed 108800  /  1314720\n",
      "Processed 110400  /  1314720\n",
      "Processed 112000  /  1314720\n",
      "Processed 113600  /  1314720\n",
      "Processed 115200  /  1314720\n",
      "Processed 116800  /  1314720\n",
      "Processed 118400  /  1314720\n",
      "Processed 120000  /  1314720\n",
      "Processed 121600  /  1314720\n",
      "Processed 123200  /  1314720\n",
      "Processed 124800  /  1314720\n",
      "Processed 126400  /  1314720\n",
      "Processed 128000  /  1314720\n",
      "Processed 129600  /  1314720\n",
      "Processed 131200  /  1314720\n",
      "Processed 132800  /  1314720\n",
      "Processed 134400  /  1314720\n",
      "Processed 136000  /  1314720\n",
      "Processed 137600  /  1314720\n",
      "Processed 139200  /  1314720\n",
      "Processed 140800  /  1314720\n",
      "Processed 142400  /  1314720\n",
      "Processed 144000  /  1314720\n",
      "Processed 145600  /  1314720\n",
      "Processed 147200  /  1314720\n",
      "Processed 148800  /  1314720\n",
      "Processed 150400  /  1314720\n",
      "Processed 152000  /  1314720\n",
      "Processed 153600  /  1314720\n",
      "Processed 155200  /  1314720\n",
      "Processed 156800  /  1314720\n",
      "Processed 158400  /  1314720\n",
      "Processed 160000  /  1314720\n",
      "Processed 161600  /  1314720\n",
      "Processed 163200  /  1314720\n",
      "Processed 164800  /  1314720\n",
      "Processed 166400  /  1314720\n",
      "Processed 168000  /  1314720\n",
      "Processed 169600  /  1314720\n",
      "Processed 171200  /  1314720\n",
      "Processed 172800  /  1314720\n",
      "Processed 174400  /  1314720\n",
      "Processed 176000  /  1314720\n",
      "Processed 177600  /  1314720\n",
      "Processed 179200  /  1314720\n",
      "Processed 180800  /  1314720\n",
      "Processed 182400  /  1314720\n",
      "Processed 184000  /  1314720\n",
      "Processed 185600  /  1314720\n",
      "Processed 187200  /  1314720\n",
      "Processed 188800  /  1314720\n",
      "Processed 190400  /  1314720\n",
      "Processed 192000  /  1314720\n",
      "Processed 193600  /  1314720\n",
      "Processed 195200  /  1314720\n",
      "Processed 196800  /  1314720\n",
      "Processed 198400  /  1314720\n",
      "Processed 200000  /  1314720\n",
      "Processed 201600  /  1314720\n",
      "Processed 203200  /  1314720\n",
      "Processed 204800  /  1314720\n",
      "Processed 206400  /  1314720\n",
      "Processed 208000  /  1314720\n",
      "Processed 209600  /  1314720\n",
      "Processed 211200  /  1314720\n",
      "Processed 212800  /  1314720\n",
      "Processed 214400  /  1314720\n",
      "Processed 216000  /  1314720\n",
      "Processed 217600  /  1314720\n",
      "Processed 219200  /  1314720\n",
      "Processed 220800  /  1314720\n",
      "Processed 222400  /  1314720\n",
      "Processed 224000  /  1314720\n",
      "Processed 225600  /  1314720\n",
      "Processed 227200  /  1314720\n",
      "Processed 228800  /  1314720\n",
      "Processed 230400  /  1314720\n",
      "Processed 232000  /  1314720\n",
      "Processed 233600  /  1314720\n",
      "Processed 235200  /  1314720\n",
      "Processed 236800  /  1314720\n",
      "Processed 238400  /  1314720\n",
      "Processed 240000  /  1314720\n",
      "Processed 241600  /  1314720\n",
      "Processed 243200  /  1314720\n",
      "Processed 244800  /  1314720\n",
      "Processed 246400  /  1314720\n",
      "Processed 248000  /  1314720\n",
      "Processed 249600  /  1314720\n",
      "Processed 251200  /  1314720\n",
      "Processed 252800  /  1314720\n",
      "Processed 254400  /  1314720\n",
      "Processed 256000  /  1314720\n",
      "Processed 257600  /  1314720\n",
      "Processed 259200  /  1314720\n",
      "Processed 260800  /  1314720\n",
      "Processed 262400  /  1314720\n",
      "Processed 264000  /  1314720\n",
      "Processed 265600  /  1314720\n",
      "Processed 267200  /  1314720\n",
      "Processed 268800  /  1314720\n",
      "Processed 270400  /  1314720\n",
      "Processed 272000  /  1314720\n",
      "Processed 273600  /  1314720\n",
      "Processed 275200  /  1314720\n",
      "Processed 276800  /  1314720\n",
      "Processed 278400  /  1314720\n",
      "Processed 280000  /  1314720\n",
      "Processed 281600  /  1314720\n",
      "Processed 283200  /  1314720\n",
      "Processed 284800  /  1314720\n",
      "Processed 286400  /  1314720\n",
      "Processed 288000  /  1314720\n",
      "Processed 289600  /  1314720\n",
      "Processed 291200  /  1314720\n",
      "Processed 292800  /  1314720\n",
      "Processed 294400  /  1314720\n",
      "Processed 296000  /  1314720\n",
      "Processed 297600  /  1314720\n",
      "Processed 299200  /  1314720\n",
      "Processed 300800  /  1314720\n",
      "Processed 302400  /  1314720\n",
      "Processed 304000  /  1314720\n",
      "Processed 305600  /  1314720\n",
      "Processed 307200  /  1314720\n",
      "Processed 308800  /  1314720\n",
      "Processed 310400  /  1314720\n",
      "Processed 312000  /  1314720\n",
      "Processed 313600  /  1314720\n",
      "Processed 315200  /  1314720\n",
      "Processed 316800  /  1314720\n",
      "Processed 318400  /  1314720\n",
      "Processed 320000  /  1314720\n",
      "Processed 321600  /  1314720\n",
      "Processed 323200  /  1314720\n",
      "Processed 324800  /  1314720\n",
      "Processed 326400  /  1314720\n",
      "Processed 328000  /  1314720\n",
      "Processed 329600  /  1314720\n",
      "Processed 331200  /  1314720\n",
      "Processed 332800  /  1314720\n",
      "Processed 334400  /  1314720\n",
      "Processed 336000  /  1314720\n",
      "Processed 337600  /  1314720\n",
      "Processed 339200  /  1314720\n",
      "Processed 340800  /  1314720\n",
      "Processed 342400  /  1314720\n",
      "Processed 344000  /  1314720\n",
      "Processed 345600  /  1314720\n",
      "Processed 347200  /  1314720\n",
      "Processed 348800  /  1314720\n",
      "Processed 350400  /  1314720\n",
      "Processed 352000  /  1314720\n",
      "Processed 353600  /  1314720\n",
      "Processed 355200  /  1314720\n",
      "Processed 356800  /  1314720\n",
      "Processed 358400  /  1314720\n",
      "Processed 360000  /  1314720\n",
      "Processed 361600  /  1314720\n",
      "Processed 363200  /  1314720\n",
      "Processed 364800  /  1314720\n",
      "Processed 366400  /  1314720\n",
      "Processed 368000  /  1314720\n",
      "Processed 369600  /  1314720\n",
      "Processed 371200  /  1314720\n",
      "Processed 372800  /  1314720\n",
      "Processed 374400  /  1314720\n",
      "Processed 376000  /  1314720\n",
      "Processed 377600  /  1314720\n",
      "Processed 379200  /  1314720\n",
      "Processed 380800  /  1314720\n",
      "Processed 382400  /  1314720\n",
      "Processed 384000  /  1314720\n",
      "Processed 385600  /  1314720\n",
      "Processed 387200  /  1314720\n",
      "Processed 388800  /  1314720\n",
      "Processed 390400  /  1314720\n",
      "Processed 392000  /  1314720\n",
      "Processed 393600  /  1314720\n",
      "Processed 395200  /  1314720\n",
      "Processed 396800  /  1314720\n",
      "Processed 398400  /  1314720\n",
      "Processed 400000  /  1314720\n",
      "Processed 401600  /  1314720\n",
      "Processed 403200  /  1314720\n",
      "Processed 404800  /  1314720\n",
      "Processed 406400  /  1314720\n",
      "Processed 408000  /  1314720\n",
      "Processed 409600  /  1314720\n",
      "Processed 411200  /  1314720\n",
      "Processed 412800  /  1314720\n",
      "Processed 414400  /  1314720\n",
      "Processed 416000  /  1314720\n",
      "Processed 417600  /  1314720\n",
      "Processed 419200  /  1314720\n",
      "Processed 420800  /  1314720\n",
      "Processed 422400  /  1314720\n",
      "Processed 424000  /  1314720\n",
      "Processed 425600  /  1314720\n",
      "Processed 427200  /  1314720\n",
      "Processed 428800  /  1314720\n",
      "Processed 430400  /  1314720\n",
      "Processed 432000  /  1314720\n",
      "Processed 433600  /  1314720\n",
      "Processed 435200  /  1314720\n",
      "Processed 436800  /  1314720\n",
      "Processed 438400  /  1314720\n",
      "Processed 440000  /  1314720\n",
      "Processed 441600  /  1314720\n",
      "Processed 443200  /  1314720\n",
      "Processed 444800  /  1314720\n",
      "Processed 446400  /  1314720\n",
      "Processed 448000  /  1314720\n",
      "Processed 449600  /  1314720\n",
      "Processed 451200  /  1314720\n",
      "Processed 452800  /  1314720\n",
      "Processed 454400  /  1314720\n",
      "Processed 456000  /  1314720\n",
      "Processed 457600  /  1314720\n",
      "Processed 459200  /  1314720\n",
      "Processed 460800  /  1314720\n",
      "Processed 462400  /  1314720\n",
      "Processed 464000  /  1314720\n",
      "Processed 465600  /  1314720\n",
      "Processed 467200  /  1314720\n",
      "Processed 468800  /  1314720\n",
      "Processed 470400  /  1314720\n",
      "Processed 472000  /  1314720\n",
      "Processed 473600  /  1314720\n",
      "Processed 475200  /  1314720\n",
      "Processed 476800  /  1314720\n",
      "Processed 478400  /  1314720\n",
      "Processed 480000  /  1314720\n",
      "Processed 481600  /  1314720\n",
      "Processed 483200  /  1314720\n",
      "Processed 484800  /  1314720\n",
      "Processed 486400  /  1314720\n",
      "Processed 488000  /  1314720\n",
      "Processed 489600  /  1314720\n",
      "Processed 491200  /  1314720\n",
      "Processed 492800  /  1314720\n",
      "Processed 494400  /  1314720\n",
      "Processed 496000  /  1314720\n",
      "Processed 497600  /  1314720\n",
      "Processed 499200  /  1314720\n",
      "Processed 500800  /  1314720\n",
      "Processed 502400  /  1314720\n",
      "Processed 504000  /  1314720\n",
      "Processed 505600  /  1314720\n",
      "Processed 507200  /  1314720\n",
      "Processed 508800  /  1314720\n",
      "Processed 510400  /  1314720\n",
      "Processed 512000  /  1314720\n",
      "Processed 513600  /  1314720\n",
      "Processed 515200  /  1314720\n",
      "Processed 516800  /  1314720\n",
      "Processed 518400  /  1314720\n",
      "Processed 520000  /  1314720\n",
      "Processed 521600  /  1314720\n",
      "Processed 523200  /  1314720\n",
      "Processed 524800  /  1314720\n",
      "Processed 526400  /  1314720\n",
      "Processed 528000  /  1314720\n",
      "Processed 529600  /  1314720\n",
      "Processed 531200  /  1314720\n",
      "Processed 532800  /  1314720\n",
      "Processed 534400  /  1314720\n",
      "Processed 536000  /  1314720\n",
      "Processed 537600  /  1314720\n",
      "Processed 539200  /  1314720\n",
      "Processed 540800  /  1314720\n",
      "Processed 542400  /  1314720\n",
      "Processed 544000  /  1314720\n",
      "Processed 545600  /  1314720\n",
      "Processed 547200  /  1314720\n",
      "Processed 548800  /  1314720\n",
      "Processed 550400  /  1314720\n",
      "Processed 552000  /  1314720\n",
      "Processed 553600  /  1314720\n",
      "Processed 555200  /  1314720\n",
      "Processed 556800  /  1314720\n",
      "Processed 558400  /  1314720\n",
      "Processed 560000  /  1314720\n",
      "Processed 561600  /  1314720\n",
      "Processed 563200  /  1314720\n",
      "Processed 564800  /  1314720\n",
      "Processed 566400  /  1314720\n",
      "Processed 568000  /  1314720\n",
      "Processed 569600  /  1314720\n",
      "Processed 571200  /  1314720\n",
      "Processed 572800  /  1314720\n",
      "Processed 574400  /  1314720\n",
      "Processed 576000  /  1314720\n",
      "Processed 577600  /  1314720\n",
      "Processed 579200  /  1314720\n",
      "Processed 580800  /  1314720\n",
      "Processed 582400  /  1314720\n",
      "Processed 584000  /  1314720\n",
      "Processed 585600  /  1314720\n",
      "Processed 587200  /  1314720\n",
      "Processed 588800  /  1314720\n",
      "Processed 590400  /  1314720\n",
      "Processed 592000  /  1314720\n",
      "Processed 593600  /  1314720\n",
      "Processed 595200  /  1314720\n",
      "Processed 596800  /  1314720\n",
      "Processed 598400  /  1314720\n",
      "Processed 600000  /  1314720\n",
      "Processed 601600  /  1314720\n",
      "Processed 603200  /  1314720\n",
      "Processed 604800  /  1314720\n",
      "Processed 606400  /  1314720\n",
      "Processed 608000  /  1314720\n",
      "Processed 609600  /  1314720\n",
      "Processed 611200  /  1314720\n",
      "Processed 612800  /  1314720\n",
      "Processed 614400  /  1314720\n",
      "Processed 616000  /  1314720\n",
      "Processed 617600  /  1314720\n",
      "Processed 619200  /  1314720\n",
      "Processed 620800  /  1314720\n",
      "Processed 622400  /  1314720\n",
      "Processed 624000  /  1314720\n",
      "Processed 625600  /  1314720\n",
      "Processed 627200  /  1314720\n",
      "Processed 628800  /  1314720\n",
      "Processed 630400  /  1314720\n",
      "Processed 632000  /  1314720\n",
      "Processed 633600  /  1314720\n",
      "Processed 635200  /  1314720\n",
      "Processed 636800  /  1314720\n",
      "Processed 638400  /  1314720\n",
      "Processed 640000  /  1314720\n",
      "Processed 641600  /  1314720\n",
      "Processed 643200  /  1314720\n",
      "Processed 644800  /  1314720\n",
      "Processed 646400  /  1314720\n",
      "Processed 648000  /  1314720\n",
      "Processed 649600  /  1314720\n",
      "Processed 651200  /  1314720\n",
      "Processed 652800  /  1314720\n",
      "Processed 654400  /  1314720\n",
      "Processed 656000  /  1314720\n",
      "Processed 657600  /  1314720\n",
      "Processed 659200  /  1314720\n",
      "Processed 660800  /  1314720\n",
      "Processed 662400  /  1314720\n",
      "Processed 664000  /  1314720\n",
      "Processed 665600  /  1314720\n",
      "Processed 667200  /  1314720\n",
      "Processed 668800  /  1314720\n",
      "Processed 670400  /  1314720\n",
      "Processed 672000  /  1314720\n",
      "Processed 673600  /  1314720\n",
      "Processed 675200  /  1314720\n",
      "Processed 676800  /  1314720\n",
      "Processed 678400  /  1314720\n",
      "Processed 680000  /  1314720\n",
      "Processed 681600  /  1314720\n",
      "Processed 683200  /  1314720\n",
      "Processed 684800  /  1314720\n",
      "Processed 686400  /  1314720\n",
      "Processed 688000  /  1314720\n",
      "Processed 689600  /  1314720\n",
      "Processed 691200  /  1314720\n",
      "Processed 692800  /  1314720\n",
      "Processed 694400  /  1314720\n",
      "Processed 696000  /  1314720\n",
      "Processed 697600  /  1314720\n",
      "Processed 699200  /  1314720\n",
      "Processed 700800  /  1314720\n",
      "Processed 702400  /  1314720\n",
      "Processed 704000  /  1314720\n",
      "Processed 705600  /  1314720\n",
      "Processed 707200  /  1314720\n",
      "Processed 708800  /  1314720\n",
      "Processed 710400  /  1314720\n",
      "Processed 712000  /  1314720\n",
      "Processed 713600  /  1314720\n",
      "Processed 715200  /  1314720\n",
      "Processed 716800  /  1314720\n",
      "Processed 718400  /  1314720\n",
      "Processed 720000  /  1314720\n",
      "Processed 721600  /  1314720\n",
      "Processed 723200  /  1314720\n",
      "Processed 724800  /  1314720\n",
      "Processed 726400  /  1314720\n",
      "Processed 728000  /  1314720\n",
      "Processed 729600  /  1314720\n",
      "Processed 731200  /  1314720\n",
      "Processed 732800  /  1314720\n",
      "Processed 734400  /  1314720\n",
      "Processed 736000  /  1314720\n",
      "Processed 737600  /  1314720\n",
      "Processed 739200  /  1314720\n",
      "Processed 740800  /  1314720\n",
      "Processed 742400  /  1314720\n",
      "Processed 744000  /  1314720\n",
      "Processed 745600  /  1314720\n",
      "Processed 747200  /  1314720\n",
      "Processed 748800  /  1314720\n",
      "Processed 750400  /  1314720\n",
      "Processed 752000  /  1314720\n",
      "Processed 753600  /  1314720\n",
      "Processed 755200  /  1314720\n",
      "Processed 756800  /  1314720\n",
      "Processed 758400  /  1314720\n",
      "Processed 760000  /  1314720\n",
      "Processed 761600  /  1314720\n",
      "Processed 763200  /  1314720\n",
      "Processed 764800  /  1314720\n",
      "Processed 766400  /  1314720\n",
      "Processed 768000  /  1314720\n",
      "Processed 769600  /  1314720\n",
      "Processed 771200  /  1314720\n",
      "Processed 772800  /  1314720\n",
      "Processed 774400  /  1314720\n",
      "Processed 776000  /  1314720\n",
      "Processed 777600  /  1314720\n",
      "Processed 779200  /  1314720\n",
      "Processed 780800  /  1314720\n",
      "Processed 782400  /  1314720\n",
      "Processed 784000  /  1314720\n",
      "Processed 785600  /  1314720\n",
      "Processed 787200  /  1314720\n",
      "Processed 788800  /  1314720\n",
      "Processed 790400  /  1314720\n",
      "Processed 792000  /  1314720\n",
      "Processed 793600  /  1314720\n",
      "Processed 795200  /  1314720\n",
      "Processed 796800  /  1314720\n",
      "Processed 798400  /  1314720\n",
      "Processed 800000  /  1314720\n",
      "Processed 801600  /  1314720\n",
      "Processed 803200  /  1314720\n",
      "Processed 804800  /  1314720\n",
      "Processed 806400  /  1314720\n",
      "Processed 808000  /  1314720\n",
      "Processed 809600  /  1314720\n",
      "Processed 811200  /  1314720\n",
      "Processed 812800  /  1314720\n",
      "Processed 814400  /  1314720\n",
      "Processed 816000  /  1314720\n",
      "Processed 817600  /  1314720\n",
      "Processed 819200  /  1314720\n",
      "Processed 820800  /  1314720\n",
      "Processed 822400  /  1314720\n",
      "Processed 824000  /  1314720\n",
      "Processed 825600  /  1314720\n",
      "Processed 827200  /  1314720\n",
      "Processed 828800  /  1314720\n",
      "Processed 830400  /  1314720\n",
      "Processed 832000  /  1314720\n",
      "Processed 833600  /  1314720\n",
      "Processed 835200  /  1314720\n",
      "Processed 836800  /  1314720\n",
      "Processed 838400  /  1314720\n",
      "Processed 840000  /  1314720\n",
      "Processed 841600  /  1314720\n",
      "Processed 843200  /  1314720\n",
      "Processed 844800  /  1314720\n",
      "Processed 846400  /  1314720\n",
      "Processed 848000  /  1314720\n",
      "Processed 849600  /  1314720\n",
      "Processed 851200  /  1314720\n",
      "Processed 852800  /  1314720\n",
      "Processed 854400  /  1314720\n",
      "Processed 856000  /  1314720\n",
      "Processed 857600  /  1314720\n",
      "Processed 859200  /  1314720\n",
      "Processed 860800  /  1314720\n",
      "Processed 862400  /  1314720\n",
      "Processed 864000  /  1314720\n",
      "Processed 865600  /  1314720\n",
      "Processed 867200  /  1314720\n",
      "Processed 868800  /  1314720\n",
      "Processed 870400  /  1314720\n",
      "Processed 872000  /  1314720\n",
      "Processed 873600  /  1314720\n",
      "Processed 875200  /  1314720\n",
      "Processed 876800  /  1314720\n",
      "Processed 878400  /  1314720\n",
      "Processed 880000  /  1314720\n",
      "Processed 881600  /  1314720\n",
      "Processed 883200  /  1314720\n",
      "Processed 884800  /  1314720\n",
      "Processed 886400  /  1314720\n",
      "Processed 888000  /  1314720\n",
      "Processed 889600  /  1314720\n",
      "Processed 891200  /  1314720\n",
      "Processed 892800  /  1314720\n",
      "Processed 894400  /  1314720\n",
      "Processed 896000  /  1314720\n",
      "Processed 897600  /  1314720\n",
      "Processed 899200  /  1314720\n",
      "Processed 900800  /  1314720\n",
      "Processed 902400  /  1314720\n",
      "Processed 904000  /  1314720\n",
      "Processed 905600  /  1314720\n",
      "Processed 907200  /  1314720\n",
      "Processed 908800  /  1314720\n",
      "Processed 910400  /  1314720\n",
      "Processed 912000  /  1314720\n",
      "Processed 913600  /  1314720\n",
      "Processed 915200  /  1314720\n",
      "Processed 916800  /  1314720\n",
      "Processed 918400  /  1314720\n",
      "Processed 920000  /  1314720\n",
      "Processed 921600  /  1314720\n",
      "Processed 923200  /  1314720\n",
      "Processed 924800  /  1314720\n",
      "Processed 926400  /  1314720\n",
      "Processed 928000  /  1314720\n",
      "Processed 929600  /  1314720\n",
      "Processed 931200  /  1314720\n",
      "Processed 932800  /  1314720\n",
      "Processed 934400  /  1314720\n",
      "Processed 936000  /  1314720\n",
      "Processed 937600  /  1314720\n",
      "Processed 939200  /  1314720\n",
      "Processed 940800  /  1314720\n",
      "Processed 942400  /  1314720\n",
      "Processed 944000  /  1314720\n",
      "Processed 945600  /  1314720\n",
      "Processed 947200  /  1314720\n",
      "Processed 948800  /  1314720\n",
      "Processed 950400  /  1314720\n",
      "Processed 952000  /  1314720\n",
      "Processed 953600  /  1314720\n",
      "Processed 955200  /  1314720\n",
      "Processed 956800  /  1314720\n",
      "Processed 958400  /  1314720\n",
      "Processed 960000  /  1314720\n",
      "Processed 961600  /  1314720\n",
      "Processed 963200  /  1314720\n",
      "Processed 964800  /  1314720\n",
      "Processed 966400  /  1314720\n",
      "Processed 968000  /  1314720\n",
      "Processed 969600  /  1314720\n",
      "Processed 971200  /  1314720\n",
      "Processed 972800  /  1314720\n",
      "Processed 974400  /  1314720\n",
      "Processed 976000  /  1314720\n",
      "Processed 977600  /  1314720\n",
      "Processed 979200  /  1314720\n",
      "Processed 980800  /  1314720\n",
      "Processed 982400  /  1314720\n",
      "Processed 984000  /  1314720\n",
      "Processed 985600  /  1314720\n",
      "Processed 987200  /  1314720\n",
      "Processed 988800  /  1314720\n",
      "Processed 990400  /  1314720\n",
      "Processed 992000  /  1314720\n",
      "Processed 993600  /  1314720\n",
      "Processed 995200  /  1314720\n",
      "Processed 996800  /  1314720\n",
      "Processed 998400  /  1314720\n",
      "Processed 1000000  /  1314720\n",
      "Processed 1001600  /  1314720\n",
      "Processed 1003200  /  1314720\n",
      "Processed 1004800  /  1314720\n",
      "Processed 1006400  /  1314720\n",
      "Processed 1008000  /  1314720\n",
      "Processed 1009600  /  1314720\n",
      "Processed 1011200  /  1314720\n",
      "Processed 1012800  /  1314720\n",
      "Processed 1014400  /  1314720\n",
      "Processed 1016000  /  1314720\n",
      "Processed 1017600  /  1314720\n",
      "Processed 1019200  /  1314720\n",
      "Processed 1020800  /  1314720\n",
      "Processed 1022400  /  1314720\n",
      "Processed 1024000  /  1314720\n",
      "Processed 1025600  /  1314720\n",
      "Processed 1027200  /  1314720\n",
      "Processed 1028800  /  1314720\n",
      "Processed 1030400  /  1314720\n",
      "Processed 1032000  /  1314720\n",
      "Processed 1033600  /  1314720\n",
      "Processed 1035200  /  1314720\n",
      "Processed 1036800  /  1314720\n",
      "Processed 1038400  /  1314720\n",
      "Processed 1040000  /  1314720\n",
      "Processed 1041600  /  1314720\n",
      "Processed 1043200  /  1314720\n",
      "Processed 1044800  /  1314720\n",
      "Processed 1046400  /  1314720\n",
      "Processed 1048000  /  1314720\n",
      "Processed 1049600  /  1314720\n",
      "Processed 1051200  /  1314720\n",
      "Processed 1052800  /  1314720\n",
      "Processed 1054400  /  1314720\n",
      "Processed 1056000  /  1314720\n",
      "Processed 1057600  /  1314720\n",
      "Processed 1059200  /  1314720\n",
      "Processed 1060800  /  1314720\n",
      "Processed 1062400  /  1314720\n",
      "Processed 1064000  /  1314720\n",
      "Processed 1065600  /  1314720\n",
      "Processed 1067200  /  1314720\n",
      "Processed 1068800  /  1314720\n",
      "Processed 1070400  /  1314720\n",
      "Processed 1072000  /  1314720\n",
      "Processed 1073600  /  1314720\n",
      "Processed 1075200  /  1314720\n",
      "Processed 1076800  /  1314720\n",
      "Processed 1078400  /  1314720\n",
      "Processed 1080000  /  1314720\n",
      "Processed 1081600  /  1314720\n",
      "Processed 1083200  /  1314720\n",
      "Processed 1084800  /  1314720\n",
      "Processed 1086400  /  1314720\n",
      "Processed 1088000  /  1314720\n",
      "Processed 1089600  /  1314720\n",
      "Processed 1091200  /  1314720\n",
      "Processed 1092800  /  1314720\n",
      "Processed 1094400  /  1314720\n",
      "Processed 1096000  /  1314720\n",
      "Processed 1097600  /  1314720\n",
      "Processed 1099200  /  1314720\n",
      "Processed 1100800  /  1314720\n",
      "Processed 1102400  /  1314720\n",
      "Processed 1104000  /  1314720\n",
      "Processed 1105600  /  1314720\n",
      "Processed 1107200  /  1314720\n",
      "Processed 1108800  /  1314720\n",
      "Processed 1110400  /  1314720\n",
      "Processed 1112000  /  1314720\n",
      "Processed 1113600  /  1314720\n",
      "Processed 1115200  /  1314720\n",
      "Processed 1116800  /  1314720\n",
      "Processed 1118400  /  1314720\n",
      "Processed 1120000  /  1314720\n",
      "Processed 1121600  /  1314720\n",
      "Processed 1123200  /  1314720\n",
      "Processed 1124800  /  1314720\n",
      "Processed 1126400  /  1314720\n",
      "Processed 1128000  /  1314720\n",
      "Processed 1129600  /  1314720\n",
      "Processed 1131200  /  1314720\n",
      "Processed 1132800  /  1314720\n",
      "Processed 1134400  /  1314720\n",
      "Processed 1136000  /  1314720\n",
      "Processed 1137600  /  1314720\n",
      "Processed 1139200  /  1314720\n",
      "Processed 1140800  /  1314720\n",
      "Processed 1142400  /  1314720\n",
      "Processed 1144000  /  1314720\n",
      "Processed 1145600  /  1314720\n",
      "Processed 1147200  /  1314720\n",
      "Processed 1148800  /  1314720\n",
      "Processed 1150400  /  1314720\n",
      "Processed 1152000  /  1314720\n",
      "Processed 1153600  /  1314720\n",
      "Processed 1155200  /  1314720\n",
      "Processed 1156800  /  1314720\n",
      "Processed 1158400  /  1314720\n",
      "Processed 1160000  /  1314720\n",
      "Processed 1161600  /  1314720\n",
      "Processed 1163200  /  1314720\n",
      "Processed 1164800  /  1314720\n",
      "Processed 1166400  /  1314720\n",
      "Processed 1168000  /  1314720\n",
      "Processed 1169600  /  1314720\n",
      "Processed 1171200  /  1314720\n",
      "Processed 1172800  /  1314720\n",
      "Processed 1174400  /  1314720\n",
      "Processed 1176000  /  1314720\n",
      "Processed 1177600  /  1314720\n",
      "Processed 1179200  /  1314720\n",
      "Processed 1180800  /  1314720\n",
      "Processed 1182400  /  1314720\n",
      "Processed 1184000  /  1314720\n",
      "Processed 1185600  /  1314720\n",
      "Processed 1187200  /  1314720\n",
      "Processed 1188800  /  1314720\n",
      "Processed 1190400  /  1314720\n",
      "Processed 1192000  /  1314720\n",
      "Processed 1193600  /  1314720\n",
      "Processed 1195200  /  1314720\n",
      "Processed 1196800  /  1314720\n",
      "Processed 1198400  /  1314720\n",
      "Processed 1200000  /  1314720\n",
      "Processed 1201600  /  1314720\n",
      "Processed 1203200  /  1314720\n",
      "Processed 1204800  /  1314720\n",
      "Processed 1206400  /  1314720\n",
      "Processed 1208000  /  1314720\n",
      "Processed 1209600  /  1314720\n",
      "Processed 1211200  /  1314720\n",
      "Processed 1212800  /  1314720\n",
      "Processed 1214400  /  1314720\n",
      "Processed 1216000  /  1314720\n",
      "Processed 1217600  /  1314720\n",
      "Processed 1219200  /  1314720\n",
      "Processed 1220800  /  1314720\n",
      "Processed 1222400  /  1314720\n",
      "Processed 1224000  /  1314720\n",
      "Processed 1225600  /  1314720\n",
      "Processed 1227200  /  1314720\n",
      "Processed 1228800  /  1314720\n",
      "Processed 1230400  /  1314720\n",
      "Processed 1232000  /  1314720\n",
      "Processed 1233600  /  1314720\n",
      "Processed 1235200  /  1314720\n",
      "Processed 1236800  /  1314720\n",
      "Processed 1238400  /  1314720\n",
      "Processed 1240000  /  1314720\n",
      "Processed 1241600  /  1314720\n",
      "Processed 1243200  /  1314720\n",
      "Processed 1244800  /  1314720\n",
      "Processed 1246400  /  1314720\n",
      "Processed 1248000  /  1314720\n",
      "Processed 1249600  /  1314720\n",
      "Processed 1251200  /  1314720\n",
      "Processed 1252800  /  1314720\n",
      "Processed 1254400  /  1314720\n",
      "Processed 1256000  /  1314720\n",
      "Processed 1257600  /  1314720\n",
      "Processed 1259200  /  1314720\n",
      "Processed 1260800  /  1314720\n",
      "Processed 1262400  /  1314720\n",
      "Processed 1264000  /  1314720\n",
      "Processed 1265600  /  1314720\n",
      "Processed 1267200  /  1314720\n",
      "Processed 1268800  /  1314720\n",
      "Processed 1270400  /  1314720\n",
      "Processed 1272000  /  1314720\n",
      "Processed 1273600  /  1314720\n",
      "Processed 1275200  /  1314720\n",
      "Processed 1276800  /  1314720\n",
      "Processed 1278400  /  1314720\n",
      "Processed 1280000  /  1314720\n",
      "Processed 1281600  /  1314720\n",
      "Processed 1283200  /  1314720\n",
      "Processed 1284800  /  1314720\n",
      "Processed 1286400  /  1314720\n",
      "Processed 1288000  /  1314720\n",
      "Processed 1289600  /  1314720\n",
      "Processed 1291200  /  1314720\n",
      "Processed 1292800  /  1314720\n",
      "Processed 1294400  /  1314720\n",
      "Processed 1296000  /  1314720\n",
      "Processed 1297600  /  1314720\n",
      "Processed 1299200  /  1314720\n",
      "Processed 1300800  /  1314720\n",
      "Processed 1302400  /  1314720\n",
      "Processed 1304000  /  1314720\n",
      "Processed 1305600  /  1314720\n",
      "Processed 1307200  /  1314720\n",
      "Processed 1308800  /  1314720\n",
      "Processed 1310400  /  1314720\n",
      "Processed 1312000  /  1314720\n",
      "Processed 1313600  /  1314720\n",
      "Saved pseudo-labels to artifacts\\emotions_pseudo_all.parquet\n",
      "Pseudo-labels shape: (1314720, 28)\n"
     ]
    }
   ],
   "source": [
    "# Snippet : build_pseudo_labels_full (optionnel, long)\n",
    "from pathlib import Path\n",
    "import pandas as pd, os\n",
    "from transformers import pipeline\n",
    "ART = Path(\"artifacts\")\n",
    "OUT_PSEUDO = ART / \"emotions_pseudo_all.parquet\"\n",
    "TEXT_COL = \"review_body\"\n",
    "DEVICE = 0 if (USE_GPU and __import__(\"torch\").cuda.is_available()) else -1\n",
    "BATCH = 32\n",
    "\n",
    "if OUT_PSEUDO.exists():\n",
    "    print(\"Pseudo-labels file exists:\", OUT_PSEUDO)\n",
    "    emo_df = pd.read_parquet(OUT_PSEUDO)\n",
    "else:\n",
    "    emo_pipe = pipeline(\"text-classification\", model=\"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "                        return_all_scores=True, device=DEVICE)\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(0, n, BATCH):\n",
    "        batch_texts = df[TEXT_COL].astype(str).iloc[i:i+BATCH].tolist()\n",
    "        out = emo_pipe(batch_texts, truncation=True, batch_size=BATCH)\n",
    "        for scores in out:\n",
    "            rows.append({f\"emo_{s['label']}\": s[\"score\"] for s in scores})\n",
    "        if (i // BATCH) % 50 == 0:\n",
    "            print(\"Processed\", i, \" / \", n)\n",
    "    emo_df = pd.DataFrame(rows).fillna(0.0)\n",
    "    emo_df.to_parquet(OUT_PSEUDO)\n",
    "    print(\"Saved pseudo-labels to\", OUT_PSEUDO)\n",
    "\n",
    "# expose\n",
    "globals()[\"emo_pseudo_df_all\"] = emo_df\n",
    "print(\"Pseudo-labels shape:\", emo_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6399abd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Diagnostics variables numpy/pandas en m√©moire (taille estim√©e) ===\n",
      "[('X_all', 'ndarray', (1314720, 384), 2019409920),\n",
      " ('X_all_emb', 'ndarray', (1314720, 384), 2019409920),\n",
      " ('X_mm', 'memmap', (1314720, 384), 2019409920),\n",
      " ('X_temp', 'ndarray', (1183248, 384), 1817468928),\n",
      " ('Xtr_s', 'ndarray', (1051789, 384), 1615547904),\n",
      " ('X_train', 'ndarray', (1051789, 384), 1615547904),\n",
      " ('X_tr', 'csr_matrix', (96000, 180007), 437043120),\n",
      " ('Y_all', 'ndarray', (1314720, 25), 262944000),\n",
      " ('Y_temp', 'ndarray', (1183248, 25), 236649600),\n",
      " ('Y_train', 'ndarray', (1051789, 25), 210357800),\n",
      " ('Xte_s', 'ndarray', (131472, 384), 201940992),\n",
      " ('X_test', 'ndarray', (131472, 384), 201940992),\n",
      " ('Xva_s', 'ndarray', (131459, 384), 201921024),\n",
      " ('X_val', 'ndarray', (131459, 384), 201921024),\n",
      " ('X_tr_', 'csr_matrix', (96000, 50000), 146848124),\n",
      " ('X_tmp', 'csr_matrix', (24000, 180007), 109848072),\n",
      " ('X_te', 'csr_matrix', (12000, 180007), 55617728),\n",
      " ('X_va', 'csr_matrix', (12000, 180007), 54230344),\n",
      " ('X_te_', 'csr_matrix', (12000, 50000), 37240352),\n",
      " ('Y_test', 'ndarray', (131472, 25), 26294400),\n",
      " ('Y_val', 'ndarray', (131459, 25), 26291800),\n",
      " ('X_tr_emb', 'ndarray', (16000, 384), 24576000),\n",
      " ('X_all_s', 'ndarray', (16000, 384), 24576000),\n",
      " ('Xtr', 'ndarray', (12800, 384), 19660800),\n",
      " ('X_va_', 'csr_matrix', (12000, 50000), 18214740),\n",
      " ('X_train_cv', 'ndarray', (10240, 384), 15728640),\n",
      " ('X_te_emb', 'ndarray', (4000, 384), 6144000),\n",
      " ('Xte', 'ndarray', (4000, 384), 6144000),\n",
      " ('Xva', 'ndarray', (3200, 384), 4915200),\n",
      " ('Y', 'ndarray', (20000, 28), 4480000),\n",
      " ('X_val_cv', 'ndarray', (2560, 384), 3932160),\n",
      " ('Y_tr', 'ndarray', (16000, 28), 3584000),\n",
      " ('Ytr_k', 'ndarray', (16000, 10), 1280000),\n",
      " ('Ytr', 'ndarray', (12800, 10), 1024000),\n",
      " ('Y_te', 'ndarray', (4000, 28), 896000),\n",
      " ('Y_train_cv', 'ndarray', (10240, 10), 819200),\n",
      " ('proba_te', 'ndarray', (4000, 10), 320000),\n",
      " ('Yte_k', 'ndarray', (4000, 10), 320000),\n",
      " ('proba', 'ndarray', (4000, 10), 320000),\n",
      " ('pred', 'ndarray', (4000, 10), 320000)]\n",
      "\n",
      "=== M√©moire Python utilis√©e (approx) ===\n",
      "RSS (MB): 2263.34375\n",
      "\n",
      "Candidates to free (variables present): ['X_train', 'X_val', 'X_test', 'X_all_emb', 'Xtr_s', 'Xva_s', 'Xte_s', 'X_mm', 'X_all']\n"
     ]
    }
   ],
   "source": [
    "# Snippet 1 ‚Äî Diagnostic m√©moire & tentative safe free\n",
    "import gc, sys, os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== Diagnostics variables numpy/pandas en m√©moire (taille estim√©e) ===\")\n",
    "vars_info = []\n",
    "for name, val in list(globals().items()):\n",
    "    try:\n",
    "        if hasattr(val, \"shape\") and hasattr(val, \"dtype\"):\n",
    "            nbytes = getattr(val, \"nbytes\", None)\n",
    "            if nbytes is None:\n",
    "                nbytes = val.size * np.dtype(val.dtype).itemsize\n",
    "            vars_info.append((name, type(val).__name__, getattr(val, \"shape\", None), nbytes))\n",
    "        elif hasattr(val, \"__len__\") and (name.startswith(\"X\") or name.startswith(\"Y\") or name.endswith(\"_emb\")):\n",
    "            # rough fallback\n",
    "            try:\n",
    "                length = len(val)\n",
    "                vars_info.append((name, type(val).__name__, f\"len={length}\", None))\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# sort by size (None last)\n",
    "vars_info = sorted(vars_info, key=lambda x: (x[3] is None, -x[3] if x[3] is not None else 0))\n",
    "pprint(vars_info[:40])\n",
    "\n",
    "print(\"\\n=== M√©moire Python utilis√©e (approx) ===\")\n",
    "try:\n",
    "    import psutil\n",
    "    p = psutil.Process(os.getpid())\n",
    "    print(\"RSS (MB):\", p.memory_info().rss / 1024**2)\n",
    "except Exception:\n",
    "    print(\"psutil non dispo ‚Äî ignorer\")\n",
    "\n",
    "# Safe cleanup suggestions (ne supprime pas automatiquement sauf si tu confirmes)\n",
    "candidates = ['X_train','X_val','X_test','X_all_emb','Xtr_s','Xva_s','Xte_s','X_mm','mm','X_scaled','X_all','X_all_shallow']\n",
    "to_remove = []\n",
    "for c in candidates:\n",
    "    if c in globals():\n",
    "        to_remove.append(c)\n",
    "print(\"\\nCandidates to free (variables present):\", to_remove)\n",
    "\n",
    "# If you want to free them now (uncomment next lines). I keep them commented to avoid accidental deletion.\n",
    "# for c in to_remove:\n",
    "#     try:\n",
    "#         del globals()[c]\n",
    "#         print(\"Deleted\", c)\n",
    "#     except Exception as e:\n",
    "#         print(\"Failed to delete\", c, e)\n",
    "# gc.collect()\n",
    "# print(\"Freed memory; current RSS (MB):\", psutil.Process(os.getpid()).memory_info().rss/1024**2 if 'psutil' in globals() else \"n/a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "debc2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG -> √©tats actuels (pr√©sence des objets cl√©s) :\n",
      "  emo_df in globals? True\n",
      "  emo_pseudo_df_all in globals? True\n",
      "  LABELS_EMO in globals? True\n",
      "  Y_all in globals? True\n",
      "  X_mm memmap in globals? True\n",
      "\n",
      "Y_all.shape = (1314720, 25) -> n_labels = 25\n",
      "Found emo_df.columns (len): 28\n",
      "Found emo_pseudo_df_all.columns (len): 28\n",
      "Found LABELS_EMO (len): 28\n",
      "\n",
      "Candidate label sources and lengths:\n",
      " - emo_df.columns -> 28\n",
      " - emo_pseudo_df_all.columns -> 28\n",
      " - LABELS_EMO -> 28\n",
      "\n",
      "No exact-length candidate. Best candidate by length: emo_df.columns (len=28, diff=3).\n",
      "Auto-truncating emo_df.columns to first 25 items to match Y_all. (Verify later!)\n",
      "\n",
      "Final label_names length: 25\n",
      "Example label_names[:20]: ['emo_admiration', 'emo_amusement', 'emo_anger', 'emo_annoyance', 'emo_approval', 'emo_caring', 'emo_confusion', 'emo_curiosity', 'emo_desire', 'emo_disappointment', 'emo_disapproval', 'emo_disgust', 'emo_embarrassment', 'emo_excitement', 'emo_fear', 'emo_gratitude', 'emo_grief', 'emo_joy', 'emo_love', 'emo_nervousness']\n",
      "\n",
      "label_support sample (first 20): [  564   623    52  3727  2382   867  2579   739  2297 28397   508    46\n",
      "  1229    32  8815    71   692   264   110  1019]\n",
      "keep_mask sum: 25 of 25\n",
      "\n",
      "After applying keep_mask -> kept labels: 25\n",
      "Examples kept label names[:20]: [np.str_('emo_admiration'), np.str_('emo_amusement'), np.str_('emo_anger'), np.str_('emo_annoyance'), np.str_('emo_approval'), np.str_('emo_caring'), np.str_('emo_confusion'), np.str_('emo_curiosity'), np.str_('emo_desire'), np.str_('emo_disappointment'), np.str_('emo_disapproval'), np.str_('emo_disgust'), np.str_('emo_embarrassment'), np.str_('emo_excitement'), np.str_('emo_fear'), np.str_('emo_gratitude'), np.str_('emo_grief'), np.str_('emo_joy'), np.str_('emo_love'), np.str_('emo_nervousness')]\n",
      "Y_all_filtered.shape: (1314720, 25)\n",
      "\n",
      "Saved mapping -> artifacts/label_names_resolved.joblib\n",
      "\n",
      "Termin√© ‚Äî v√©rifie les noms d'√©tiquettes conserv√©s (label_names_kept) avant de poursuivre.\n"
     ]
    }
   ],
   "source": [
    "# Snippet: r√©parer/aligne label_names vs Y_all sans relancer les gros calculs\n",
    "import os, joblib, json, numpy as np, pandas as pd\n",
    "ART = \"artifacts\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "print(\"DEBUG -> √©tats actuels (pr√©sence des objets cl√©s) :\")\n",
    "print(\"  emo_df in globals?\", 'emo_df' in globals())\n",
    "print(\"  emo_pseudo_df_all in globals?\", 'emo_pseudo_df_all' in globals())\n",
    "print(\"  LABELS_EMO in globals?\", 'LABELS_EMO' in globals())\n",
    "print(\"  Y_all in globals?\", 'Y_all' in globals())\n",
    "print(\"  X_mm memmap in globals?\", 'X_mm' in globals())\n",
    "print()\n",
    "\n",
    "# require Y_all\n",
    "if 'Y_all' not in globals():\n",
    "    raise RuntimeError(\"Y_all absent. Il faut reconstruire Y_all ou recharger les pseudo-labels (mais pas les r√©-encoder).\")\n",
    "\n",
    "Y_all = globals()['Y_all']\n",
    "n_labels_Y = Y_all.shape[1]\n",
    "print(f\"Y_all.shape = {Y_all.shape} -> n_labels = {n_labels_Y}\")\n",
    "\n",
    "# collect candidate sources for label names\n",
    "candidates = []\n",
    "\n",
    "# 1) emo_df.columns (if exists)\n",
    "if 'emo_df' in globals():\n",
    "    try:\n",
    "        cols = list(globals()['emo_df'].columns)\n",
    "        candidates.append((\"emo_df.columns\", cols))\n",
    "        print(\"Found emo_df.columns (len):\", len(cols))\n",
    "    except Exception as e:\n",
    "        print(\"Could not read emo_df.columns:\", e)\n",
    "\n",
    "# 2) emo_pseudo_df_all (var name used earlier)\n",
    "if 'emo_pseudo_df_all' in globals():\n",
    "    try:\n",
    "        cols = list(globals()['emo_pseudo_df_all'].columns)\n",
    "        candidates.append((\"emo_pseudo_df_all.columns\", cols))\n",
    "        print(\"Found emo_pseudo_df_all.columns (len):\", len(cols))\n",
    "    except Exception as e:\n",
    "        print(\"Could not read emo_pseudo_df_all.columns:\", e)\n",
    "\n",
    "# 3) LABELS_EMO list\n",
    "if 'LABELS_EMO' in globals():\n",
    "    try:\n",
    "        cols = list(globals()['LABELS_EMO'])\n",
    "        candidates.append((\"LABELS_EMO\", cols))\n",
    "        print(\"Found LABELS_EMO (len):\", len(cols))\n",
    "    except Exception as e:\n",
    "        print(\"Could not read LABELS_EMO:\", e)\n",
    "\n",
    "# 4) artifacts: try known artifacts that may contain names or meta\n",
    "files_checked = []\n",
    "# split_and_labels_meta.joblib\n",
    "meta_path = os.path.join(ART, \"split_and_labels_meta.joblib\")\n",
    "if os.path.exists(meta_path):\n",
    "    try:\n",
    "        meta = joblib.load(meta_path)\n",
    "        if \"label_names\" in meta:\n",
    "            candidates.append((\"artifacts/split_and_labels_meta.label_names\", list(meta[\"label_names\"])))\n",
    "            print(\"Loaded label_names from split_and_labels_meta.joblib (len):\", len(meta[\"label_names\"]))\n",
    "        files_checked.append(meta_path)\n",
    "    except Exception as e:\n",
    "        print(\"Could not load split_and_labels_meta.joblib:\", e)\n",
    "# emo_label_names.json\n",
    "json_path = os.path.join(ART, \"emo_label_names.json\")\n",
    "if os.path.exists(json_path):\n",
    "    try:\n",
    "        cols = json.load(open(json_path, \"r\", encoding=\"utf8\"))\n",
    "        candidates.append((\"artifacts/emo_label_names.json\", list(cols)))\n",
    "        print(\"Loaded emo_label_names.json (len):\", len(cols))\n",
    "        files_checked.append(json_path)\n",
    "    except Exception as e:\n",
    "        print(\"Could not load emo_label_names.json:\", e)\n",
    "\n",
    "# 5) label_names_repaired or label_names_and_mask_debug\n",
    "for name in [\"artifacts/label_names_repaired.joblib\", \"artifacts/label_names_and_mask_debug.joblib\", \"artifacts/label_names_resolved.joblib\"]:\n",
    "    if os.path.exists(name):\n",
    "        try:\n",
    "            data = joblib.load(name)\n",
    "            if isinstance(data, dict) and \"label_names\" in data:\n",
    "                candidates.append((name, list(data[\"label_names\"])))\n",
    "                print(\"Loaded label_names from\", name, \"len:\", len(data[\"label_names\"]))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Diagnostic printout of candidates and lengths\n",
    "print(\"\\nCandidate label sources and lengths:\")\n",
    "for src, cols in candidates:\n",
    "    print(\" -\", src, \"->\", len(cols))\n",
    "\n",
    "# Try to find a candidate with matching length\n",
    "chosen = None\n",
    "for src, cols in candidates:\n",
    "    if len(cols) == n_labels_Y:\n",
    "        chosen = (src, cols)\n",
    "        print(\"\\nSelected candidate (exact length match):\", src)\n",
    "        break\n",
    "\n",
    "# If none exact match, try to find best match via substring intersection heuristics\n",
    "if chosen is None and candidates:\n",
    "    # pick candidate with minimal absolute length difference\n",
    "    diffs = [(src, cols, abs(len(cols) - n_labels_Y)) for src, cols in candidates]\n",
    "    diffs_sorted = sorted(diffs, key=lambda x: x[2])\n",
    "    src0, cols0, diff0 = diffs_sorted[0]\n",
    "    print(f\"\\nNo exact-length candidate. Best candidate by length: {src0} (len={len(cols0)}, diff={diff0}).\")\n",
    "    # If diff small (e.g., <=3) offer to auto-truncate/align; otherwise fallback to generic names\n",
    "    if diff0 <= 3:\n",
    "        # Warn and take first n_labels_Y from that candidate\n",
    "        print(f\"Auto-truncating {src0} to first {n_labels_Y} items to match Y_all. (Verify later!)\")\n",
    "        chosen = (src0, cols0[:n_labels_Y])\n",
    "    else:\n",
    "        print(\"Difference too large ‚Äî will not truncate automatically. Will create generic labels.\")\n",
    "\n",
    "# If still no chosen, fallback to generic labels\n",
    "if chosen is None:\n",
    "    print(\"\\nNo usable candidate label names found. Generating generic labels 'label_0..label_{n-1}'.\")\n",
    "    cols_gen = [f\"label_{i}\" for i in range(n_labels_Y)]\n",
    "    chosen = (\"generated_generic\", cols_gen)\n",
    "\n",
    "# Apply chosen labels\n",
    "label_names = list(chosen[1])\n",
    "print(\"\\nFinal label_names length:\", len(label_names))\n",
    "print(\"Example label_names[:20]:\", label_names[:20])\n",
    "\n",
    "# Recompute keep_mask safely based on current Y_all (if missing)\n",
    "label_support = Y_all.sum(axis=0).astype(int)\n",
    "keep_mask = (label_support >= globals().get(\"MIN_SUPPORT_LABEL\", 30))\n",
    "print(\"\\nlabel_support sample (first 20):\", label_support[:20])\n",
    "print(\"keep_mask sum:\", keep_mask.sum(), \"of\", len(keep_mask))\n",
    "\n",
    "# If keep_mask length differs from label_names length, align/truncate both to min length (safe)\n",
    "if len(keep_mask) != len(label_names):\n",
    "    print(\"Length mismatch between keep_mask and label_names; truncating to min length.\")\n",
    "    m = min(len(keep_mask), len(label_names))\n",
    "    keep_mask = keep_mask[:m]\n",
    "    label_names = label_names[:m]\n",
    "    Y_all = Y_all[:, :m]\n",
    "    print(\"Now Y_all.shape:\", Y_all.shape)\n",
    "\n",
    "# Apply keep_mask to produce Y_all_filtered and label_names_kept\n",
    "label_names_kept = list(np.array(label_names)[keep_mask])\n",
    "Y_all_filtered = Y_all[:, keep_mask]\n",
    "print(\"\\nAfter applying keep_mask -> kept labels:\", len(label_names_kept))\n",
    "print(\"Examples kept label names[:20]:\", label_names_kept[:20])\n",
    "print(\"Y_all_filtered.shape:\", Y_all_filtered.shape)\n",
    "\n",
    "# Save resolved mapping for later reuse\n",
    "joblib.dump({\n",
    "    \"resolved_source\": chosen[0],\n",
    "    \"label_names_full\": label_names,\n",
    "    \"keep_mask\": keep_mask.tolist(),\n",
    "    \"label_names_kept\": label_names_kept\n",
    "}, os.path.join(ART, \"label_names_resolved.joblib\"))\n",
    "print(\"\\nSaved mapping -> artifacts/label_names_resolved.joblib\")\n",
    "\n",
    "# Export variables to globals for downstream code\n",
    "globals()['label_names'] = label_names\n",
    "globals()['label_names_kept'] = label_names_kept\n",
    "globals()['keep_mask'] = keep_mask\n",
    "globals()['Y_all'] = Y_all\n",
    "globals()['Y_all_filtered'] = Y_all_filtered\n",
    "\n",
    "print(\"\\nTermin√© ‚Äî v√©rifie les noms d'√©tiquettes conserv√©s (label_names_kept) avant de poursuivre.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5859efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted variable 'models'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4322"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup des mod√®les partiels potentiellement invalides\n",
    "import gc\n",
    "if 'models' in globals():\n",
    "    try:\n",
    "        del globals()['models']\n",
    "        print(\"Deleted variable 'models'\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not delete 'models':\", e)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7696e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mm loaded shape: (1314720, 384)\n",
      "Y_all.shape: (1314720, 25)\n",
      "Loaded idx splits from artifacts.\n",
      "n_labels after filter: 25\n",
      "Example pos_weight: [100.         100.         100.         100.         100.\n",
      " 100.         100.         100.         100.          45.41610768]\n",
      "\n",
      "=== EPOCH 1/2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [02:53<00:00, 6057.65rows/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set ...\n",
      "Epoch 1 ‚Äî AP_macro on val: 0.2754\n",
      "Checkpoint saved for epoch 1\n",
      "\n",
      "=== EPOCH 2/2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:28<00:00, 37401.01rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set ...\n",
      "Epoch 2 ‚Äî AP_macro on val: 0.2843\n",
      "Checkpoint saved for epoch 2\n",
      "\n",
      "Training completed in 205.0s. Saving final models ...\n",
      "Saved models and split metadata.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Partial-fit multi-label training (SGDClassifier) via memmap + mini-batches (CORRIG√â) ===\n",
    "import os, math, time, joblib, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ART = Path(\"artifacts\"); ART.mkdir(exist_ok=True)\n",
    "# Hyperparams (ajuste si besoin)\n",
    "BATCH_SIZE = 8192\n",
    "N_EPOCHS = 2\n",
    "MIN_SUPPORT_LABEL = globals().get(\"MIN_SUPPORT_LABEL\", 30)\n",
    "TH_EMO = globals().get(\"TH_EMO\", 0.30)\n",
    "ALPHA = 1e-4\n",
    "RNG = 42\n",
    "SAVE_EVERY_EPOCH = True\n",
    "\n",
    "# 1) ouvrir memmap\n",
    "memfile = ART / \"X_all_memmap.dat\"\n",
    "meta_path = ART / \"X_all_memmap_meta.joblib\"\n",
    "assert memfile.exists() and meta_path.exists(), \"Memmap ou meta absents. Cr√©e X_all_memmap avant.\"\n",
    "meta = joblib.load(meta_path)\n",
    "n, d = int(meta[\"n\"]), int(meta[\"d\"])\n",
    "X_mm = np.memmap(str(memfile), dtype=\"float32\", mode=\"r\", shape=(n,d))\n",
    "print(\"X_mm loaded shape:\", X_mm.shape)\n",
    "\n",
    "# 2) r√©cup√©rer Y_all_filtered et label_names_kept\n",
    "assert 'Y_all_filtered' in globals(), \"Y_all_filtered absent. Ex√©cute le snippet de r√©solution des labels.\"\n",
    "Y_all = globals()['Y_all_filtered']\n",
    "label_names = globals().get('label_names_kept', [f\"label_{i}\" for i in range(Y_all.shape[1])])\n",
    "print(\"Y_all.shape:\", Y_all.shape)\n",
    "\n",
    "# align rows if needed\n",
    "if X_mm.shape[0] != Y_all.shape[0]:\n",
    "    if Y_all.shape[0] > X_mm.shape[0]:\n",
    "        Y_all = Y_all[:X_mm.shape[0]]\n",
    "        print(\"Trimmed Y_all to match X_mm rows.\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Mismatch rows X_mm vs Y_all (Y shorter) ‚Äî aligne les donn√©es.\")\n",
    "globals()['Y_all_filtered'] = Y_all\n",
    "\n",
    "# 3) splits\n",
    "if (ART / \"split_and_labels_meta.joblib\").exists():\n",
    "    meta2 = joblib.load(ART / \"split_and_labels_meta.joblib\")\n",
    "    if 'idx_train' in meta2:\n",
    "        idx_train = meta2['idx_train']; idx_val = meta2['idx_val']; idx_test = meta2['idx_test']\n",
    "        print(\"Loaded idx splits from artifacts.\")\n",
    "    else:\n",
    "        idx = np.arange(X_mm.shape[0])\n",
    "        strat_key = np.sum(Y_all, axis=1)\n",
    "        idx_temp, idx_test, Y_temp, Y_test = train_test_split(idx, Y_all, test_size=0.10, random_state=RNG, stratify=strat_key)\n",
    "        strat_temp = np.sum(Y_temp, axis=1)\n",
    "        idx_train, idx_val = train_test_split(idx_temp, test_size=0.1111, random_state=RNG, stratify=strat_temp)\n",
    "        joblib.dump({\"idx_train\": idx_train, \"idx_val\": idx_val, \"idx_test\": idx_test}, ART / \"split_and_labels_meta.joblib\")\n",
    "        print(\"Created and saved idx splits.\")\n",
    "else:\n",
    "    idx = np.arange(X_mm.shape[0])\n",
    "    strat_key = np.sum(Y_all, axis=1)\n",
    "    idx_temp, idx_test, Y_temp, Y_test = train_test_split(idx, Y_all, test_size=0.10, random_state=RNG, stratify=strat_key)\n",
    "    strat_temp = np.sum(Y_temp, axis=1)\n",
    "    idx_train, idx_val = train_test_split(idx_temp, test_size=0.1111, random_state=RNG, stratify=strat_temp)\n",
    "    joblib.dump({\"idx_train\": idx_train, \"idx_val\": idx_val, \"idx_test\": idx_test}, ART / \"split_and_labels_meta.joblib\")\n",
    "    print(\"Created and saved idx splits.\")\n",
    "\n",
    "# 4) filter labels (double-check)\n",
    "label_support = Y_all.sum(axis=0).astype(int)\n",
    "keep_mask = label_support >= MIN_SUPPORT_LABEL\n",
    "if keep_mask.sum() < Y_all.shape[1]:\n",
    "    Y_all = Y_all[:, keep_mask]\n",
    "    label_names = list(np.array(label_names)[keep_mask])\n",
    "n_labels = Y_all.shape[1]\n",
    "print(\"n_labels after filter:\", n_labels)\n",
    "\n",
    "# 5) pos_weight\n",
    "pos_counts = Y_all[idx_train].sum(axis=0).astype(int)\n",
    "neg_counts = len(idx_train) - pos_counts\n",
    "pos_counts = np.maximum(pos_counts, 1)\n",
    "pos_weight = (neg_counts / pos_counts).astype(float)\n",
    "pos_weight = np.minimum(pos_weight, 100.0)\n",
    "print(\"Example pos_weight:\", pos_weight[:10])\n",
    "\n",
    "# 6) instantiate classifiers (FORCE use of 'log_loss')\n",
    "# If models existed in globals they were deleted before running this cell (recommended)\n",
    "models = []\n",
    "for j in range(n_labels):\n",
    "    # use explicit 'log_loss' which is supported in modern sklearn\n",
    "    clf = SGDClassifier(loss='log_loss',\n",
    "                        penalty='l2', alpha=ALPHA, learning_rate='optimal',\n",
    "                        random_state=RNG, tol=1e-3)\n",
    "    models.append(clf)\n",
    "\n",
    "# 7) training loop (partial_fit)\n",
    "t0 = time.time()\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    print(f\"\\n=== EPOCH {epoch}/{N_EPOCHS} ===\")\n",
    "    rng = np.random.default_rng(RNG + epoch)\n",
    "    train_idx_shuf = rng.permutation(idx_train)\n",
    "    n_batches = math.ceil(len(train_idx_shuf) / BATCH_SIZE)\n",
    "\n",
    "    with tqdm(total=len(train_idx_shuf), desc=f\"epoch{epoch}\", unit=\"rows\") as pbar:\n",
    "        for b in range(n_batches):\n",
    "            s = b * BATCH_SIZE\n",
    "            e = min(len(train_idx_shuf), (b+1)*BATCH_SIZE)\n",
    "            batch_idx = train_idx_shuf[s:e]\n",
    "            X_batch = np.asarray(X_mm[batch_idx], dtype=np.float32)\n",
    "            y_batch = Y_all[batch_idx]\n",
    "\n",
    "            for j in range(n_labels):\n",
    "                yb = y_batch[:, j].astype(int)\n",
    "                sw = np.where(yb==1, pos_weight[j], 1.0).astype(np.float32) if pos_weight[j] > 1.0 else None\n",
    "                if epoch == 1 and b == 0:\n",
    "                    models[j].partial_fit(X_batch, yb, classes=[0,1], sample_weight=sw)\n",
    "                else:\n",
    "                    models[j].partial_fit(X_batch, yb, sample_weight=sw)\n",
    "            pbar.update(e-s)\n",
    "\n",
    "    # validation quick eval\n",
    "    print(\"Evaluating on validation set ...\")\n",
    "    def compute_scores_on_idx(idx_list):\n",
    "        mb = BATCH_SIZE\n",
    "        cols_acc = []\n",
    "        for i in range(0, len(idx_list), mb):\n",
    "            j = min(len(idx_list), i+mb)\n",
    "            Xb = np.asarray(X_mm[idx_list[i:j]], dtype=np.float32)\n",
    "            cols = []\n",
    "            for mdl in models:\n",
    "                try:\n",
    "                    s_arr = mdl.decision_function(Xb)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        s_arr = mdl.predict_proba(Xb)[:, -1]\n",
    "                    except Exception:\n",
    "                        s_arr = mdl.predict(Xb)\n",
    "                cols.append(s_arr)\n",
    "            cols = np.vstack(cols).T\n",
    "            cols_acc.append(cols)\n",
    "        return np.vstack(cols_acc)\n",
    "\n",
    "    val_scores = compute_scores_on_idx(idx_val)\n",
    "    y_val = Y_all[idx_val]\n",
    "    ap_vals = []\n",
    "    for j in range(n_labels):\n",
    "        if y_val[:, j].sum() == 0:\n",
    "            ap_vals.append(np.nan)\n",
    "        else:\n",
    "            ap_vals.append(float(average_precision_score(y_val[:, j], val_scores[:, j])))\n",
    "    mean_ap = np.nanmean([a for a in ap_vals if not np.isnan(a)])\n",
    "    print(f\"Epoch {epoch} ‚Äî AP_macro on val: {mean_ap:.4f}\")\n",
    "\n",
    "    # checkpoint\n",
    "    if SAVE_EVERY_EPOCH:\n",
    "        joblib.dump(models, ART / f\"emo_sgd_partial_models_epoch{epoch}.joblib\", compress=3)\n",
    "        joblib.dump({\"idx_train\": idx_train, \"idx_val\": idx_val, \"idx_test\": idx_test, \"label_names\": label_names}, ART / \"split_and_labels_meta.joblib\")\n",
    "        print(\"Checkpoint saved for epoch\", epoch)\n",
    "\n",
    "# end\n",
    "t_elapsed = time.time() - t0\n",
    "print(f\"\\nTraining completed in {t_elapsed:.1f}s. Saving final models ...\")\n",
    "joblib.dump(models, ART / \"emo_sgd_partial_models.joblib\", compress=3)\n",
    "joblib.dump({\"idx_train\": idx_train, \"idx_val\": idx_val, \"idx_test\": idx_test, \"label_names\": label_names}, ART / \"split_and_labels_meta.joblib\")\n",
    "print(\"Saved models and split metadata.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fe57892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 models.\n",
      "Computing val scores ...\n",
      "Computing test scores ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label_idx",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "support_test",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AP_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_used",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8d5c7703-48db-486d-90de-a4a8e03289fc",
       "rows": [
        [
         "0",
         "5",
         "emo_caring",
         "84",
         "0.7015072412229194",
         "0.9607843137254902",
         "0.5833333333333334",
         "0.725925925925926",
         "2.226034851706438"
        ],
        [
         "1",
         "24",
         "emo_remorse",
         "41",
         "0.6730060697001569",
         "0.6428571428571429",
         "0.6585365853658537",
         "0.6506024096385542",
         "2.1229110228318397"
        ],
        [
         "2",
         "16",
         "emo_grief",
         "77",
         "0.6635594084835748",
         "0.8301886792452831",
         "0.5714285714285714",
         "0.676923076923077",
         "2.1338043146838315"
        ],
        [
         "3",
         "8",
         "emo_desire",
         "213",
         "0.49374961458216804",
         "0.6307692307692307",
         "0.38497652582159625",
         "0.478134110787172",
         "3.4441009179333886"
        ],
        [
         "4",
         "4",
         "emo_approval",
         "249",
         "0.4751546698333839",
         "0.59375",
         "0.4578313253012048",
         "0.5170068027210885",
         "2.608159064778749"
        ],
        [
         "5",
         "14",
         "emo_fear",
         "920",
         "0.4439654635291348",
         "0.5537280701754386",
         "0.5489130434782609",
         "0.5513100436681223",
         "2.398303907329171"
        ],
        [
         "6",
         "10",
         "emo_disapproval",
         "64",
         "0.4161637925875272",
         "0.44",
         "0.515625",
         "0.4748201438848921",
         "1.347655000892538"
        ],
        [
         "7",
         "1",
         "emo_amusement",
         "59",
         "0.34013762835512706",
         "0.4878048780487805",
         "0.3389830508474576",
         "0.4",
         "2.2059908082948656"
        ],
        [
         "8",
         "9",
         "emo_disappointment",
         "2831",
         "0.3232954736112336",
         "0.3373597929249353",
         "0.4143412221829742",
         "0.3719086873811034",
         "2.486216052796463"
        ],
        [
         "9",
         "0",
         "emo_admiration",
         "57",
         "0.27626632144894714",
         "0.2833333333333333",
         "0.2982456140350877",
         "0.2905982905982906",
         "0.028263148376105107"
        ],
        [
         "10",
         "2",
         "emo_anger",
         "4",
         "0.2522248970205229",
         "0.2",
         "0.25",
         "0.2222222222222222",
         "-1.8850651394590283"
        ],
        [
         "11",
         "20",
         "emo_optimism",
         "233",
         "0.248923124772311",
         "0.4649122807017544",
         "0.22746781115879827",
         "0.30547550432276654",
         "2.880226467034669"
        ],
        [
         "12",
         "18",
         "emo_love",
         "12",
         "0.24225762597986303",
         "0.2",
         "0.16666666666666666",
         "0.18181818181818182",
         "-0.2903202966589582"
        ],
        [
         "13",
         "12",
         "emo_embarrassment",
         "123",
         "0.18638228445012042",
         "0.34177215189873417",
         "0.21951219512195122",
         "0.26732673267326734",
         "1.7206424647302327"
        ],
        [
         "14",
         "13",
         "emo_excitement",
         "3",
         "0.17282382140302333",
         "0.0",
         "0.0",
         "0.0",
         "-2.4044194790790865"
        ],
        [
         "15",
         "3",
         "emo_annoyance",
         "375",
         "0.17266313312166673",
         "0.2033271719038817",
         "0.29333333333333333",
         "0.24017467248908297",
         "1.8908653425605864"
        ],
        [
         "16",
         "6",
         "emo_confusion",
         "227",
         "0.1687981105654899",
         "0.28402366863905326",
         "0.21145374449339208",
         "0.24242424242424243",
         "2.167777251634096"
        ],
        [
         "17",
         "19",
         "emo_nervousness",
         "83",
         "0.1449849161277215",
         "0.30357142857142855",
         "0.20481927710843373",
         "0.2446043165467626",
         "1.0563839529657688"
        ],
        [
         "18",
         "23",
         "emo_relief",
         "557",
         "0.14385546928350598",
         "0.2817258883248731",
         "0.1992818671454219",
         "0.2334384858044164",
         "2.3895810930337107"
        ],
        [
         "19",
         "11",
         "emo_disgust",
         "5",
         "0.12056385056385056",
         "0.0",
         "0.0",
         "0.0",
         "-0.03667279647872679"
        ],
        [
         "20",
         "17",
         "emo_joy",
         "27",
         "0.0842559216578894",
         "0.18181818181818182",
         "0.07407407407407407",
         "0.10526315789473684",
         "0.20854203795752735"
        ],
        [
         "21",
         "15",
         "emo_gratitude",
         "3",
         "0.052222222222222225",
         "0.044444444444444446",
         "0.6666666666666666",
         "0.08333333333333333",
         "-2.0132120718638236"
        ],
        [
         "22",
         "22",
         "emo_realization",
         "23",
         "0.03860255168719843",
         "0.06153846153846154",
         "0.17391304347826086",
         "0.09090909090909091",
         "-0.272654263921404"
        ],
        [
         "23",
         "7",
         "emo_curiosity",
         "92",
         "0.03726258852487174",
         "0.0663265306122449",
         "0.14130434782608695",
         "0.09027777777777778",
         "-0.29859195735725796"
        ],
        [
         "24",
         "21",
         "emo_pride",
         "6",
         "0.0024513843850094267",
         "0.0",
         "0.0",
         "0.0",
         "-0.8432213447447259"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 25
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_idx</th>\n",
       "      <th>label_name</th>\n",
       "      <th>support_test</th>\n",
       "      <th>AP_test</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>thr_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>emo_caring</td>\n",
       "      <td>84</td>\n",
       "      <td>0.701507</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>2.226035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>emo_remorse</td>\n",
       "      <td>41</td>\n",
       "      <td>0.673006</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>2.122911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>emo_grief</td>\n",
       "      <td>77</td>\n",
       "      <td>0.663559</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>2.133804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>emo_desire</td>\n",
       "      <td>213</td>\n",
       "      <td>0.493750</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.384977</td>\n",
       "      <td>0.478134</td>\n",
       "      <td>3.444101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>emo_approval</td>\n",
       "      <td>249</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>2.608159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>emo_fear</td>\n",
       "      <td>920</td>\n",
       "      <td>0.443965</td>\n",
       "      <td>0.553728</td>\n",
       "      <td>0.548913</td>\n",
       "      <td>0.551310</td>\n",
       "      <td>2.398304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>emo_disapproval</td>\n",
       "      <td>64</td>\n",
       "      <td>0.416164</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.474820</td>\n",
       "      <td>1.347655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>emo_amusement</td>\n",
       "      <td>59</td>\n",
       "      <td>0.340138</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.205991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>emo_disappointment</td>\n",
       "      <td>2831</td>\n",
       "      <td>0.323295</td>\n",
       "      <td>0.337360</td>\n",
       "      <td>0.414341</td>\n",
       "      <td>0.371909</td>\n",
       "      <td>2.486216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>emo_admiration</td>\n",
       "      <td>57</td>\n",
       "      <td>0.276266</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>0.028263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>emo_anger</td>\n",
       "      <td>4</td>\n",
       "      <td>0.252225</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>-1.885065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>emo_optimism</td>\n",
       "      <td>233</td>\n",
       "      <td>0.248923</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.227468</td>\n",
       "      <td>0.305476</td>\n",
       "      <td>2.880226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>emo_love</td>\n",
       "      <td>12</td>\n",
       "      <td>0.242258</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>-0.290320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>emo_embarrassment</td>\n",
       "      <td>123</td>\n",
       "      <td>0.186382</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>1.720642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>emo_excitement</td>\n",
       "      <td>3</td>\n",
       "      <td>0.172824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.404419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>emo_annoyance</td>\n",
       "      <td>375</td>\n",
       "      <td>0.172663</td>\n",
       "      <td>0.203327</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.240175</td>\n",
       "      <td>1.890865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>emo_confusion</td>\n",
       "      <td>227</td>\n",
       "      <td>0.168798</td>\n",
       "      <td>0.284024</td>\n",
       "      <td>0.211454</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>2.167777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>emo_nervousness</td>\n",
       "      <td>83</td>\n",
       "      <td>0.144985</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>0.244604</td>\n",
       "      <td>1.056384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23</td>\n",
       "      <td>emo_relief</td>\n",
       "      <td>557</td>\n",
       "      <td>0.143855</td>\n",
       "      <td>0.281726</td>\n",
       "      <td>0.199282</td>\n",
       "      <td>0.233438</td>\n",
       "      <td>2.389581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11</td>\n",
       "      <td>emo_disgust</td>\n",
       "      <td>5</td>\n",
       "      <td>0.120564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.036673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17</td>\n",
       "      <td>emo_joy</td>\n",
       "      <td>27</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.208542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>emo_gratitude</td>\n",
       "      <td>3</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-2.013212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>emo_realization</td>\n",
       "      <td>23</td>\n",
       "      <td>0.038603</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.272654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>emo_curiosity</td>\n",
       "      <td>92</td>\n",
       "      <td>0.037263</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.090278</td>\n",
       "      <td>-0.298592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>21</td>\n",
       "      <td>emo_pride</td>\n",
       "      <td>6</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.843221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label_idx          label_name  support_test   AP_test  precision  \\\n",
       "0           5          emo_caring            84  0.701507   0.960784   \n",
       "1          24         emo_remorse            41  0.673006   0.642857   \n",
       "2          16           emo_grief            77  0.663559   0.830189   \n",
       "3           8          emo_desire           213  0.493750   0.630769   \n",
       "4           4        emo_approval           249  0.475155   0.593750   \n",
       "5          14            emo_fear           920  0.443965   0.553728   \n",
       "6          10     emo_disapproval            64  0.416164   0.440000   \n",
       "7           1       emo_amusement            59  0.340138   0.487805   \n",
       "8           9  emo_disappointment          2831  0.323295   0.337360   \n",
       "9           0      emo_admiration            57  0.276266   0.283333   \n",
       "10          2           emo_anger             4  0.252225   0.200000   \n",
       "11         20        emo_optimism           233  0.248923   0.464912   \n",
       "12         18            emo_love            12  0.242258   0.200000   \n",
       "13         12   emo_embarrassment           123  0.186382   0.341772   \n",
       "14         13      emo_excitement             3  0.172824   0.000000   \n",
       "15          3       emo_annoyance           375  0.172663   0.203327   \n",
       "16          6       emo_confusion           227  0.168798   0.284024   \n",
       "17         19     emo_nervousness            83  0.144985   0.303571   \n",
       "18         23          emo_relief           557  0.143855   0.281726   \n",
       "19         11         emo_disgust             5  0.120564   0.000000   \n",
       "20         17             emo_joy            27  0.084256   0.181818   \n",
       "21         15       emo_gratitude             3  0.052222   0.044444   \n",
       "22         22     emo_realization            23  0.038603   0.061538   \n",
       "23          7       emo_curiosity            92  0.037263   0.066327   \n",
       "24         21           emo_pride             6  0.002451   0.000000   \n",
       "\n",
       "      recall        f1  thr_used  \n",
       "0   0.583333  0.725926  2.226035  \n",
       "1   0.658537  0.650602  2.122911  \n",
       "2   0.571429  0.676923  2.133804  \n",
       "3   0.384977  0.478134  3.444101  \n",
       "4   0.457831  0.517007  2.608159  \n",
       "5   0.548913  0.551310  2.398304  \n",
       "6   0.515625  0.474820  1.347655  \n",
       "7   0.338983  0.400000  2.205991  \n",
       "8   0.414341  0.371909  2.486216  \n",
       "9   0.298246  0.290598  0.028263  \n",
       "10  0.250000  0.222222 -1.885065  \n",
       "11  0.227468  0.305476  2.880226  \n",
       "12  0.166667  0.181818 -0.290320  \n",
       "13  0.219512  0.267327  1.720642  \n",
       "14  0.000000  0.000000 -2.404419  \n",
       "15  0.293333  0.240175  1.890865  \n",
       "16  0.211454  0.242424  2.167777  \n",
       "17  0.204819  0.244604  1.056384  \n",
       "18  0.199282  0.233438  2.389581  \n",
       "19  0.000000  0.000000 -0.036673  \n",
       "20  0.074074  0.105263  0.208542  \n",
       "21  0.666667  0.083333 -2.013212  \n",
       "22  0.173913  0.090909 -0.272654  \n",
       "23  0.141304  0.090278 -0.298592  \n",
       "24  0.000000  0.000000 -0.843221  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test metrics -> artifacts\\emotions_test_metrics_partial_sgd_final.csv\n"
     ]
    }
   ],
   "source": [
    "# === Eval: calcule val/test scores, thresholds (max-F1 on PR) et m√©triques finales ===\n",
    "import joblib, numpy as np, os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_recall_fscore_support\n",
    "ART = Path(\"artifacts\")\n",
    "# 1) load models\n",
    "models_path = ART / \"emo_sgd_partial_models.joblib\"\n",
    "if not models_path.exists():\n",
    "    # fallback pick last epoch checkpoint\n",
    "    ck = sorted(list(ART.glob(\"emo_sgd_partial_models_epoch*.joblib\")))\n",
    "    if ck:\n",
    "        models_path = ck[-1]\n",
    "        print(\"Using checkpoint:\", models_path.name)\n",
    "    else:\n",
    "        raise RuntimeError(\"Aucun modele trouv√© dans artifacts/. Run training or use checkpoint.\")\n",
    "models = joblib.load(models_path)\n",
    "print(\"Loaded\", len(models), \"models.\")\n",
    "\n",
    "# 2) load splits & meta\n",
    "meta = joblib.load(ART / \"split_and_labels_meta.joblib\")\n",
    "idx_train, idx_val, idx_test = meta['idx_train'], meta['idx_val'], meta['idx_test']\n",
    "label_names = meta.get('label_names', [f\"label_{i}\" for i in range(len(models))])\n",
    "\n",
    "# 3) memmap\n",
    "meta_mm = joblib.load(ART / \"X_all_memmap_meta.joblib\")\n",
    "X_mm = np.memmap(str(ART / \"X_all_memmap.dat\"), dtype=\"float32\", mode=\"r\", shape=(int(meta_mm[\"n\"]), int(meta_mm[\"d\"])))\n",
    "\n",
    "# helper to compute scores in batches (decision_function or proba fallback)\n",
    "def compute_scores(models, idx_list, batch_size=8192):\n",
    "    import math, numpy as np\n",
    "    out_chunks = []\n",
    "    for i in range(0, len(idx_list), batch_size):\n",
    "        j = min(len(idx_list), i+batch_size)\n",
    "        Xb = np.asarray(X_mm[idx_list[i:j]], dtype=np.float32)\n",
    "        cols = []\n",
    "        for mdl in models:\n",
    "            try:\n",
    "                s = mdl.decision_function(Xb)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    s = mdl.predict_proba(Xb)[:, -1]\n",
    "                except Exception:\n",
    "                    s = mdl.predict(Xb)  # fallback (discrete, not ideal)\n",
    "            cols.append(s)\n",
    "        out_chunks.append(np.vstack(cols).T)\n",
    "    return np.vstack(out_chunks)\n",
    "\n",
    "print(\"Computing val scores ...\")\n",
    "val_scores = compute_scores(models, idx_val)\n",
    "print(\"Computing test scores ...\")\n",
    "test_scores = compute_scores(models, idx_test)\n",
    "\n",
    "# 4) derive per-label thresholds on val (max F1 on PR curve)\n",
    "n_labels = val_scores.shape[1]\n",
    "thr = np.full(n_labels, 0.5, dtype=float)\n",
    "for j in range(n_labels):\n",
    "    yv = joblib.load(ART / \"split_and_labels_meta.joblib\").get('Y_all_partial_dummy', None)  # just to silence linter\n",
    "    # we assume Y_all_filtered is available in globals\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    yv = globals().get('Y_all_filtered')[idx_val, j]\n",
    "    if yv.sum() == 0:\n",
    "        thr[j] = 0.5\n",
    "        continue\n",
    "    p, r, t = precision_recall_curve(yv, val_scores[:, j])\n",
    "    f1s = (2*p*r)/(p+r+1e-12)\n",
    "    ibest = int(np.nanargmax(f1s))\n",
    "    thr[j] = float(t[ibest]) if ibest < len(t) else 0.5\n",
    "\n",
    "# 5) metrics on test\n",
    "y_test = globals().get('Y_all_filtered')[idx_test]\n",
    "ap_list = []\n",
    "for j in range(n_labels):\n",
    "    if y_test[:, j].sum() == 0:\n",
    "        ap_list.append(np.nan)\n",
    "    else:\n",
    "        ap_list.append(float(average_precision_score(y_test[:, j], test_scores[:, j])))\n",
    "\n",
    "yhat_test = (test_scores >= thr.reshape(1, -1)).astype(int)\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(y_test, yhat_test, average=None, zero_division=0)\n",
    "\n",
    "import pandas as pd\n",
    "df_test = pd.DataFrame({\n",
    "    \"label_idx\": np.arange(n_labels),\n",
    "    \"label_name\": label_names,\n",
    "    \"support_test\": sup,\n",
    "    \"AP_test\": ap_list,\n",
    "    \"precision\": prec,\n",
    "    \"recall\": rec,\n",
    "    \"f1\": f1,\n",
    "    \"thr_used\": thr\n",
    "}).sort_values(\"AP_test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(df_test.head(60))\n",
    "df_test.to_csv(ART / \"emotions_test_metrics_partial_sgd_final.csv\", index=False)\n",
    "joblib.dump({\"thr\": thr, \"val_scores\": val_scores, \"test_scores\": test_scores}, ART / \"emo_partial_test_artifacts.joblib\")\n",
    "print(\"Saved test metrics ->\", ART / \"emotions_test_metrics_partial_sgd_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21049a6e",
   "metadata": {},
   "source": [
    "# Explication ‚Äî pourquoi ces 3 snippets et √† quoi ils servent\n",
    "\n",
    "## 1) Encodage complet (SBERT) ‚Äî `encode_full_sbert`\n",
    "- **But** : convertir **tous** les textes en vecteurs (embeddings) SBERT (`X_all_emb`) pour pouvoir entra√Æner un mod√®le rapide et reproductible sans relancer l‚Äôencodage √† chaque essai.\n",
    "- **Pourquoi** : l‚Äôencodage est co√ªteux (temps/CPU/GPU). Le stocker en `.npz` permet de r√©utiliser les embeddings pour plusieurs exp√©riences (s√©lection de labels, entra√Ænements, calibration, etc.).\n",
    "- **Sorties** : fichier compress√© `artifacts/emotions_all_embeddings.npz` contenant `X` (embeddings). Expose `X_all_emb` en m√©moire pour la suite.\n",
    "\n",
    "## 2) R√©-entrainement final + calibration ‚Äî `train_on_full_embeddings` / calibration cell\n",
    "- **But** : entra√Æner un classifieur final (OneVsRest + LogisticRegression) sur l‚Äôensemble des embeddings et, si besoin, **calibrer** ses probabilit√©s (CalibratedClassifierCV).\n",
    "- **Pourquoi** :  \n",
    "  - on choisit d‚Äôabord une proc√©dure robuste (baseline rapide et stable) ;  \n",
    "  - la calibration est importante parce que **les thresholds par label** (d√©cisions binaire) sont bas√©s sur des probabilit√©s compar√©es √† un seuil ‚Äî des probabilit√©s mal calibr√©es biaisent ces d√©cisions.\n",
    "- **Sorties** :  \n",
    "  - mod√®le(s) enregistr√©s (`artifacts/emo_clf_sbert_full_train.joblib`, `emo_clf_final_trainval_calibrated.joblib`),  \n",
    "  - scaler (`emo_sbert_scaler.joblib`),  \n",
    "  - thresholds calcul√©s et sauvegard√©s.\n",
    "\n",
    "## 3) √âvaluation post-calibration ‚Äî `eval finale`\n",
    "- **But** : appliquer le mod√®le final (calibr√© si disponible) sur le jeu de test, appliquer les thresholds robustes (K-fold CV / floor) et calculer des m√©triques exploitables (AP par label, precision/recall/f1, micro/macro).  \n",
    "- **Pourquoi** : mesurer la performance finale **r√©aliste** du pipeline, avec d√©cisions binaires pr√™tes pour la d√©mo (plut√¥t que seules les probabilit√©s). Les m√©triques finales alimentent ton rapport (AP_macro, micro-F1, tableau par label).\n",
    "- **Sorties** : `artifacts/emotions_test_metrics.csv`, `artifacts/emotions_proba_te.joblib`, et mod√®les calibr√©s/finaux pour d√©mo.\n",
    "\n",
    "---\n",
    "\n",
    "## Remarques pratiques (√† expliquer √† l‚Äôoral)\n",
    "- La calibration + K-fold thresholds r√©duisent le risque que des labels rares d√©clenchent des d√©cisions erron√©es (trop de faux positifs).  \n",
    "- Encodage complet co√ªte du temps/ESPACE : pr√©vois disque (‚âà 384 * 4 bytes * N_samples), et ajuste `BATCH_SIZE` selon le GPU/CPU.  \n",
    "- Pour am√©liorer la qualit√© : annoter manuellement quelques centaines d‚Äôexemples cibl√©s (active learning), ou fine-tuner un transformer si tu veux pousser la performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4df1951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: emo_sgd_partial_models_epoch2.joblib\n",
      "Loaded memmap, splits, Y shape: (1314720, 384) (1314720, 25) n_labels: 25\n",
      "pos_weight sample (first 10): [20. 20. 20. 20. 20. 20. 20. 20. 20. 20.]\n",
      "Resuming from epoch: 2\n",
      "\n",
      "=== RESUME EPOCH 3/6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [02:50<00:00, 6167.28rows/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 3 ‚Äî AP_macro on val: 0.2882\n",
      "Checkpoint saved for epoch 3\n",
      "\n",
      "=== RESUME EPOCH 4/6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:28<00:00, 36600.60rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 4 ‚Äî AP_macro on val: 0.2880\n",
      "Checkpoint saved for epoch 4\n",
      "\n",
      "=== RESUME EPOCH 5/6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:29<00:00, 35548.66rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 5 ‚Äî AP_macro on val: 0.2856\n",
      "Checkpoint saved for epoch 5\n",
      "\n",
      "=== RESUME EPOCH 6/6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:29<00:00, 36171.40rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 6 ‚Äî AP_macro on val: 0.2845\n",
      "Checkpoint saved for epoch 6\n",
      "Resume training finished and models saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7541"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Resume training from checkpoint with safer pos_weight cap & lower alpha ===\n",
    "import joblib, time, math, gc, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "# config √† modifier si besoin\n",
    "pos_weight_max = 20.0        # <--- cap conseill√© (20 ou 50)\n",
    "alpha_new = 1e-5             # <--- moins de r√©gularisation\n",
    "BATCH_SIZE = 8192\n",
    "N_EPOCHS_TOTAL = 6           # nombre d'epochs total vis√© (si tu as d√©j√† fait 2 -> fera 4 epochs de plus)\n",
    "RNG = 42\n",
    "SAVE_EVERY_EPOCH = True\n",
    "\n",
    "# 1) charger checkpoint le plus r√©cent\n",
    "ckpts = sorted(ART.glob(\"emo_sgd_partial_models_epoch*.joblib\"))\n",
    "if not ckpts:\n",
    "    # fallback to generic\n",
    "    if (ART / \"emo_sgd_partial_models.joblib\").exists():\n",
    "        ckpt_path = ART / \"emo_sgd_partial_models.joblib\"\n",
    "    else:\n",
    "        raise RuntimeError(\"Aucun checkpoint trouv√© dans artifacts/ ‚Äî ex√©cute l'entra√Ænement initial d'abord.\")\n",
    "else:\n",
    "    ckpt_path = ckpts[-1]\n",
    "print(\"Using checkpoint:\", ckpt_path.name)\n",
    "models = joblib.load(ckpt_path)\n",
    "\n",
    "# 2) load memmap & splits & Y\n",
    "meta_mm = joblib.load(ART / \"X_all_memmap_meta.joblib\")\n",
    "X_mm = np.memmap(str(ART / \"X_all_memmap.dat\"), dtype=\"float32\", mode=\"r\", shape=(int(meta_mm[\"n\"]), int(meta_mm[\"d\"])))\n",
    "meta2 = joblib.load(ART / \"split_and_labels_meta.joblib\")\n",
    "idx_train, idx_val, idx_test = meta2['idx_train'], meta2['idx_val'], meta2['idx_test']\n",
    "label_names = meta2.get('label_names', [f\"label_{i}\" for i in range(len(models))])\n",
    "Y_all = globals().get('Y_all_filtered')\n",
    "if Y_all is None:\n",
    "    raise RuntimeError(\"Y_all_filtered absent en m√©moire. Recharge la variable avant.\")\n",
    "\n",
    "n_labels = Y_all.shape[1]\n",
    "print(\"Loaded memmap, splits, Y shape:\", X_mm.shape, Y_all.shape, \"n_labels:\", n_labels)\n",
    "if len(models) != n_labels:\n",
    "    raise RuntimeError(f\"Nombre de mod√®les ({len(models)}) != n_labels ({n_labels}). V√©rifie checkpoint/labels.\")\n",
    "\n",
    "# 3) recompute pos_weight but cap it\n",
    "pos_counts = Y_all[idx_train].sum(axis=0).astype(int)\n",
    "neg_counts = len(idx_train) - pos_counts\n",
    "pos_counts = np.maximum(pos_counts, 1)\n",
    "pos_weight = (neg_counts / pos_counts).astype(float)\n",
    "pos_weight = np.minimum(pos_weight, pos_weight_max)\n",
    "print(\"pos_weight sample (first 10):\", pos_weight[:10])\n",
    "\n",
    "# 4) If checkpoint models used different alpha, we will not reinit them;\n",
    "#    but we will adjust classifiers by continuing partial_fit. To be safe,\n",
    "#    re-wrap models if you want to re-create with new alpha (optionnel).\n",
    "#    Here we continue with existing models (recommended to preserve learned weights).\n",
    "\n",
    "# 5) helper to compute validation AP in batches\n",
    "def compute_scores(models, idx_list, batch_size=8192):\n",
    "    out_chunks = []\n",
    "    for i in range(0, len(idx_list), batch_size):\n",
    "        j = min(len(idx_list), i+batch_size)\n",
    "        Xb = np.asarray(X_mm[idx_list[i:j]], dtype=np.float32)\n",
    "        cols = []\n",
    "        for mdl in models:\n",
    "            try:\n",
    "                s = mdl.decision_function(Xb)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    s = mdl.predict_proba(Xb)[:, -1]\n",
    "                except Exception:\n",
    "                    s = mdl.predict(Xb)\n",
    "            cols.append(s)\n",
    "        out_chunks.append(np.vstack(cols).T)\n",
    "    return np.vstack(out_chunks)\n",
    "\n",
    "# 6) continue training epochs (we find start_epoch from filename)\n",
    "import re\n",
    "m = re.search(r\"epoch(\\d+)\", ckpt_path.name)\n",
    "start_epoch = int(m.group(1)) if m else 0\n",
    "print(\"Resuming from epoch:\", start_epoch)\n",
    "epoch_start = start_epoch + 1\n",
    "epoch_end = N_EPOCHS_TOTAL\n",
    "\n",
    "# 7) continue partial_fit loop (same logic as before)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "for epoch in range(epoch_start, epoch_end+1):\n",
    "    print(f\"\\n=== RESUME EPOCH {epoch}/{epoch_end} ===\")\n",
    "    rng = np.random.default_rng(RNG + epoch)\n",
    "    train_idx_shuf = rng.permutation(idx_train)\n",
    "    n_batches = math.ceil(len(train_idx_shuf) / BATCH_SIZE)\n",
    "    with tqdm(total=len(train_idx_shuf), desc=f\"epoch{epoch}\", unit=\"rows\") as pbar:\n",
    "        for b in range(n_batches):\n",
    "            s = b * BATCH_SIZE\n",
    "            e = min(len(train_idx_shuf), (b+1)*BATCH_SIZE)\n",
    "            batch_idx = train_idx_shuf[s:e]\n",
    "            X_batch = np.asarray(X_mm[batch_idx], dtype=np.float32)\n",
    "            y_batch = Y_all[batch_idx]\n",
    "            # partial_fit for each label\n",
    "            for j in range(n_labels):\n",
    "                yb = y_batch[:, j].astype(int)\n",
    "                sw = np.where(yb==1, pos_weight[j], 1.0).astype(np.float32) if pos_weight[j] > 1.0 else None\n",
    "                # ensure classes param only for first ever partial_fit of that model\n",
    "                # we cannot easily know if model was fully fit before; safe approach:\n",
    "                try:\n",
    "                    models[j].partial_fit(X_batch, yb, classes=[0,1], sample_weight=sw)\n",
    "                except Exception:\n",
    "                    models[j].partial_fit(X_batch, yb, sample_weight=sw)\n",
    "            pbar.update(e-s)\n",
    "\n",
    "    # validation after epoch\n",
    "    print(\"Computing validation AP (batch-wise)...\")\n",
    "    val_scores = compute_scores(models, idx_val, batch_size=BATCH_SIZE)\n",
    "    y_val = Y_all[idx_val]\n",
    "    ap_vals = []\n",
    "    for j in range(n_labels):\n",
    "        if y_val[:, j].sum() == 0:\n",
    "            ap_vals.append(np.nan)\n",
    "        else:\n",
    "            ap_vals.append(float(average_precision_score(y_val[:, j], val_scores[:, j])))\n",
    "    mean_ap = np.nanmean([a for a in ap_vals if not np.isnan(a)])\n",
    "    print(f\"Epoch {epoch} ‚Äî AP_macro on val: {mean_ap:.4f}\")\n",
    "\n",
    "    # save checkpoint\n",
    "    if SAVE_EVERY_EPOCH:\n",
    "        joblib.dump(models, ART / f\"emo_sgd_partial_models_epoch{epoch}.joblib\", compress=3)\n",
    "        joblib.dump({\"idx_train\": idx_train, \"idx_val\": idx_val, \"idx_test\": idx_test, \"label_names\": label_names}, ART / \"split_and_labels_meta.joblib\")\n",
    "        print(\"Checkpoint saved for epoch\", epoch)\n",
    "\n",
    "# final save\n",
    "joblib.dump(models, ART / \"emo_sgd_partial_models_resumed_final.joblib\", compress=3)\n",
    "print(\"Resume training finished and models saved.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cc91bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: emo_sgd_partial_models_epoch6.joblib\n",
      "Loaded memmap, splits, Y shape: (1314720, 384) (1314720, 25) n_labels: 25\n",
      "pos_weight sample (first 10): [20. 20. 20. 20. 20. 20. 20. 20. 20. 20.]\n",
      "Resuming from epoch: 6\n",
      "\n",
      "=== RESUME EPOCH 7/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [02:57<00:00, 5925.41rows/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 7 ‚Äî AP_macro on val: 0.2827\n",
      "Checkpoint saved for epoch 7\n",
      "\n",
      "=== RESUME EPOCH 8/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:29<00:00, 36199.48rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 8 ‚Äî AP_macro on val: 0.2832\n",
      "Checkpoint saved for epoch 8\n",
      "\n",
      "=== RESUME EPOCH 9/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:28<00:00, 36690.75rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 9 ‚Äî AP_macro on val: 0.2828\n",
      "Checkpoint saved for epoch 9\n",
      "\n",
      "=== RESUME EPOCH 10/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:29<00:00, 35961.84rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 10 ‚Äî AP_macro on val: 0.2820\n",
      "Checkpoint saved for epoch 10\n",
      "\n",
      "=== RESUME EPOCH 11/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:29<00:00, 35418.15rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 11 ‚Äî AP_macro on val: 0.2835\n",
      "Checkpoint saved for epoch 11\n",
      "\n",
      "=== RESUME EPOCH 12/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:30<00:00, 34237.66rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 12 ‚Äî AP_macro on val: 0.2832\n",
      "Checkpoint saved for epoch 12\n",
      "\n",
      "=== RESUME EPOCH 13/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:28<00:00, 36356.79rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 13 ‚Äî AP_macro on val: 0.2833\n",
      "Checkpoint saved for epoch 13\n",
      "\n",
      "=== RESUME EPOCH 14/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:28<00:00, 36447.43rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 14 ‚Äî AP_macro on val: 0.2809\n",
      "Checkpoint saved for epoch 14\n",
      "\n",
      "=== RESUME EPOCH 15/15 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1051789/1051789 [00:28<00:00, 36722.06rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing validation AP (batch-wise)...\n",
      "Epoch 15 ‚Äî AP_macro on val: 0.2809\n",
      "Checkpoint saved for epoch 15\n",
      "Resume training finished and models saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Resume training from checkpoint with safer pos_weight cap & lower alpha ===\n",
    "import joblib, time, math, gc, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "# config √† modifier si besoin\n",
    "pos_weight_max = 20.0        # <--- cap conseill√© (20 ou 50)\n",
    "alpha_new = 1e-5             # <--- moins de r√©gularisation\n",
    "BATCH_SIZE = 8192\n",
    "N_EPOCHS_TOTAL = 15           # nombre d'epochs total vis√© (si tu as d√©j√† fait 2 -> fera 4 epochs de plus)\n",
    "RNG = 42\n",
    "SAVE_EVERY_EPOCH = True\n",
    "\n",
    "# 1) charger checkpoint le plus r√©cent\n",
    "ckpts = sorted(ART.glob(\"emo_sgd_partial_models_epoch*.joblib\"))\n",
    "if not ckpts:\n",
    "    # fallback to generic\n",
    "    if (ART / \"emo_sgd_partial_models.joblib\").exists():\n",
    "        ckpt_path = ART / \"emo_sgd_partial_models.joblib\"\n",
    "    else:\n",
    "        raise RuntimeError(\"Aucun checkpoint trouv√© dans artifacts/ ‚Äî ex√©cute l'entra√Ænement initial d'abord.\")\n",
    "else:\n",
    "    ckpt_path = ckpts[-1]\n",
    "print(\"Using checkpoint:\", ckpt_path.name)\n",
    "models = joblib.load(ckpt_path)\n",
    "\n",
    "# 2) load memmap & splits & Y\n",
    "meta_mm = joblib.load(ART / \"X_all_memmap_meta.joblib\")\n",
    "X_mm = np.memmap(str(ART / \"X_all_memmap.dat\"), dtype=\"float32\", mode=\"r\", shape=(int(meta_mm[\"n\"]), int(meta_mm[\"d\"])))\n",
    "meta2 = joblib.load(ART / \"split_and_labels_meta.joblib\")\n",
    "idx_train, idx_val, idx_test = meta2['idx_train'], meta2['idx_val'], meta2['idx_test']\n",
    "label_names = meta2.get('label_names', [f\"label_{i}\" for i in range(len(models))])\n",
    "Y_all = globals().get('Y_all_filtered')\n",
    "if Y_all is None:\n",
    "    raise RuntimeError(\"Y_all_filtered absent en m√©moire. Recharge la variable avant.\")\n",
    "\n",
    "n_labels = Y_all.shape[1]\n",
    "print(\"Loaded memmap, splits, Y shape:\", X_mm.shape, Y_all.shape, \"n_labels:\", n_labels)\n",
    "if len(models) != n_labels:\n",
    "    raise RuntimeError(f\"Nombre de mod√®les ({len(models)}) != n_labels ({n_labels}). V√©rifie checkpoint/labels.\")\n",
    "\n",
    "# 3) recompute pos_weight but cap it\n",
    "pos_counts = Y_all[idx_train].sum(axis=0).astype(int)\n",
    "neg_counts = len(idx_train) - pos_counts\n",
    "pos_counts = np.maximum(pos_counts, 1)\n",
    "pos_weight = (neg_counts / pos_counts).astype(float)\n",
    "pos_weight = np.minimum(pos_weight, pos_weight_max)\n",
    "print(\"pos_weight sample (first 10):\", pos_weight[:10])\n",
    "\n",
    "# 4) If checkpoint models used different alpha, we will not reinit them;\n",
    "#    but we will adjust classifiers by continuing partial_fit. To be safe,\n",
    "#    re-wrap models if you want to re-create with new alpha (optionnel).\n",
    "#    Here we continue with existing models (recommended to preserve learned weights).\n",
    "\n",
    "# 5) helper to compute validation AP in batches\n",
    "def compute_scores(models, idx_list, batch_size=8192):\n",
    "    out_chunks = []\n",
    "    for i in range(0, len(idx_list), batch_size):\n",
    "        j = min(len(idx_list), i+batch_size)\n",
    "        Xb = np.asarray(X_mm[idx_list[i:j]], dtype=np.float32)\n",
    "        cols = []\n",
    "        for mdl in models:\n",
    "            try:\n",
    "                s = mdl.decision_function(Xb)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    s = mdl.predict_proba(Xb)[:, -1]\n",
    "                except Exception:\n",
    "                    s = mdl.predict(Xb)\n",
    "            cols.append(s)\n",
    "        out_chunks.append(np.vstack(cols).T)\n",
    "    return np.vstack(out_chunks)\n",
    "\n",
    "# 6) continue training epochs (we find start_epoch from filename)\n",
    "import re\n",
    "m = re.search(r\"epoch(\\d+)\", ckpt_path.name)\n",
    "start_epoch = int(m.group(1)) if m else 0\n",
    "print(\"Resuming from epoch:\", start_epoch)\n",
    "epoch_start = start_epoch + 1\n",
    "epoch_end = N_EPOCHS_TOTAL\n",
    "\n",
    "# 7) continue partial_fit loop (same logic as before)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "for epoch in range(epoch_start, epoch_end+1):\n",
    "    print(f\"\\n=== RESUME EPOCH {epoch}/{epoch_end} ===\")\n",
    "    rng = np.random.default_rng(RNG + epoch)\n",
    "    train_idx_shuf = rng.permutation(idx_train)\n",
    "    n_batches = math.ceil(len(train_idx_shuf) / BATCH_SIZE)\n",
    "    with tqdm(total=len(train_idx_shuf), desc=f\"epoch{epoch}\", unit=\"rows\") as pbar:\n",
    "        for b in range(n_batches):\n",
    "            s = b * BATCH_SIZE\n",
    "            e = min(len(train_idx_shuf), (b+1)*BATCH_SIZE)\n",
    "            batch_idx = train_idx_shuf[s:e]\n",
    "            X_batch = np.asarray(X_mm[batch_idx], dtype=np.float32)\n",
    "            y_batch = Y_all[batch_idx]\n",
    "            # partial_fit for each label\n",
    "            for j in range(n_labels):\n",
    "                yb = y_batch[:, j].astype(int)\n",
    "                sw = np.where(yb==1, pos_weight[j], 1.0).astype(np.float32) if pos_weight[j] > 1.0 else None\n",
    "                # ensure classes param only for first ever partial_fit of that model\n",
    "                # we cannot easily know if model was fully fit before; safe approach:\n",
    "                try:\n",
    "                    models[j].partial_fit(X_batch, yb, classes=[0,1], sample_weight=sw)\n",
    "                except Exception:\n",
    "                    models[j].partial_fit(X_batch, yb, sample_weight=sw)\n",
    "            pbar.update(e-s)\n",
    "\n",
    "    # validation after epoch\n",
    "    print(\"Computing validation AP (batch-wise)...\")\n",
    "    val_scores = compute_scores(models, idx_val, batch_size=BATCH_SIZE)\n",
    "    y_val = Y_all[idx_val]\n",
    "    ap_vals = []\n",
    "    for j in range(n_labels):\n",
    "        if y_val[:, j].sum() == 0:\n",
    "            ap_vals.append(np.nan)\n",
    "        else:\n",
    "            ap_vals.append(float(average_precision_score(y_val[:, j], val_scores[:, j])))\n",
    "    mean_ap = np.nanmean([a for a in ap_vals if not np.isnan(a)])\n",
    "    print(f\"Epoch {epoch} ‚Äî AP_macro on val: {mean_ap:.4f}\")\n",
    "\n",
    "    # save checkpoint\n",
    "    if SAVE_EVERY_EPOCH:\n",
    "        joblib.dump(models, ART / f\"emo_sgd_partial_models_epoch{epoch}.joblib\", compress=3)\n",
    "        joblib.dump({\"idx_train\": idx_train, \"idx_val\": idx_val, \"idx_test\": idx_test, \"label_names\": label_names}, ART / \"split_and_labels_meta.joblib\")\n",
    "        print(\"Checkpoint saved for epoch\", epoch)\n",
    "\n",
    "# final save\n",
    "joblib.dump(models, ART / \"emo_sgd_partial_models_resumed_final.joblib\", compress=3)\n",
    "print(\"Resume training finished and models saved.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "357825ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers artifacts existants (trier):\n",
      " - X_all_memmap.dat\n",
      " - X_all_memmap_meta.joblib\n",
      " - absa_auto_ensemble_conservative.csv\n",
      " - absa_auto_ensemble_conservative_disagreements.csv\n",
      " - absa_auto_ensemble_disagreements.csv\n",
      " - absa_auto_ensemble_labels.csv\n",
      " - absa_auto_ensemble_ml_disagreements.csv\n",
      " - absa_auto_ensemble_ml_labels.csv\n",
      " - absa_auto_mapping.joblib\n",
      " - absa_candidates_embeddings.joblib\n",
      " - absa_candidates_embs_reduced_labels.joblib\n",
      " - absa_candidates_umap_embs_labels.joblib\n",
      " - absa_manual_qc_sample.csv\n",
      " - absa_reviews_with_auto_aspects.csv\n",
      " - emo_calibrated_ovr_sigmoid.joblib\n",
      " - emo_clf_final_trainval.joblib\n",
      " - emo_clf_final_trainval_calibrated.joblib\n",
      " - emo_partial_test_artifacts.joblib\n",
      " - emo_sbert_scaler.joblib\n",
      " - emo_sgd_partial_models.joblib\n",
      " - emo_sgd_partial_models_epoch1.joblib\n",
      " - emo_sgd_partial_models_epoch10.joblib\n",
      " - emo_sgd_partial_models_epoch11.joblib\n",
      " - emo_sgd_partial_models_epoch12.joblib\n",
      " - emo_sgd_partial_models_epoch13.joblib\n",
      " - emo_sgd_partial_models_epoch14.joblib\n",
      " - emo_sgd_partial_models_epoch15.joblib\n",
      " - emo_sgd_partial_models_epoch2.joblib\n",
      " - emo_sgd_partial_models_epoch3.joblib\n",
      " - emo_sgd_partial_models_epoch4.joblib\n",
      " - emo_sgd_partial_models_epoch5.joblib\n",
      " - emo_sgd_partial_models_epoch6.joblib\n",
      " - emo_sgd_partial_models_epoch7.joblib\n",
      " - emo_sgd_partial_models_epoch8.joblib\n",
      " - emo_sgd_partial_models_epoch9.joblib\n",
      " - emo_sgd_partial_models_resumed_final.joblib\n",
      " - emo_thr_mean_floor.joblib\n",
      " - emo_thr_mean_floor_from_calib_folds.joblib\n",
      " - emotions_all_embeddings.npz\n",
      " - emotions_pseudo_all.parquet\n",
      " - emotions_test_metrics_partial_sgd_final.csv\n",
      " - label_names_resolved.joblib\n",
      " - metrics_with_cvthr.csv\n",
      " - metrics_with_cvthr_from_calib_folds.csv\n",
      " - min_thr_comparison.csv\n",
      " - per_label_results_by_min_thr.joblib\n",
      " - split_and_labels_meta.joblib\n",
      " - thresholds_cv_summary.csv\n",
      " - thresholds_cv_summary_from_calib_folds.csv\n",
      "\n",
      "=== artifacts\\emotions_test_metrics_partial_sgd_final.csv ===\n",
      "AP_macro (approx): 0.2750031034047775\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label_idx",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "support_test",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AP_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "thr_used",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "17b3dba3-15ae-4435-8255-80906886f7bf",
       "rows": [
        [
         "0",
         "5",
         "emo_caring",
         "84",
         "0.7015072412229194",
         "0.9607843137254902",
         "0.5833333333333334",
         "0.725925925925926",
         "2.226034851706438"
        ],
        [
         "1",
         "24",
         "emo_remorse",
         "41",
         "0.6730060697001569",
         "0.6428571428571429",
         "0.6585365853658537",
         "0.6506024096385542",
         "2.12291102283184"
        ],
        [
         "2",
         "16",
         "emo_grief",
         "77",
         "0.6635594084835748",
         "0.8301886792452831",
         "0.5714285714285714",
         "0.676923076923077",
         "2.1338043146838315"
        ],
        [
         "3",
         "8",
         "emo_desire",
         "213",
         "0.493749614582168",
         "0.6307692307692307",
         "0.3849765258215962",
         "0.478134110787172",
         "3.444100917933389"
        ],
        [
         "4",
         "4",
         "emo_approval",
         "249",
         "0.4751546698333839",
         "0.59375",
         "0.4578313253012048",
         "0.5170068027210885",
         "2.608159064778749"
        ],
        [
         "5",
         "14",
         "emo_fear",
         "920",
         "0.4439654635291348",
         "0.5537280701754386",
         "0.5489130434782609",
         "0.5513100436681223",
         "2.398303907329171"
        ],
        [
         "6",
         "10",
         "emo_disapproval",
         "64",
         "0.4161637925875272",
         "0.44",
         "0.515625",
         "0.4748201438848921",
         "1.347655000892538"
        ],
        [
         "7",
         "1",
         "emo_amusement",
         "59",
         "0.340137628355127",
         "0.4878048780487805",
         "0.3389830508474576",
         "0.4",
         "2.2059908082948656"
        ],
        [
         "8",
         "9",
         "emo_disappointment",
         "2831",
         "0.3232954736112336",
         "0.3373597929249353",
         "0.4143412221829742",
         "0.3719086873811034",
         "2.486216052796463"
        ],
        [
         "9",
         "0",
         "emo_admiration",
         "57",
         "0.2762663214489471",
         "0.2833333333333333",
         "0.2982456140350877",
         "0.2905982905982906",
         "0.0282631483761051"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_idx</th>\n",
       "      <th>label_name</th>\n",
       "      <th>support_test</th>\n",
       "      <th>AP_test</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>thr_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>emo_caring</td>\n",
       "      <td>84</td>\n",
       "      <td>0.701507</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>2.226035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>emo_remorse</td>\n",
       "      <td>41</td>\n",
       "      <td>0.673006</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>2.122911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>emo_grief</td>\n",
       "      <td>77</td>\n",
       "      <td>0.663559</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>2.133804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>emo_desire</td>\n",
       "      <td>213</td>\n",
       "      <td>0.493750</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.384977</td>\n",
       "      <td>0.478134</td>\n",
       "      <td>3.444101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>emo_approval</td>\n",
       "      <td>249</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>2.608159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>emo_fear</td>\n",
       "      <td>920</td>\n",
       "      <td>0.443965</td>\n",
       "      <td>0.553728</td>\n",
       "      <td>0.548913</td>\n",
       "      <td>0.551310</td>\n",
       "      <td>2.398304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>emo_disapproval</td>\n",
       "      <td>64</td>\n",
       "      <td>0.416164</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.474820</td>\n",
       "      <td>1.347655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>emo_amusement</td>\n",
       "      <td>59</td>\n",
       "      <td>0.340138</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.205991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>emo_disappointment</td>\n",
       "      <td>2831</td>\n",
       "      <td>0.323295</td>\n",
       "      <td>0.337360</td>\n",
       "      <td>0.414341</td>\n",
       "      <td>0.371909</td>\n",
       "      <td>2.486216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>emo_admiration</td>\n",
       "      <td>57</td>\n",
       "      <td>0.276266</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>0.028263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_idx          label_name  support_test   AP_test  precision    recall  \\\n",
       "0          5          emo_caring            84  0.701507   0.960784  0.583333   \n",
       "1         24         emo_remorse            41  0.673006   0.642857  0.658537   \n",
       "2         16           emo_grief            77  0.663559   0.830189  0.571429   \n",
       "3          8          emo_desire           213  0.493750   0.630769  0.384977   \n",
       "4          4        emo_approval           249  0.475155   0.593750  0.457831   \n",
       "5         14            emo_fear           920  0.443965   0.553728  0.548913   \n",
       "6         10     emo_disapproval            64  0.416164   0.440000  0.515625   \n",
       "7          1       emo_amusement            59  0.340138   0.487805  0.338983   \n",
       "8          9  emo_disappointment          2831  0.323295   0.337360  0.414341   \n",
       "9          0      emo_admiration            57  0.276266   0.283333  0.298246   \n",
       "\n",
       "         f1  thr_used  \n",
       "0  0.725926  2.226035  \n",
       "1  0.650602  2.122911  \n",
       "2  0.676923  2.133804  \n",
       "3  0.478134  3.444101  \n",
       "4  0.517007  2.608159  \n",
       "5  0.551310  2.398304  \n",
       "6  0.474820  1.347655  \n",
       "7  0.400000  2.205991  \n",
       "8  0.371909  2.486216  \n",
       "9  0.290598  0.028263  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mod√®les finals trouv√©s: ['artifacts\\\\emo_clf_final_trainval.joblib', 'artifacts\\\\emo_clf_final_trainval_calibrated.joblib', 'artifacts\\\\emo_sgd_partial_models.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch1.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch10.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch11.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch12.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch13.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch14.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch15.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch2.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch3.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch4.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch5.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch6.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch7.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch8.joblib', 'artifacts\\\\emo_sgd_partial_models_epoch9.joblib', 'artifacts\\\\emo_sgd_partial_models_resumed_final.joblib', 'artifacts\\\\emo_sgd_partial_models_resumed_final.joblib']\n"
     ]
    }
   ],
   "source": [
    "# Ex√©cuter pour v√©rifier rapidement que tout est sauvegard√© et voir les m√©triques\n",
    "import os, glob, pandas as pd, joblib, json\n",
    "\n",
    "art = \"artifacts\"\n",
    "print(\"Fichiers artifacts existants (trier):\")\n",
    "for p in sorted(glob.glob(os.path.join(art,\"*\"))):\n",
    "    print(\" -\", os.path.basename(p))\n",
    "\n",
    "# Charger les m√©triques test disponibles\n",
    "candidates = sorted(glob.glob(os.path.join(art,\"emotions_test_metrics*.csv\")))\n",
    "for path in candidates:\n",
    "    print(\"\\n===\", path, \"===\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"AP_macro (approx):\", df['AP_test'].dropna().mean())\n",
    "    display(df.head(10))\n",
    "\n",
    "# mod√®les disponibles\n",
    "models = sorted(glob.glob(os.path.join(art,\"*emo*_final*.joblib\")) + glob.glob(os.path.join(art,\"emo_*sgd*.joblib\")))\n",
    "print(\"\\nMod√®les finals trouv√©s:\", models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc80621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport √©crit -> artifacts\\report_C5_2_3.md\n"
     ]
    }
   ],
   "source": [
    "# Cela cr√©e artifacts/report_C5_2_3.md avec un r√©sum√© que tu peux joindre au rendu.\n",
    "import os, json, joblib, pandas as pd, numpy as np\n",
    "ART = \"artifacts\"\n",
    "out = os.path.join(ART, \"report_C5_2_3.md\")\n",
    "\n",
    "# Try to get main metrics file\n",
    "met = None\n",
    "for cand in [\"emotions_test_metrics.csv\", \"emotions_test_metrics_partial_sgd_final.csv\", \"emotions_test_metrics_partial_sgd.csv\", \"emotions_test_metrics_full.csv\"]:\n",
    "    p = os.path.join(ART, cand)\n",
    "    if os.path.exists(p):\n",
    "        met = p; break\n",
    "\n",
    "model_info = None\n",
    "for m in sorted(glob.glob(os.path.join(ART,\"*final*.joblib\")) + glob.glob(os.path.join(ART,\"emo_*sgd*.joblib\"))):\n",
    "    model_info = m; break\n",
    "\n",
    "with open(out, \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(\"# Rapport C5.2.3 ‚Äî Entra√Ænement mod√®le √©motions\\n\\n\")\n",
    "    f.write(\"## Artefacts trouv√©s\\n\\n\")\n",
    "    for p in sorted(glob.glob(os.path.join(ART,\"*\"))):\n",
    "        f.write(f\"- `{os.path.basename(p)}`\\n\")\n",
    "    f.write(\"\\n## Mod√®le s√©lectionn√©\\n\")\n",
    "    f.write(f\"- {model_info or 'Aucun mod√®le final trouv√©'}\\n\\n\")\n",
    "    if met:\n",
    "        df = pd.read_csv(met)\n",
    "        f.write(\"## Extrait m√©triques test (top 10 par AP)\\n\\n\")\n",
    "        f.write(df.sort_values(\"AP_test\", ascending=False).head(10).to_markdown(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"## M√©triques test non trouv√©es.\\n\")\n",
    "print(\"Rapport √©crit ->\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43d84ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifacts/final_model_choice.json -> {'chosen_model': 'emo_clf_final_trainval_calibrated.joblib', 'thresholds': 'artifacts/emo_thr_mean_floor.joblib', 'note': 'Choisi automatiquement apr√®s comparaison des metrics'}\n"
     ]
    }
   ],
   "source": [
    "# Si tu choisis un mod√®le (par ex. le calibr√©), enregistre un petit wrapper json indiquant le choix.\n",
    "import json, os\n",
    "choice = {\n",
    "    \"chosen_model\": \"emo_clf_final_trainval_calibrated.joblib\" if os.path.exists(\"artifacts/emo_clf_final_trainval_calibrated.joblib\") else \"emo_clf_final_trainval.joblib\",\n",
    "    \"thresholds\": \"artifacts/emo_thr_mean_floor.joblib\" if os.path.exists(\"artifacts/emo_thr_mean_floor.joblib\") else None,\n",
    "    \"note\": \"Choisi automatiquement apr√®s comparaison des metrics\"\n",
    "}\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "json.dump(choice, open(\"artifacts/final_model_choice.json\",\"w\"), indent=2)\n",
    "print(\"Saved artifacts/final_model_choice.json ->\", choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fa0b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: docs\\manifests\\models_manifest.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " {'task': 'sentiment',\n",
       "  'artifact': 'models\\\\clf_logreg_chi2_final_20250912_235354.joblib',\n",
       "  'hash': 'c91c05601524c8ab05cd76c862a72a718d49cc420729903d994b4b8213dc102a',\n",
       "  'created': '2025-09-21 20:56:02',\n",
       "  'git': None,\n",
       "  'env': {'python': '3.11.5',\n",
       "   'platform': 'Windows-10-10.0.26100-SP0',\n",
       "   'sklearn': '1.7.2',\n",
       "   'numpy': '2.3.3',\n",
       "   'pandas': '2.2.2',\n",
       "   'torch': '2.4.1+cu121',\n",
       "   'lightgbm': None},\n",
       "  'notes': 'LogReg TF-IDF (chi2/L1/SVD optionnels) ‚Äî grid + calibration + seuils val si fournis.',\n",
       "  'meta': {'artifact': 'clf_logreg_chi2_final_20250912_235354.joblib',\n",
       "   'X_shape': [120000, 180007],\n",
       "   'train/val/test': {'train': 96000, 'val': 12000, 'test': 12000},\n",
       "   'threshold': 0.2748431536764288,\n",
       "   'notes': 'TF-IDF -> chi2(k) -> LogReg(lbfgs).'}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C5.3.1 ‚Äî Manifestes par mod√®le (sentiment & √©motions)\n",
    "from pathlib import Path\n",
    "import json, hashlib, subprocess, sys, platform, time\n",
    "import joblib, os\n",
    "\n",
    "ART = Path(\"artifacts\"); ART.mkdir(exist_ok=True)\n",
    "DOC = Path(\"docs\"); DOC.mkdir(exist_ok=True)\n",
    "\n",
    "def sha256sum(path, buf_size=1024*1024):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(buf_size)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def get_git_hash():\n",
    "    try:\n",
    "        return subprocess.check_output([\"git\",\"rev-parse\",\"--short\",\"HEAD\"], text=True).strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def sys_env_info():\n",
    "    try:\n",
    "        import sklearn, numpy, pandas, torch\n",
    "        lgbv = None\n",
    "        try:\n",
    "            import lightgbm as lgb; lgbv = lgb.__version__\n",
    "        except: pass\n",
    "        info = {\n",
    "            \"python\": sys.version.split()[0],\n",
    "            \"platform\": platform.platform(),\n",
    "            \"sklearn\": sklearn.__version__,\n",
    "            \"numpy\": numpy.__version__,\n",
    "            \"pandas\": pandas.__version__,\n",
    "            \"torch\": torch.__version__ if \"torch\" in sys.modules else None,\n",
    "            \"lightgbm\": lgbv,\n",
    "        }\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return {\"python\": sys.version.split()[0], \"platform\": platform.platform(), \"error\": str(e)}\n",
    "\n",
    "MANIFESTS = []\n",
    "\n",
    "# 1) Sentiment\n",
    "cand_sent = sorted(Path(\"models\").glob(\"clf_logreg_chi2_gridcal_final_*.joblib\")) + \\\n",
    "            sorted(ART.glob(\"sentiment_grid_best_calibrated.joblib\")) + \\\n",
    "            sorted(Path(\"models\").glob(\"clf_logreg_chi2_final_*.joblib\"))\n",
    "for p in cand_sent:\n",
    "    man = {\n",
    "        \"task\": \"sentiment\",\n",
    "        \"artifact\": str(p),\n",
    "        \"hash\": sha256sum(p),\n",
    "        \"created\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"git\": get_git_hash(),\n",
    "        \"env\": sys_env_info(),\n",
    "        \"notes\": \"LogReg TF-IDF (chi2/L1/SVD optionnels) ‚Äî grid + calibration + seuils val si fournis.\"\n",
    "    }\n",
    "    # si tu as un json compagnon (tu en √©cris un pour certains bundles)\n",
    "    meta_path = p.with_suffix(\".json\")\n",
    "    if meta_path.exists():\n",
    "        try:\n",
    "            man[\"meta\"] = json.loads(meta_path.read_text(encoding=\"utf8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    MANIFESTS.append(man)\n",
    "\n",
    "# 2) √âmotions\n",
    "cand_emo = []\n",
    "cand_emo += list(ART.glob(\"emo_grid_best_bundle.joblib\"))\n",
    "cand_emo += list(ART.glob(\"emo_sgd_partial_models_final.joblib\"))\n",
    "cand_emo += list(ART.glob(\"emo_sgd_partial_models_resumed_final.joblib\"))\n",
    "cand_emo += list(ART.glob(\"final_emotions_lgb_subset.joblib\"))\n",
    "for p in cand_emo:\n",
    "    man = {\n",
    "        \"task\": \"emotions\",\n",
    "        \"artifact\": str(p),\n",
    "        \"hash\": sha256sum(p),\n",
    "        \"created\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"git\": get_git_hash(),\n",
    "        \"env\": sys_env_info(),\n",
    "        \"notes\": \"OVR LR / Calibrated LinearSVC / SGD partial_fit; thresholds per-label (thr_mean_floor).\"\n",
    "    }\n",
    "    # si bundle avec seuils/labels int√©gr√©s\n",
    "    try:\n",
    "        obj = joblib.load(p)\n",
    "        if isinstance(obj, dict):\n",
    "            if \"label_names_kept\" in obj: man[\"labels\"] = list(obj[\"label_names_kept\"])\n",
    "            if \"thresholds\" in obj: man[\"thresholds_shape\"] = len(obj[\"thresholds\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    MANIFESTS.append(man)\n",
    "\n",
    "# √âcrit un fichier manifest global (liste)\n",
    "(DOC/\"manifests\").mkdir(exist_ok=True)\n",
    "out_manifest = DOC/\"manifests\"/\"models_manifest.json\"\n",
    "out_manifest.write_text(json.dumps(MANIFESTS, indent=2, ensure_ascii=False), encoding=\"utf8\")\n",
    "print(\"Saved:\", out_manifest)\n",
    "len(MANIFESTS), MANIFESTS[0] if MANIFESTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c2ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: docs/manifests/registry_index.csv & registry_full.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "task",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "artifact",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hash",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "git",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "python",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sklearn",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "notes",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "dd41258e-f858-478e-b3f9-8d4ee7f3f812",
       "rows": [
        [
         "0",
         "emotions",
         "artifacts\\emo_sgd_partial_models_resumed_final.joblib",
         "00f163274b3d685b00fde76f9a2cc09c83c057ec2364398d54be4c93614bdde5",
         "2025-09-21 20:56:07",
         null,
         "3.11.5",
         "1.7.2",
         "OVR LR / Calibrated LinearSVC / SGD partial_fit; thresholds per-label (thr_mean_floor)."
        ],
        [
         "1",
         "sentiment",
         "models\\clf_logreg_chi2_final_20250918_231306.joblib",
         "e11a2fa13da4556b0bf421096e77b7f527ff23da37b05624576166524868be22",
         "2025-09-21 20:56:07",
         null,
         "3.11.5",
         "1.7.2",
         "LogReg TF-IDF (chi2/L1/SVD optionnels) ‚Äî grid + calibration + seuils val si fournis."
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>artifact</th>\n",
       "      <th>hash</th>\n",
       "      <th>created</th>\n",
       "      <th>git</th>\n",
       "      <th>python</th>\n",
       "      <th>sklearn</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emotions</td>\n",
       "      <td>artifacts\\emo_sgd_partial_models_resumed_final...</td>\n",
       "      <td>00f163274b3d685b00fde76f9a2cc09c83c057ec236439...</td>\n",
       "      <td>2025-09-21 20:56:07</td>\n",
       "      <td>None</td>\n",
       "      <td>3.11.5</td>\n",
       "      <td>1.7.2</td>\n",
       "      <td>OVR LR / Calibrated LinearSVC / SGD partial_fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentiment</td>\n",
       "      <td>models\\clf_logreg_chi2_final_20250918_231306.j...</td>\n",
       "      <td>e11a2fa13da4556b0bf421096e77b7f527ff23da37b056...</td>\n",
       "      <td>2025-09-21 20:56:07</td>\n",
       "      <td>None</td>\n",
       "      <td>3.11.5</td>\n",
       "      <td>1.7.2</td>\n",
       "      <td>LogReg TF-IDF (chi2/L1/SVD optionnels) ‚Äî grid ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        task                                           artifact  \\\n",
       "0   emotions  artifacts\\emo_sgd_partial_models_resumed_final...   \n",
       "1  sentiment  models\\clf_logreg_chi2_final_20250918_231306.j...   \n",
       "\n",
       "                                                hash              created  \\\n",
       "0  00f163274b3d685b00fde76f9a2cc09c83c057ec236439...  2025-09-21 20:56:07   \n",
       "1  e11a2fa13da4556b0bf421096e77b7f527ff23da37b056...  2025-09-21 20:56:07   \n",
       "\n",
       "    git  python sklearn                                              notes  \n",
       "0  None  3.11.5   1.7.2  OVR LR / Calibrated LinearSVC / SGD partial_fi...  \n",
       "1  None  3.11.5   1.7.2  LogReg TF-IDF (chi2/L1/SVD optionnels) ‚Äî grid ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C5.3.1 ‚Äî Registre index (CSV) avec dernier meilleur mod√®le par t√¢che\n",
    "import pandas as pd, json\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(\"docs/manifests/models_manifest.json\")\n",
    "assert manifest_path.exists(), \"Ex√©cute la cellule Manifestes d'abord.\"\n",
    "mans = json.loads(manifest_path.read_text(encoding=\"utf8\"))\n",
    "\n",
    "rows = []\n",
    "for m in mans:\n",
    "    rows.append({\n",
    "        \"task\": m[\"task\"],\n",
    "        \"artifact\": m[\"artifact\"],\n",
    "        \"hash\": m.get(\"hash\"),\n",
    "        \"created\": m.get(\"created\"),\n",
    "        \"git\": m.get(\"git\"),\n",
    "        \"python\": m.get(\"env\",{}).get(\"python\"),\n",
    "        \"sklearn\": m.get(\"env\",{}).get(\"sklearn\"),\n",
    "        \"notes\": m.get(\"notes\")\n",
    "    })\n",
    "df = pd.DataFrame(rows).sort_values([\"task\",\"created\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Choix na√Øf du \"best current\": dernier cr√©√© par t√¢che (tu peux remplacer par: lire ton CSV de comparaisons et prendre max AP/F1)\n",
    "best_rows = df.groupby(\"task\").head(1).copy()\n",
    "best_rows.to_csv(\"docs/manifests/registry_index.csv\", index=False)\n",
    "df.to_csv(\"docs/manifests/registry_full.csv\", index=False)\n",
    "print(\"Saved: docs/manifests/registry_index.csv & registry_full.csv\")\n",
    "best_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a423e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loaders robustes pour tes artefacts actuels ===\n",
    "from pathlib import Path\n",
    "import joblib, numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# ---------- SENTIMENT ----------\n",
    "def find_sentiment_artifact():\n",
    "    cands = []\n",
    "    cands += sorted(Path(\"models\").glob(\"clf_logreg_chi2_gridcal_final_*.joblib\"))\n",
    "    cands += [Path(\"artifacts\")/\"sentiment_grid_best_calibrated.joblib\"]\n",
    "    cands += sorted(Path(\"models\").glob(\"clf_logreg_chi2_final_*.joblib\"))  # <-- ajout important\n",
    "    cands = [p for p in cands if p.exists()]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"Aucun artefact sentiment trouv√©.\")\n",
    "    return cands[-1]\n",
    "\n",
    "def load_sentiment_bundle(path=None):\n",
    "    path = Path(path) if path else find_sentiment_artifact()\n",
    "    obj = joblib.load(path)\n",
    "    # 3 cas possibles:\n",
    "    #  a) dict avec \"model_cal\" ou \"model_uncal\" (pipeline complet)\n",
    "    #  b) dict avec \"model\" (LogReg sur X_tfidf_* d√©j√† transform√©)\n",
    "    #  c) pipeline sklearn directement\n",
    "    bundle = {\"path\": str(path)}\n",
    "    if isinstance(obj, dict):\n",
    "        bundle.update(obj)\n",
    "    else:\n",
    "        bundle[\"model\"] = obj\n",
    "    return bundle\n",
    "\n",
    "def predict_sentiment_from_features(X, bundle=None):\n",
    "    \"\"\"Pr√©dit √† partir de X d√©j√† transform√© (TF-IDF/chi¬≤/‚Ä¶); pas de vectorizer ici.\"\"\"\n",
    "    b = bundle or load_sentiment_bundle()\n",
    "    clf = b.get(\"model_cal\") or b.get(\"model_uncal\") or b.get(\"model\")\n",
    "    if clf is None:\n",
    "        raise ValueError(\"Bundle sentiment invalide: mod√®l(e) introuvable.\")\n",
    "    # accepte sparse ou dense\n",
    "    return clf.predict(X), (clf.predict_proba(X)[:,1] if hasattr(clf, \"predict_proba\") else None)\n",
    "\n",
    "# ---------- √âMOTIONS ----------\n",
    "class OVRListWrapper:\n",
    "    \"\"\"\n",
    "    Wrap une liste de classifieurs binaires (SGD) pour exposer predict_proba(X)->(n_samples, n_labels)\n",
    "    Compatible avec tes artefacts 'emo_sgd_partial_models*_final.joblib'\n",
    "    \"\"\"\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def predict_proba(self, X, batch=16384):\n",
    "        n = X.shape[0]; L = len(self.models)\n",
    "        out = np.empty((n, L), dtype=np.float32)\n",
    "        for i in range(0, n, batch):\n",
    "            j = min(n, i+batch)\n",
    "            Xb = np.asarray(X[i:j], dtype=np.float32)\n",
    "            for k, m in enumerate(self.models):\n",
    "                try:\n",
    "                    out[i:j, k] = m.predict_proba(Xb)[:,1]\n",
    "                except Exception:\n",
    "                    # SGD peut ne pas avoir predict_proba -> decision_function logit-like\n",
    "                    out[i:j, k] = m.decision_function(Xb)\n",
    "        return out\n",
    "\n",
    "def find_emotions_artifact():\n",
    "    cands = []\n",
    "    cands += [Path(\"artifacts\")/\"emo_grid_best_bundle.joblib\"]\n",
    "    cands += sorted(Path(\"artifacts\").glob(\"emo_sgd_partial_models_resumed_final.joblib\"))\n",
    "    cands += sorted(Path(\"artifacts\").glob(\"emo_sgd_partial_models_final.joblib\"))\n",
    "    cands = [p for p in cands if p.exists()]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"Aucun artefact √©motions trouv√©.\")\n",
    "    return cands[0]\n",
    "\n",
    "def load_emotions_runtime(path=None):\n",
    "    \"\"\"\n",
    "    Retourne (estimator_like, thresholds, label_names)\n",
    "    - si bundle dict (grid_best): utilise le best_estimator + thresholds si pr√©sents\n",
    "    - si liste SGD: wrap en OVRListWrapper + thresholds depuis artifacts/emo_thr_mean_floor*.joblib\n",
    "    \"\"\"\n",
    "    p = Path(path) if path else find_emotions_artifact()\n",
    "    obj = joblib.load(p)\n",
    "\n",
    "    thresholds = None\n",
    "    labels = None\n",
    "    if isinstance(obj, dict):\n",
    "        est = obj.get(\"best_estimator\", obj.get(\"estimator\", None))\n",
    "        thresholds = obj.get(\"thresholds\", thresholds)\n",
    "        labels = obj.get(\"label_names_kept\", labels)\n",
    "        if est is None:\n",
    "            raise ValueError(f\"Bundle √©motions {p.name} ne contient pas 'best_estimator'.\")\n",
    "    elif isinstance(obj, list):\n",
    "        est = OVRListWrapper(obj)\n",
    "        # chercher un fichier de seuils d√©j√† calcul√©s\n",
    "        from glob import glob\n",
    "        cand_thr = glob(\"artifacts/emo_thr_mean_floor*.joblib\")\n",
    "        if cand_thr:\n",
    "            thresholds = joblib.load(cand_thr[-1])\n",
    "    else:\n",
    "        # fallback: d√©j√† un estimator\n",
    "        est = obj\n",
    "\n",
    "    return est, thresholds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cbdb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sentiment loader test failed: X_te_ / X_va_ introuvables. Ex√©cute le split sentiment (tes cellules).\n",
      "‚ùå Emotions loader test failed: Xte_s / Xva_s introuvables. Ex√©cute tes cellules √©motions.\n"
     ]
    }
   ],
   "source": [
    "# === Smoke tests avec les features d√©j√† en m√©moire ===\n",
    "# SENTIMENT\n",
    "try:\n",
    "    bundle_sent = load_sentiment_bundle()\n",
    "    # On utilise X_te_ si dispo; sinon X_va_\n",
    "    X_for_test = globals().get(\"X_te_\", globals().get(\"X_va_\", None))\n",
    "    if X_for_test is None:\n",
    "        raise RuntimeError(\"X_te_ / X_va_ introuvables. Ex√©cute le split sentiment (tes cellules).\")\n",
    "    yhat_s, proba_s = predict_sentiment_from_features(X_for_test[:1000], bundle=bundle_sent)\n",
    "    print(\"‚úÖ Sentiment loader OK ‚Äî\", len(yhat_s), \"pr√©dictions\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Sentiment loader test failed:\", e)\n",
    "\n",
    "# √âMOTIONS\n",
    "try:\n",
    "    est_emo, thr_emo, labels_emo = load_emotions_runtime()\n",
    "    # utilise Xte_s si dispo\n",
    "    X_for_test = globals().get(\"Xte_s\", globals().get(\"Xva_s\", None))\n",
    "    if X_for_test is None:\n",
    "        raise RuntimeError(\"Xte_s / Xva_s introuvables. Ex√©cute tes cellules √©motions.\")\n",
    "    proba_e = est_emo.predict_proba(X_for_test[:2000])\n",
    "    if thr_emo is None:\n",
    "        print(\"Aucun seuil trouv√© ‚Äî j'utilise 0.5 par d√©faut.\")\n",
    "        thr_use = 0.5\n",
    "    else:\n",
    "        thr_use = np.asarray(thr_emo, dtype=float)\n",
    "        if thr_use.ndim == 1 and thr_use.size != proba_e.shape[1]:\n",
    "            print(\"‚ö†Ô∏è Longueur thresholds != n_labels; fallback √† 0.5.\")\n",
    "            thr_use = 0.5\n",
    "    yhat_e = (proba_e >= (thr_use if np.isscalar(thr_use) else thr_use.reshape(1,-1))).astype(int)\n",
    "    print(\"‚úÖ Emotions loader OK ‚Äî proba:\", proba_e.shape, \"| pred:\", yhat_e.shape)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Emotions loader test failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5071476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
